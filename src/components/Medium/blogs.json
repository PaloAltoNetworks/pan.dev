{
  "status": "ok",
  "feed": {
    "url": "https://medium.com/feed/palo-alto-networks-developer-blog",
    "title": "Palo Alto Networks Developer Blog - Medium",
    "link": "https://medium.com/palo-alto-networks-developer-blog?source=rss----7f77455ad9a7---4",
    "author": "",
    "description": "All things API, DevOps, SecOps, Security, Automation - Medium",
    "image": "https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png"
  },
  "items": [
    {
      "title": "User-ID / EDL … better both of them",
      "pubDate": "2021-04-13 16:39:40",
      "link": "https://medium.com/palo-alto-networks-developer-blog/user-id-edl-better-both-of-them-65b6a9d6c424?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/65b6a9d6c424",
      "author": "Xavier Homs",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*J8LUAx85I7Hf-SKk47MZfQ.jpeg",
      "description": "\n<h3>User-ID or EDL … why not both?</h3>\n<p>User-ID and External Dynamic Lists (EDL’s) are probably the most commonly used PAN-OS features to share external IP metadata with the NGFW. The Use cases for them are enormous: from blocking known DoS sources to forward traffic from specific IP addresses on low latency links.</p>\n<p>Let’s take for instance the following logs generated by the SSH daemon in a linux host.</p>\n<p>Won’t it be great to have a way to share the IP addresses used by the user <em>xhoms</em> with the NGFW so specific security policies are applied to traffic sourced by that address just because we now know the endpoint is being managed by that user?</p>\n<p>The following Shell CGI script could be used to create an External Dynamic List (EDL) that would list all known IP addresses the account <em>xhoms</em> was used to login into this server.</p>\n<p>That is great but:</p>\n<ul>\n<li>When would addresses be removed from the list? At the daily log rollout?</li>\n<li>Are all listed IP addresses still being operated by the user <em>xhoms</em>?</li>\n<li>Is there any way to remove addresses from the list at user logout?</li>\n</ul>\n<p>To overcome these (and many other) limitations Palo Alto Networks NGFW feature a very powerful IP tagging API called User-ID. Although EDL and User-ID cover similar objectives there are fundamental technical differences between them:</p>\n<ul>\n<li>User-ID is “asynchronous” (push mode), supports bot “set” and “delete” operations and provides a very flexible way to create groupings. Either by tagging address objects (Dynamic Address Group — DAG ) or by mapping users to addresses and then tagging these users (Dynamic User Group — DUG). User-ID allows for these tags to have a timeout in order for the PAN-OS device to take care of removing them at their due time.</li>\n<li>EDL is “universal”. Almost all network appliance vendors provide a feature to fetch an IP address list from a URL.</li>\n</ul>\n<p>Years ago it was up to the end customer to create its own connectors. Just like the CGI script we shared before. But as presence of PAN-OS powered NGFW’s grown among enterprise customers more application vendors decided to leverage the User-ID value by providing API clients out-of-the-box. Although this is great (for Palo Alto Network customers) losing the option to fetch a list from a URL (EDL mode) makes it more difficult to integrate legacy technologies that might be out there still in the network.</p>\n<h4>Creating EDL’s from User-ID enabled applications</h4>\n<p>Let’s assume you have an application that features a PAN-OS User-ID API client and that is capable of pushing log entries to the PAN-OS NGFW in an asynchronous way. Let’s assume, as well, that there are still some legacy network devices in your network that need to fetch that address metadata from a URL. How difficult it would be to create a micro-service for that?</p>\n<p>One option would be to leverage a PAN-OS XML SDK like PAN Python or PAN GO and to create a REST API that extracts the current state from the PAN-OS device using its operations API.</p>\n<pre>GET https://&lt;pan-os-device&gt;/api<br>    ?key=&lt;API-KEY&gt;<br>    &amp;type=op<br>    &amp;cmd=&lt;show&gt;&lt;object&gt;&lt;registered-ip&gt;&lt;all&gt;&lt;/all&gt;&lt;/registered-ip&gt;&lt;/object&gt;&lt;/show&gt;</pre>\n<pre>HTTP/1.1 200 OK</pre>\n<pre>&lt;response status=\"success\"&gt;<br>  &lt;result&gt;<br>    &lt;entry ip=\"10.10.10.11\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;entry ip=\"10.10.10.10\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;count&gt;2&lt;/count&gt;<br>  &lt;/result&gt;<br>&lt;/response&gt;</pre>\n<p>It shouldn’t be that difficult to parse the response and provide a plain list out of it, right? Come back to me if you’re thinking on using a XSLT processor to pipe it at the output of a cURL command packing everything as a CGI script because we might have some common grounds ;-)</p>\n<p>I’d love to propose a different approach though: to hijack User-ID messages by implementing a micro-service that behaves as a <strong>reverse proxy</strong> between the application that features the User-ID client and the PAN-OS device. I like this approach because it opens the door to other interesting use cases like enforcing timeout (adding timeout to entries that do not have it), converting DAG messages into DUG equivalents or adding additional tags based on the source application.</p>\n<h4>Components for a User-ID to EDL reverse proxy</h4>\n<p>The ingredients for such a receipt would be:</p>\n<ol>\n<li>a light http web server featuring routing (hijack messages sent to /api and let other requests pass through). GO http package and the type ReverseType fits like a glove in this case.</li>\n<li>a collection of XML-enabled GO structs to unmarshal captured User-ID messages.</li>\n<li>a library featuring User-ID message payload processing capable of keeping a valid (non-expired) state of User-to-IP, Group-to-IP and Tag-to-IP maps.</li>\n</ol>\n<p>Fortunately for us there is the xhoms/panoslib GO module that covers 2 and 3 (OK, let’s be honest, I purpose-built the library to support this article)</p>\n<p>With all these components it won’t be that difficult to reverse proxy a PAN-OS https service monitoring User-ID messages and exposing the state of the corresponding entries on a virtual endpoint named /edl that won’t conflict at all with the PAN-OS schema.</p>\n<p>I wanted to prove the point and ended up coding the receipt as a micro-service. You can either check the code in the the GitHub repository xhoms/uidmonitor or run it from a Docker-enabled host. Take a look to the related panos.pan.dev tutorial for insights in the source code rationale</p>\n<pre>docker run --rm -p 8080:8080 -e TARGET=&lt;pan-os-device&gt; ghcr.io/xhoms/uidmonitor</pre>\n<h4>Some payload examples</h4>\n<p>To get started you can check the following User-ID payload that registers the tag test to the IP addresses 10.10.10.10 and 10.10.20.20. Notice the different timeout values (100 vs 10 seconds)</p>\n<pre>POST http://127.0.0.1:8080/api/ HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;register&gt;<br>            &lt;entry ip=\"10.10.10.10\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry ip=\"10.10.20.20\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register&gt;<br>    &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Just after pushing the payload (before the 10 second tag expires) perform the following GET request on the /edl endpoint and verify the provided list contains both IP addresses.</p>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:07:50 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<p>A few seconds (+10) later the same transaction should return only the IP address whose tag has not expired.</p>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:08:55 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<p>Now let’s use a bit more complex payload. The following one register (login) two users mapped to IP addresses and then apply the user tag admin to both of them. As in the previous case, each tag has a different expiration value.</p>\n<pre>POST http://127.0.0.1:8080/api/ HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;login&gt;<br>            &lt;entry name=\"bar@test.local\" ip=\"10.10.20.20\" timeout=\"100\"&gt;&lt;/entry&gt;<br>            &lt;entry name=\"foo@test.local\" ip=\"10.10.10.10\" timeout=\"100\"&gt;&lt;/entry&gt;<br>        &lt;/login&gt;<br>        &lt;register-user&gt;<br>            &lt;entry user=\"foo@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry user=\"bar@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register-user&gt;<br>   &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Although the login transaction has a longer timeout on both cases, the group membership tag for the user “bar@test.local” is expected to timeout in 10 seconds.</p>\n<p>Calling the /edl endpoint at specific times would demonstrate the first tag expiration.</p>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=group<br>    &amp;key=admin</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:16 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<pre>...</pre>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=group<br>    &amp;key=admin<br><br>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:32 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<h3>Summary</h3>\n<p>Both EDL (because it provides “universal” coverage) and User-ID (because its advanced feature set) have their place in a large network. Implementing a reverse proxy between User-ID enabled applications and the managed PAN-OS device capable of hijacking User-ID messages have many different use cases: from basic User-ID to EDL conversion (as in the demo application) to advanced cases like applying policies to User-ID messages, enforcing maximum timeouts or transposing Address Groups into User Groups.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=65b6a9d6c424\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>User-ID / EDL … better both of them was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<h3>User-ID or EDL … why not both?</h3>\n<p>User-ID and External Dynamic Lists (EDL’s) are probably the most commonly used PAN-OS features to share external IP metadata with the NGFW. The Use cases for them are enormous: from blocking known DoS sources to forward traffic from specific IP addresses on low latency links.</p>\n<p>Let’s take for instance the following logs generated by the SSH daemon in a linux host.</p>\n<p>Won’t it be great to have a way to share the IP addresses used by the user <em>xhoms</em> with the NGFW so specific security policies are applied to traffic sourced by that address just because we now know the endpoint is being managed by that user?</p>\n<p>The following Shell CGI script could be used to create an External Dynamic List (EDL) that would list all known IP addresses the account <em>xhoms</em> was used to login into this server.</p>\n<p>That is great but:</p>\n<ul>\n<li>When would addresses be removed from the list? At the daily log rollout?</li>\n<li>Are all listed IP addresses still being operated by the user <em>xhoms</em>?</li>\n<li>Is there any way to remove addresses from the list at user logout?</li>\n</ul>\n<p>To overcome these (and many other) limitations Palo Alto Networks NGFW feature a very powerful IP tagging API called User-ID. Although EDL and User-ID cover similar objectives there are fundamental technical differences between them:</p>\n<ul>\n<li>User-ID is “asynchronous” (push mode), supports bot “set” and “delete” operations and provides a very flexible way to create groupings. Either by tagging address objects (Dynamic Address Group — DAG ) or by mapping users to addresses and then tagging these users (Dynamic User Group — DUG). User-ID allows for these tags to have a timeout in order for the PAN-OS device to take care of removing them at their due time.</li>\n<li>EDL is “universal”. Almost all network appliance vendors provide a feature to fetch an IP address list from a URL.</li>\n</ul>\n<p>Years ago it was up to the end customer to create its own connectors. Just like the CGI script we shared before. But as presence of PAN-OS powered NGFW’s grown among enterprise customers more application vendors decided to leverage the User-ID value by providing API clients out-of-the-box. Although this is great (for Palo Alto Network customers) losing the option to fetch a list from a URL (EDL mode) makes it more difficult to integrate legacy technologies that might be out there still in the network.</p>\n<h4>Creating EDL’s from User-ID enabled applications</h4>\n<p>Let’s assume you have an application that features a PAN-OS User-ID API client and that is capable of pushing log entries to the PAN-OS NGFW in an asynchronous way. Let’s assume, as well, that there are still some legacy network devices in your network that need to fetch that address metadata from a URL. How difficult it would be to create a micro-service for that?</p>\n<p>One option would be to leverage a PAN-OS XML SDK like PAN Python or PAN GO and to create a REST API that extracts the current state from the PAN-OS device using its operations API.</p>\n<pre>GET https://&lt;pan-os-device&gt;/api<br>    ?key=&lt;API-KEY&gt;<br>    &amp;type=op<br>    &amp;cmd=&lt;show&gt;&lt;object&gt;&lt;registered-ip&gt;&lt;all&gt;&lt;/all&gt;&lt;/registered-ip&gt;&lt;/object&gt;&lt;/show&gt;</pre>\n<pre>HTTP/1.1 200 OK</pre>\n<pre>&lt;response status=\"success\"&gt;<br>  &lt;result&gt;<br>    &lt;entry ip=\"10.10.10.11\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;entry ip=\"10.10.10.10\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;count&gt;2&lt;/count&gt;<br>  &lt;/result&gt;<br>&lt;/response&gt;</pre>\n<p>It shouldn’t be that difficult to parse the response and provide a plain list out of it, right? Come back to me if you’re thinking on using a XSLT processor to pipe it at the output of a cURL command packing everything as a CGI script because we might have some common grounds ;-)</p>\n<p>I’d love to propose a different approach though: to hijack User-ID messages by implementing a micro-service that behaves as a <strong>reverse proxy</strong> between the application that features the User-ID client and the PAN-OS device. I like this approach because it opens the door to other interesting use cases like enforcing timeout (adding timeout to entries that do not have it), converting DAG messages into DUG equivalents or adding additional tags based on the source application.</p>\n<h4>Components for a User-ID to EDL reverse proxy</h4>\n<p>The ingredients for such a receipt would be:</p>\n<ol>\n<li>a light http web server featuring routing (hijack messages sent to /api and let other requests pass through). GO http package and the type ReverseType fits like a glove in this case.</li>\n<li>a collection of XML-enabled GO structs to unmarshal captured User-ID messages.</li>\n<li>a library featuring User-ID message payload processing capable of keeping a valid (non-expired) state of User-to-IP, Group-to-IP and Tag-to-IP maps.</li>\n</ol>\n<p>Fortunately for us there is the xhoms/panoslib GO module that covers 2 and 3 (OK, let’s be honest, I purpose-built the library to support this article)</p>\n<p>With all these components it won’t be that difficult to reverse proxy a PAN-OS https service monitoring User-ID messages and exposing the state of the corresponding entries on a virtual endpoint named /edl that won’t conflict at all with the PAN-OS schema.</p>\n<p>I wanted to prove the point and ended up coding the receipt as a micro-service. You can either check the code in the the GitHub repository xhoms/uidmonitor or run it from a Docker-enabled host. Take a look to the related panos.pan.dev tutorial for insights in the source code rationale</p>\n<pre>docker run --rm -p 8080:8080 -e TARGET=&lt;pan-os-device&gt; ghcr.io/xhoms/uidmonitor</pre>\n<h4>Some payload examples</h4>\n<p>To get started you can check the following User-ID payload that registers the tag test to the IP addresses 10.10.10.10 and 10.10.20.20. Notice the different timeout values (100 vs 10 seconds)</p>\n<pre>POST http://127.0.0.1:8080/api/ HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;register&gt;<br>            &lt;entry ip=\"10.10.10.10\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry ip=\"10.10.20.20\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register&gt;<br>    &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Just after pushing the payload (before the 10 second tag expires) perform the following GET request on the /edl endpoint and verify the provided list contains both IP addresses.</p>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:07:50 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<p>A few seconds (+10) later the same transaction should return only the IP address whose tag has not expired.</p>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:08:55 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<p>Now let’s use a bit more complex payload. The following one register (login) two users mapped to IP addresses and then apply the user tag admin to both of them. As in the previous case, each tag has a different expiration value.</p>\n<pre>POST http://127.0.0.1:8080/api/ HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;login&gt;<br>            &lt;entry name=\"bar@test.local\" ip=\"10.10.20.20\" timeout=\"100\"&gt;&lt;/entry&gt;<br>            &lt;entry name=\"foo@test.local\" ip=\"10.10.10.10\" timeout=\"100\"&gt;&lt;/entry&gt;<br>        &lt;/login&gt;<br>        &lt;register-user&gt;<br>            &lt;entry user=\"foo@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry user=\"bar@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register-user&gt;<br>   &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Although the login transaction has a longer timeout on both cases, the group membership tag for the user “bar@test.local” is expected to timeout in 10 seconds.</p>\n<p>Calling the /edl endpoint at specific times would demonstrate the first tag expiration.</p>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=group<br>    &amp;key=admin</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:16 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<pre>...</pre>\n<pre>GET http://127.0.0.1:8080/edl/<br>    ?list=group<br>    &amp;key=admin<br><br>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:32 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<h3>Summary</h3>\n<p>Both EDL (because it provides “universal” coverage) and User-ID (because its advanced feature set) have their place in a large network. Implementing a reverse proxy between User-ID enabled applications and the managed PAN-OS device capable of hijacking User-ID messages have many different use cases: from basic User-ID to EDL conversion (as in the demo application) to advanced cases like applying policies to User-ID messages, enforcing maximum timeouts or transposing Address Groups into User Groups.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=65b6a9d6c424\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>User-ID / EDL … better both of them was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": ["pano", "go", "palo-alto-networks", "user-id"]
    },
    {
      "title": "Ingest PAN-OS Alerts into Cortex XDR Pro Endpoint",
      "pubDate": "2021-03-19 14:49:33",
      "link": "https://medium.com/palo-alto-networks-developer-blog/ingest-pan-os-alerts-into-cortex-xdr-pro-endpoint-8eef3e59b264?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/8eef3e59b264",
      "author": "Amine Basli",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*_LxpVyZVZbbKSPOIjGvJdA.jpeg",
      "description": "\n<blockquote>In this blog I’m sharing my experience leveraging Cortex XDR API’s to tailor-fit this product into specific customer requirements<strong><em>.</em></strong>\n</blockquote>\n<p>I bet that if you’re a Systems Engineer operating in EMEA you’ve argued more than once about the way products and licenses are packaged from an opportunity size point of view (too big for your customer base). There are many reasons that explain why a given vendor would like to have a reduced set of SKUs. But, no doubt, the fewer SKUs you have the lower the chances are for a given product to fit exactly into the customer’s need from a sizing standpoint.</p>\n<p>At Palo Alto Networks we’re no different, but we execute on two principles that help bridge the gaps:</p>\n<ul>\n<li>Provide APIs for our products</li>\n<li>Encourage the developer community to use them through our Developer Relations team</li>\n</ul>\n<h3>The use case</h3>\n<p>A Cortex XDR Pro Endpoint customer was interested in ingesting threat logs from their PAN-OS NGFW into his tenant to stitch them with the agent’s alerts and incidents. The customer was aware that this requirement could be achieved by sharing the NGFW state into the cloud-based Cortex Data Lake. But the corresponding SKU was overkill from a feature point of view (it provides not only alert stitching but ML baselining of traffic behaviour as well) and an unreachable cost from a budgeting perspective.</p>\n<p>Cortex XDR Pro provides a REST API to ingest third-party alerts to cover this specific use case. It is rate limited to only 600 alerts per minute per tenant but was more than enough for my customer because they were only interested in medium to critical alerts that appeared at a much lower frequency in their network. What about leveraging this API to provide an exact match to my customer’s requirement?</p>\n<p>On the other end, PAN-OS can be configured to forward these filtered alerts natively to any REST API with a very flexible payload templating feature. So at first it looked like we could “connect” the PAN-OS HTTP Log Forwarding feature with the Cortex XDR Insert Parsed Alert API. But a deep dive analysis revealed inconsistencies between the mandatory timestamp field format (it must be presented as UNIX milliseconds for XDR to accept it)</p>\n<p>It was too close to give up. I just needed a transformation pipeline that could be used as a middleman between the PAN-OS HTTP log forwarding feature and the Cortex XDR Insert Parsed Alert API. Provided I was ready for a middleman, I’d like it to enforce the XDR API quota limitations (600 alerts per minute / up to 60 alerts per update)</p>\n<h3>Developers to the rescue</h3>\n<p>I engaged our Developer Relations team in Palo Alto Networks because they’re always eager to discuss new use cases for our APIs. A quick discussion ended up with the following architecture proposal:</p>\n<ul>\n<li>An HTTP server would be implemented to be used as the endpoint for PAN-OS alert ingestion. Basic authentication would be implemented by providing a pre-shared key in the Authentication header.</li>\n<li>A transformation pipeline would convert the PAN-OS payload into a ready-to-consume XDR parsed alert API payload. The pipeline would take care, as well, to enforce the XDR API quota limits buffering bursts as needed.</li>\n<li>The XDR API client implementation would use an Advanced API Key for authentication.</li>\n<li>Everything would be packaged in a Docker container to ease its deployment. Configuration parameters would be provided to the container as environment variables.</li>\n</ul>\n<p>Implementation details ended up shared in a document available in cortex.pan.dev. If you’re in the mood of creating your own implementation I highly recommend you taking the time to go over the whole tutorial. If you just want to get to the point then you can use this ready-to-go container image.</p>\n<h3>Let’s get started</h3>\n<p>I opted for using the available container image. Let me guide you through my experience using it to fulfil my customer’s request.</p>\n<p>First of all the application requires some configuration data that must be provided as a set of environmental variables. A few of them are mandatory, others are just optional with default values.</p>\n<p>The following are the required variables (the application will refuse to start without them)</p>\n<ul>\n<li>API_KEY:<strong> </strong>XDR API Key (Advanced)</li>\n<li>API_KEY_ID:<strong> </strong>The XDR API Key identifier (its sequence number)</li>\n<li>FQDN:<strong> </strong>Fully Qualified Domain Name of the corresponding XDR Instance (i.e. myxdr.xdr.us.paloaltonetworks.com)</li>\n</ul>\n<p>The following are optional variables</p>\n<ul>\n<li>PSK: the server will check the value in the Authorization header to accept the request (default to no authentication)</li>\n<li>DEBUG: if it exists then the engine will be more verbose (defaults to false)</li>\n<li>PORT: TCP port to bind the HTTP server to (defaults to 8080)</li>\n<li>OFFSET: PAN-OS timestamp does not include time zone. By default, they will be considered in UTC (defaults to +0 hours)</li>\n<li>QUOTA_SIZE: XDR ingestion alert quota (defaults to 600)</li>\n<li>QUOTA_SECONDS: XDR ingestion alert quota refresh period (defaults to 60 seconds)</li>\n<li>UPDATE_SIZE: XDR ingestion alert max number of alerts per update (defaults to 60)</li>\n<li>BUFFER_SIZE: size of the pipe buffer (defaults to 6000 = 10 minutes)</li>\n<li>T1: how often the pipe buffer polled for new alerts (defaults to 2 seconds)</li>\n</ul>\n<p>For more details, you can check public repository documentation</p>\n<h4>Step 1: <strong>Generate an Advanced API key on Cortex XDR</strong>\n</h4>\n<p>Connect to your Cortex XDR instance and navigate to Setting &gt; API Keys Generate an API Key of type Advanced granting the Administrator role to it (that role is required for Alert ingestion)</p>\n<h4><strong>Step 2: Run the container image</strong></h4>\n<p>Assuming you have Docker installed on your computer the following command line command would pull the image and run the micro-service with configuration options passed as environmental variables.</p>\n<pre>docker run -rm -p 8080:8080 -e PSK=hello -e FQDN=xxx.xdr.us.paloaltonetworks.com -e API_KEY=&lt;my-api-key&gt; -e API_KEY_ID=&lt;my-key-id&gt; -e DEBUG=yes ghcr.io/xhoms/xdrgateway</pre>\n<p>The Debug option provides more verbosity and it is recommended for initial experimentation. If everything goes as expected you’ll see a log message like the following one.</p>\n<pre>2021/03/17 21:32:25 starting http service on port 8080</pre>\n<h4>Step 3: Perform a quick test to verify the micro-service is running</h4>\n<p>Use any API test tool (I like good-old Curl) to push a test payload.</p>\n<pre>curl -X POST -H \"Authorization: hello\" \"http://127.0.0.1:8080/in\" -d '{\"src\":\"1.2.3.4\",\"sport\":1234,\"dst\":\"4.3.2.1\",\"dport\": 4231,\"time_generated\":\"2021/01/06 20:44:34\",\"rule\":\"test_rule\",\"serial\":\"9999\",\"sender_sw_version\":\"10.0.4\",\"subtype\":\"spyware\",\"threat_name\":\"bad-bad-c2c\",\"severity\":\"critical\",\"action\":\"alert\"}---annex---\"Mozi Command and Control Traffic Detection\"'</pre>\n<p>You should receive an empty status 200 response and see log messages in the server console like the following ones:</p>\n<pre>2021/03/17 21:52:16 api - successfully parsed alert<br>2021/03/17 21:52:17 xdrclient - successful call to insert_parsed_alerts</pre>\n<h4><strong>Step 4: Configure HTTP log Forward on the Firewall:</strong></h4>\n<p>PAN-OS NGFW can forward alerts to HTTP/S endpoints. The feature configuration is available in Device &gt; Server Profiles &gt; HTTP and, for this case, you should use the following parameters</p>\n<ul>\n<li>IP : IP address of the container host</li>\n<li>Protocol : HTTP</li>\n<li>Port: 8080(default)</li>\n<li>HTTP Method POST</li>\n</ul>\n<p>Some notes regarding the payload format</p>\n<ul>\n<li>URI Format:“/in” is the endpoint where the micro-service listen for POST requests containing new alerts</li>\n<li>Headers: Notice we’re setting the value hello in the Authorization header to match the -e PSK=hello configuration variable we passed to the micro-service</li>\n<li>Payload: The variable $threat_name was introduced with PAN-OS 10.0. If you’re using older versions of PAN-OS the you can use the variable $threatid instead</li>\n</ul>\n<p>The last action to complete this job is to create a new Log Forwarding profile that will consume our recently created HTTP server and to use it in all security rules we want their alerts being ingested into XDR. The configuration object is available at Objects &gt; Log Forwarding</p>\n<h4>Step 5: Final checks</h4>\n<p>As your PAN-OS NGFW starts generating alerts, you should see activity in the micro-service’s output log and alerts being ingested into the Cortex XDR instance. The source tag Palo Alto Networks — PAN-OS clearly indicates these alerts were pushed by our deployment.</p>\n<h3>Production-ready version</h3>\n<p>There a couple of additional steps to perform before considering the micro-service production-ready.</p>\n<ul>\n<li>The service should be exposed over a secure channel (TLS). Best option is to leverage the preferred forward-proxy environment available in our container environment. Notice the image honours the PORT env variable and should work out-of-the-box almost everywhere</li>\n<li>A restart-policy should be in place to restart the container in case of a crash. It is not a good idea to run more than one instance in a load balancing group because the quota enforcement won’t be synchronised between them</li>\n</ul>\n<h3>Summary</h3>\n<p>Palo Alto Networks products and services are built with automation and customisation in mind. They feature rich API’s that can be used to tailor them to specific customer needs.</p>\n<p>In my case these APIs provided the customer with a choice:</p>\n<ul>\n<li>Either wait for the scope of the project (and its budget) to increase in order to accommodate additional products like Cortex Network Traffic Analysis <strong>or</strong>\n</li>\n<li>Deploy a compact “middle-man” that would connect the PAN-OS and Cortex XDR API’s</li>\n</ul>\n<p>This experience empowered me by acquiring DevOps knowledge (building micro-services) I’m sure I’ll use in many opportunities to come. Special thanks to The Developer Relations team at Palo Alto Networks who provided documentation and examples, and were eager to explore this new use case. I couldn’t have done it without their help and the work they’ve been putting into improving our developer experience.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8eef3e59b264\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Ingest PAN-OS Alerts into Cortex XDR Pro Endpoint was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<blockquote>In this blog I’m sharing my experience leveraging Cortex XDR API’s to tailor-fit this product into specific customer requirements<strong><em>.</em></strong>\n</blockquote>\n<p>I bet that if you’re a Systems Engineer operating in EMEA you’ve argued more than once about the way products and licenses are packaged from an opportunity size point of view (too big for your customer base). There are many reasons that explain why a given vendor would like to have a reduced set of SKUs. But, no doubt, the fewer SKUs you have the lower the chances are for a given product to fit exactly into the customer’s need from a sizing standpoint.</p>\n<p>At Palo Alto Networks we’re no different, but we execute on two principles that help bridge the gaps:</p>\n<ul>\n<li>Provide APIs for our products</li>\n<li>Encourage the developer community to use them through our Developer Relations team</li>\n</ul>\n<h3>The use case</h3>\n<p>A Cortex XDR Pro Endpoint customer was interested in ingesting threat logs from their PAN-OS NGFW into his tenant to stitch them with the agent’s alerts and incidents. The customer was aware that this requirement could be achieved by sharing the NGFW state into the cloud-based Cortex Data Lake. But the corresponding SKU was overkill from a feature point of view (it provides not only alert stitching but ML baselining of traffic behaviour as well) and an unreachable cost from a budgeting perspective.</p>\n<p>Cortex XDR Pro provides a REST API to ingest third-party alerts to cover this specific use case. It is rate limited to only 600 alerts per minute per tenant but was more than enough for my customer because they were only interested in medium to critical alerts that appeared at a much lower frequency in their network. What about leveraging this API to provide an exact match to my customer’s requirement?</p>\n<p>On the other end, PAN-OS can be configured to forward these filtered alerts natively to any REST API with a very flexible payload templating feature. So at first it looked like we could “connect” the PAN-OS HTTP Log Forwarding feature with the Cortex XDR Insert Parsed Alert API. But a deep dive analysis revealed inconsistencies between the mandatory timestamp field format (it must be presented as UNIX milliseconds for XDR to accept it)</p>\n<p>It was too close to give up. I just needed a transformation pipeline that could be used as a middleman between the PAN-OS HTTP log forwarding feature and the Cortex XDR Insert Parsed Alert API. Provided I was ready for a middleman, I’d like it to enforce the XDR API quota limitations (600 alerts per minute / up to 60 alerts per update)</p>\n<h3>Developers to the rescue</h3>\n<p>I engaged our Developer Relations team in Palo Alto Networks because they’re always eager to discuss new use cases for our APIs. A quick discussion ended up with the following architecture proposal:</p>\n<ul>\n<li>An HTTP server would be implemented to be used as the endpoint for PAN-OS alert ingestion. Basic authentication would be implemented by providing a pre-shared key in the Authentication header.</li>\n<li>A transformation pipeline would convert the PAN-OS payload into a ready-to-consume XDR parsed alert API payload. The pipeline would take care, as well, to enforce the XDR API quota limits buffering bursts as needed.</li>\n<li>The XDR API client implementation would use an Advanced API Key for authentication.</li>\n<li>Everything would be packaged in a Docker container to ease its deployment. Configuration parameters would be provided to the container as environment variables.</li>\n</ul>\n<p>Implementation details ended up shared in a document available in cortex.pan.dev. If you’re in the mood of creating your own implementation I highly recommend you taking the time to go over the whole tutorial. If you just want to get to the point then you can use this ready-to-go container image.</p>\n<h3>Let’s get started</h3>\n<p>I opted for using the available container image. Let me guide you through my experience using it to fulfil my customer’s request.</p>\n<p>First of all the application requires some configuration data that must be provided as a set of environmental variables. A few of them are mandatory, others are just optional with default values.</p>\n<p>The following are the required variables (the application will refuse to start without them)</p>\n<ul>\n<li>API_KEY:<strong> </strong>XDR API Key (Advanced)</li>\n<li>API_KEY_ID:<strong> </strong>The XDR API Key identifier (its sequence number)</li>\n<li>FQDN:<strong> </strong>Fully Qualified Domain Name of the corresponding XDR Instance (i.e. myxdr.xdr.us.paloaltonetworks.com)</li>\n</ul>\n<p>The following are optional variables</p>\n<ul>\n<li>PSK: the server will check the value in the Authorization header to accept the request (default to no authentication)</li>\n<li>DEBUG: if it exists then the engine will be more verbose (defaults to false)</li>\n<li>PORT: TCP port to bind the HTTP server to (defaults to 8080)</li>\n<li>OFFSET: PAN-OS timestamp does not include time zone. By default, they will be considered in UTC (defaults to +0 hours)</li>\n<li>QUOTA_SIZE: XDR ingestion alert quota (defaults to 600)</li>\n<li>QUOTA_SECONDS: XDR ingestion alert quota refresh period (defaults to 60 seconds)</li>\n<li>UPDATE_SIZE: XDR ingestion alert max number of alerts per update (defaults to 60)</li>\n<li>BUFFER_SIZE: size of the pipe buffer (defaults to 6000 = 10 minutes)</li>\n<li>T1: how often the pipe buffer polled for new alerts (defaults to 2 seconds)</li>\n</ul>\n<p>For more details, you can check public repository documentation</p>\n<h4>Step 1: <strong>Generate an Advanced API key on Cortex XDR</strong>\n</h4>\n<p>Connect to your Cortex XDR instance and navigate to Setting &gt; API Keys Generate an API Key of type Advanced granting the Administrator role to it (that role is required for Alert ingestion)</p>\n<h4><strong>Step 2: Run the container image</strong></h4>\n<p>Assuming you have Docker installed on your computer the following command line command would pull the image and run the micro-service with configuration options passed as environmental variables.</p>\n<pre>docker run -rm -p 8080:8080 -e PSK=hello -e FQDN=xxx.xdr.us.paloaltonetworks.com -e API_KEY=&lt;my-api-key&gt; -e API_KEY_ID=&lt;my-key-id&gt; -e DEBUG=yes ghcr.io/xhoms/xdrgateway</pre>\n<p>The Debug option provides more verbosity and it is recommended for initial experimentation. If everything goes as expected you’ll see a log message like the following one.</p>\n<pre>2021/03/17 21:32:25 starting http service on port 8080</pre>\n<h4>Step 3: Perform a quick test to verify the micro-service is running</h4>\n<p>Use any API test tool (I like good-old Curl) to push a test payload.</p>\n<pre>curl -X POST -H \"Authorization: hello\" \"http://127.0.0.1:8080/in\" -d '{\"src\":\"1.2.3.4\",\"sport\":1234,\"dst\":\"4.3.2.1\",\"dport\": 4231,\"time_generated\":\"2021/01/06 20:44:34\",\"rule\":\"test_rule\",\"serial\":\"9999\",\"sender_sw_version\":\"10.0.4\",\"subtype\":\"spyware\",\"threat_name\":\"bad-bad-c2c\",\"severity\":\"critical\",\"action\":\"alert\"}---annex---\"Mozi Command and Control Traffic Detection\"'</pre>\n<p>You should receive an empty status 200 response and see log messages in the server console like the following ones:</p>\n<pre>2021/03/17 21:52:16 api - successfully parsed alert<br>2021/03/17 21:52:17 xdrclient - successful call to insert_parsed_alerts</pre>\n<h4><strong>Step 4: Configure HTTP log Forward on the Firewall:</strong></h4>\n<p>PAN-OS NGFW can forward alerts to HTTP/S endpoints. The feature configuration is available in Device &gt; Server Profiles &gt; HTTP and, for this case, you should use the following parameters</p>\n<ul>\n<li>IP : IP address of the container host</li>\n<li>Protocol : HTTP</li>\n<li>Port: 8080(default)</li>\n<li>HTTP Method POST</li>\n</ul>\n<p>Some notes regarding the payload format</p>\n<ul>\n<li>URI Format:“/in” is the endpoint where the micro-service listen for POST requests containing new alerts</li>\n<li>Headers: Notice we’re setting the value hello in the Authorization header to match the -e PSK=hello configuration variable we passed to the micro-service</li>\n<li>Payload: The variable $threat_name was introduced with PAN-OS 10.0. If you’re using older versions of PAN-OS the you can use the variable $threatid instead</li>\n</ul>\n<p>The last action to complete this job is to create a new Log Forwarding profile that will consume our recently created HTTP server and to use it in all security rules we want their alerts being ingested into XDR. The configuration object is available at Objects &gt; Log Forwarding</p>\n<h4>Step 5: Final checks</h4>\n<p>As your PAN-OS NGFW starts generating alerts, you should see activity in the micro-service’s output log and alerts being ingested into the Cortex XDR instance. The source tag Palo Alto Networks — PAN-OS clearly indicates these alerts were pushed by our deployment.</p>\n<h3>Production-ready version</h3>\n<p>There a couple of additional steps to perform before considering the micro-service production-ready.</p>\n<ul>\n<li>The service should be exposed over a secure channel (TLS). Best option is to leverage the preferred forward-proxy environment available in our container environment. Notice the image honours the PORT env variable and should work out-of-the-box almost everywhere</li>\n<li>A restart-policy should be in place to restart the container in case of a crash. It is not a good idea to run more than one instance in a load balancing group because the quota enforcement won’t be synchronised between them</li>\n</ul>\n<h3>Summary</h3>\n<p>Palo Alto Networks products and services are built with automation and customisation in mind. They feature rich API’s that can be used to tailor them to specific customer needs.</p>\n<p>In my case these APIs provided the customer with a choice:</p>\n<ul>\n<li>Either wait for the scope of the project (and its budget) to increase in order to accommodate additional products like Cortex Network Traffic Analysis <strong>or</strong>\n</li>\n<li>Deploy a compact “middle-man” that would connect the PAN-OS and Cortex XDR API’s</li>\n</ul>\n<p>This experience empowered me by acquiring DevOps knowledge (building micro-services) I’m sure I’ll use in many opportunities to come. Special thanks to The Developer Relations team at Palo Alto Networks who provided documentation and examples, and were eager to explore this new use case. I couldn’t have done it without their help and the work they’ve been putting into improving our developer experience.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8eef3e59b264\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Ingest PAN-OS Alerts into Cortex XDR Pro Endpoint was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "cortex-xdr",
        "containers",
        "pan-os-alerts",
        "api",
        "ingestion"
      ]
    },
    {
      "title": "Enterprise API design practices: Part 4",
      "pubDate": "2021-02-23 10:10:38",
      "link": "https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/b94d9e8532f5",
      "author": "Francesco Vigo",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*SmWJPvCF1Y5OMz_Wvxe5Vg.jpeg",
      "description": "\n<p>Welcome to the last part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. I initially covered some background context and the importance of design, then I presented security and backend protection and in the third chapter I explored optimizations and scale; this post is about monitoring and Developer Experience (DX).</p>\n<h3>5. Monitor your APIs</h3>\n<p>Don’t fly blind: you must make sure that your APIs are properly instrumented so you can monitor what’s going on. This is important for a lot of good reasons, such as:</p>\n<ul>\n<li>\n<strong>Security: </strong>is your service under attack or being used maliciously? You should always be able to figure out if there are anomalies and react swiftly to prevent incidents.</li>\n<li>\n<strong>Auditing</strong>: depending on the nature of your business, you might be required by compliance or investigative reasons to produce an audit trail of user activity of your product, which requires proper instrumentation of your APIs as well as logging events.</li>\n<li>\n<strong>Performance and scale</strong>: you should be aware of how your API and backend are performing and if the key metrics are within the acceptable ranges, way before your users start complaining and your business is impacted. It’s always better to optimize before performance becomes a real problem.</li>\n<li>\n<strong>Cost:</strong> similarly, with proper instrumentation, you can be aware of your infrastructure costs sustained to serve a certain amount of traffic. That can help you with capacity planning and cost modeling if you’re monetizing your service. And to avoid unexpected bills at the end of the month that might force you to disrupt the service.</li>\n<li>\n<strong>Feedback</strong>: with proper telemetry data to look at when you deploy a change, you can understand how it’s performing and whether it was a good idea to implement it. It also allows you to implement prototyping techniques such as A/B testing.</li>\n<li>\n<strong>Learn</strong>: analyzing how developers use your API can be a great source of learning. It will give you useful insights on how your service is being consumed and is a valuable source of ideas that you can evaluate for new features (i.e. new use-case driven API endpoints).</li>\n</ul>\n<p>Proper instrumentation of services is a vast topic and here I just want to summarize a few items that are usually easy to implement:</p>\n<ul>\n<li>Every API request should have a unique operation identifier that is stored in your logs and can help your ops team figure out what happened. This identifier should also be reported back to clients somewhere (usually in API response the headers) especially, but not exclusively, in case of errors.</li>\n<li>Keep an eye on API requests that fail with server errors (i.e. the HTTP 5xx ones) and, if the number is non-negligible, try to pinpoint the root cause: is it a bug, or some request that is causing timeouts in the backend? Can you fix it or make it faster?</li>\n<li>Keep a reasonable log history to allow tracking errors and auditing user activity for at least several days back in time.</li>\n<li>Create dashboards that help you monitor user activity, API usage, security metrics, etc. And make sure to <strong>check them often</strong>.</li>\n</ul>\n<h3>6. Make Developer Experience a priority</h3>\n<p>Last principle, but definitely not the least important. Even if you are lucky and the developers that use your APIs are required to do so because of an external mandate, you shouldn’t make their experience less positive.</p>\n<blockquote>In fact, the Developer Experience of your product should be awesome.</blockquote>\n<p>If developers properly understand how to work with your APIs, and enjoy doing so, they will likely run into fewer issues and be more patient when trying to overcome them. They will also provide you with good quality feedback. It will reduce the number of requests they generate on your support teams. They will give you insights and use case ideas for building a better product.</p>\n<blockquote>And, more importantly, happy developers will ultimately build better products for their own customers which, in turn, will act as a force multiplier for the value and success of your own product. Everybody wins in the API Economy model: customers, business stakeholders, partners, product teams, engineers, support teams.</blockquote>\n<p>I’ve witnessed several situations where developers were so exhausted and frustrated with working against a bad API that, as soon as they saw the light at the end of the tunnel, they stopped thinking creatively and just powered their way through to an MVP that was far from viable and valuable. But it checked the box they needed so they were allowed to move on. I consider this a very specific scenario of <em>developer fatigue</em>: let’s call it “<strong>API Fatigue</strong>”.</p>\n<p>Luckily, I’ve also experienced the other way around, where new, unplanned, great features were added because the DX was good and we had fun integrating things together.</p>\n<p>There are many resources out there that describe how to make APIs with great developer experience: the most obvious one is to create APIs that are clear and simple to consume. Apply the Principle of least astonishment.</p>\n<p>I recommend considering the following when shipping an API:</p>\n<ul>\n<li>\n<strong>Document your API properly</strong>: it’s almost certain that the time you spend creating proper documentation at the beginning is saved later on when developers start consuming it. Specification files, such as OpenAPI Specification (OAS) tremendously help here. Also, if you follow the design-first approach that we discussed in the first post of this series, you’ll probably already have a specification file ready to use.</li>\n<li>\n<strong>Support different learning paths</strong>: some developers like to read all the documentation from top to bottom before writing a single line of code, others will start with the code editor right away. Instead of forcing a learning path, try to embrace the different mindsets and provide tools for everyone: specification files, Postman collections, examples in different programming languages, an easy-to-access API sandbox (i.e. something that doesn’t require installing an Enterprise product and waiting 2 weeks to get a license to make your first API call), a developer portal, tutorials and, why not, video walkthroughs. This might sound overwhelming but pays off in the end. With the proper design done first a lot of this content can be autogenerated from the specification files.</li>\n<li>\n<strong>Document the data structure</strong>: if you can, don’t just add lists of properties to your specs and docs: strive to provide proper descriptions of the fields that your API uses so that developers that are not fully familiar with your product can understand them. Remove ambiguity from the documentation as much as possible. This can go a long way, as developers can make mental models by understanding the data properly that often leads to better use cases than “<em>just pull some data from this API”.</em>\n</li>\n<li>\n<strong>Use verbs, status codes, and error descriptions properly</strong>: leverage the power of the protocol you are using (i.e. HTTP when using REST APIs) to define how to do things and what responses mean. Proper usage of status codes and good error messages will dramatically reduce the number of requests to your support team. Developers are smart and want to solve problems quickly so if you provide them with the right information to do so, they won’t bother you. Also, if you are properly logging and monitoring your API behavior, it will be easier for your support team to troubleshoot if your errors are not all “500 Internal Server Error” without any other detail.</li>\n</ul>\n<p>Finally,<strong> stay close to the developers</strong>: especially if your API is being used by people external from your organization, it’s incredibly important to be close to them as much as you can to support them, learn and gather feedback on your API and its documentation. Allow everyone that is responsible for designing and engineering your API to be in that feedback loop, so they can share the learnings. Consider creating a community where people can ask questions and can expect quick answers (Slack, Reddit, Stack Overflow, etc.). I’ve made great friendships this way!</p>\n<h3>A few examples</h3>\n<p>There are many great APIs out there. Here is a small, not complete, list of products from other companies that, for one reason or another, are strong examples of what I described in this blog series:</p>\n<ul>\n<li>Microsoft Graph</li>\n<li>Google Cloud</li>\n<li>VirusTotal</li>\n<li>GitHub REST and GraphQL APIs</li>\n</ul>\n<h3>Conclusion</h3>\n<p>And that’s a wrap, thanks for reading! There are more things that I’ve learned that I might share in future posts, but in my opinion, these are the most relevant ones. I hope you found this series useful: looking forward to hearing your feedback and comments!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b94d9e8532f5\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 4 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<p>Welcome to the last part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. I initially covered some background context and the importance of design, then I presented security and backend protection and in the third chapter I explored optimizations and scale; this post is about monitoring and Developer Experience (DX).</p>\n<h3>5. Monitor your APIs</h3>\n<p>Don’t fly blind: you must make sure that your APIs are properly instrumented so you can monitor what’s going on. This is important for a lot of good reasons, such as:</p>\n<ul>\n<li>\n<strong>Security: </strong>is your service under attack or being used maliciously? You should always be able to figure out if there are anomalies and react swiftly to prevent incidents.</li>\n<li>\n<strong>Auditing</strong>: depending on the nature of your business, you might be required by compliance or investigative reasons to produce an audit trail of user activity of your product, which requires proper instrumentation of your APIs as well as logging events.</li>\n<li>\n<strong>Performance and scale</strong>: you should be aware of how your API and backend are performing and if the key metrics are within the acceptable ranges, way before your users start complaining and your business is impacted. It’s always better to optimize before performance becomes a real problem.</li>\n<li>\n<strong>Cost:</strong> similarly, with proper instrumentation, you can be aware of your infrastructure costs sustained to serve a certain amount of traffic. That can help you with capacity planning and cost modeling if you’re monetizing your service. And to avoid unexpected bills at the end of the month that might force you to disrupt the service.</li>\n<li>\n<strong>Feedback</strong>: with proper telemetry data to look at when you deploy a change, you can understand how it’s performing and whether it was a good idea to implement it. It also allows you to implement prototyping techniques such as A/B testing.</li>\n<li>\n<strong>Learn</strong>: analyzing how developers use your API can be a great source of learning. It will give you useful insights on how your service is being consumed and is a valuable source of ideas that you can evaluate for new features (i.e. new use-case driven API endpoints).</li>\n</ul>\n<p>Proper instrumentation of services is a vast topic and here I just want to summarize a few items that are usually easy to implement:</p>\n<ul>\n<li>Every API request should have a unique operation identifier that is stored in your logs and can help your ops team figure out what happened. This identifier should also be reported back to clients somewhere (usually in API response the headers) especially, but not exclusively, in case of errors.</li>\n<li>Keep an eye on API requests that fail with server errors (i.e. the HTTP 5xx ones) and, if the number is non-negligible, try to pinpoint the root cause: is it a bug, or some request that is causing timeouts in the backend? Can you fix it or make it faster?</li>\n<li>Keep a reasonable log history to allow tracking errors and auditing user activity for at least several days back in time.</li>\n<li>Create dashboards that help you monitor user activity, API usage, security metrics, etc. And make sure to <strong>check them often</strong>.</li>\n</ul>\n<h3>6. Make Developer Experience a priority</h3>\n<p>Last principle, but definitely not the least important. Even if you are lucky and the developers that use your APIs are required to do so because of an external mandate, you shouldn’t make their experience less positive.</p>\n<blockquote>In fact, the Developer Experience of your product should be awesome.</blockquote>\n<p>If developers properly understand how to work with your APIs, and enjoy doing so, they will likely run into fewer issues and be more patient when trying to overcome them. They will also provide you with good quality feedback. It will reduce the number of requests they generate on your support teams. They will give you insights and use case ideas for building a better product.</p>\n<blockquote>And, more importantly, happy developers will ultimately build better products for their own customers which, in turn, will act as a force multiplier for the value and success of your own product. Everybody wins in the API Economy model: customers, business stakeholders, partners, product teams, engineers, support teams.</blockquote>\n<p>I’ve witnessed several situations where developers were so exhausted and frustrated with working against a bad API that, as soon as they saw the light at the end of the tunnel, they stopped thinking creatively and just powered their way through to an MVP that was far from viable and valuable. But it checked the box they needed so they were allowed to move on. I consider this a very specific scenario of <em>developer fatigue</em>: let’s call it “<strong>API Fatigue</strong>”.</p>\n<p>Luckily, I’ve also experienced the other way around, where new, unplanned, great features were added because the DX was good and we had fun integrating things together.</p>\n<p>There are many resources out there that describe how to make APIs with great developer experience: the most obvious one is to create APIs that are clear and simple to consume. Apply the Principle of least astonishment.</p>\n<p>I recommend considering the following when shipping an API:</p>\n<ul>\n<li>\n<strong>Document your API properly</strong>: it’s almost certain that the time you spend creating proper documentation at the beginning is saved later on when developers start consuming it. Specification files, such as OpenAPI Specification (OAS) tremendously help here. Also, if you follow the design-first approach that we discussed in the first post of this series, you’ll probably already have a specification file ready to use.</li>\n<li>\n<strong>Support different learning paths</strong>: some developers like to read all the documentation from top to bottom before writing a single line of code, others will start with the code editor right away. Instead of forcing a learning path, try to embrace the different mindsets and provide tools for everyone: specification files, Postman collections, examples in different programming languages, an easy-to-access API sandbox (i.e. something that doesn’t require installing an Enterprise product and waiting 2 weeks to get a license to make your first API call), a developer portal, tutorials and, why not, video walkthroughs. This might sound overwhelming but pays off in the end. With the proper design done first a lot of this content can be autogenerated from the specification files.</li>\n<li>\n<strong>Document the data structure</strong>: if you can, don’t just add lists of properties to your specs and docs: strive to provide proper descriptions of the fields that your API uses so that developers that are not fully familiar with your product can understand them. Remove ambiguity from the documentation as much as possible. This can go a long way, as developers can make mental models by understanding the data properly that often leads to better use cases than “<em>just pull some data from this API”.</em>\n</li>\n<li>\n<strong>Use verbs, status codes, and error descriptions properly</strong>: leverage the power of the protocol you are using (i.e. HTTP when using REST APIs) to define how to do things and what responses mean. Proper usage of status codes and good error messages will dramatically reduce the number of requests to your support team. Developers are smart and want to solve problems quickly so if you provide them with the right information to do so, they won’t bother you. Also, if you are properly logging and monitoring your API behavior, it will be easier for your support team to troubleshoot if your errors are not all “500 Internal Server Error” without any other detail.</li>\n</ul>\n<p>Finally,<strong> stay close to the developers</strong>: especially if your API is being used by people external from your organization, it’s incredibly important to be close to them as much as you can to support them, learn and gather feedback on your API and its documentation. Allow everyone that is responsible for designing and engineering your API to be in that feedback loop, so they can share the learnings. Consider creating a community where people can ask questions and can expect quick answers (Slack, Reddit, Stack Overflow, etc.). I’ve made great friendships this way!</p>\n<h3>A few examples</h3>\n<p>There are many great APIs out there. Here is a small, not complete, list of products from other companies that, for one reason or another, are strong examples of what I described in this blog series:</p>\n<ul>\n<li>Microsoft Graph</li>\n<li>Google Cloud</li>\n<li>VirusTotal</li>\n<li>GitHub REST and GraphQL APIs</li>\n</ul>\n<h3>Conclusion</h3>\n<p>And that’s a wrap, thanks for reading! There are more things that I’ve learned that I might share in future posts, but in my opinion, these are the most relevant ones. I hope you found this series useful: looking forward to hearing your feedback and comments!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b94d9e8532f5\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 4 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "developer-experience",
        "development",
        "api",
        "design-patterns",
        "enterprise-software"
      ]
    },
    {
      "title": "Enterprise API design practices: Part 3",
      "pubDate": "2021-02-23 09:59:43",
      "link": "https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/c56c3091c7d",
      "author": "Francesco Vigo",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*jaVdQE1HOhSJrOK1sTNAaQ.jpeg",
      "description": "\n<p>Welcome to the third part of my series on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. Part 1 covered some background context and the importance of design, while the second post was about security and backend protection; this chapter is on optimization and scale.</p>\n<h3>3. Optimize interactions</h3>\n<p>When dealing with scenarios that weren’t anticipated in the initial release of an API (for example when integrating with an external product from a new partner), developers often have to rely on data-driven APIs to extract information from the backend and process it externally. While use-case-driven APIs are generally considered more useful, sometimes there might not be one available that suits the requirements of the novel use case that you must implement.</p>\n<p>By considering the following guidelines when building your data-driven APIs, you can make them easier to consume and more efficient for the backend and the network, improving performance and reducing the operational cost (fewer data transfers, faster and cheaper queries on the DBs, etc.).</p>\n<p>I’ll use an example with sample data. Consider the following data as a representation of your backend database: an imaginary set of alertsthat your Enterprise product detected over time. Instead of just three, imagine the following JSON output with thousands of records:</p>\n<pre>{<br>    \"alerts\" : [<br>        {<br>            \"name\": \"Impossible Travel\",<br>            \"alert_type\": \"Behavior\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Critical\",<br>            \"created\": \"2020-09-27T09:27:33Z\",<br>            \"modified\": \"2020-09-28T14:34:44Z\",<br>            \"alert_id\": \"8493638e-af28-4a83-b1a9-12085fdbf5b3\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Malware Detected\",<br>            \"alert_type\": \"Endpoint\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"High\",<br>            \"created\": \"2020-10-04T11:22:01Z\",<br>            \"modified\": \"2020-10-08T08:45:33Z\",<br>            \"alert_id\": \"b1018a33-e30f-43b9-9d07-c12d42646bbe\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Large Upload\",<br>            \"alert_type\": \"Network\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Low\",<br>            \"created\": \"2020-11-01T07:04:42Z\",<br>            \"modified\": \"2020-12-01T11:13:24Z\",<br>            \"alert_id\": \"c79ed6a8-bba0-4177-a9c5-39e7f95c86f2\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        }<br>    ]<br>}</pre>\n<p>Imagine implementing a simple /v1/alerts REST API endpoint to retrieve the data and you can’t anticipate all the future needs. I recommend considering the following guidelines:</p>\n<ul>\n<li>\n<strong>Filters</strong>: allow your consumers to reduce the result set by offering filtering capabilities in your API on as many fields as possible without stressing the backend too much (if some filters are not indexed it could become expensive, so you must find the right compromise). In the example above, good filters might include: <em>name</em>, <em>severity,</em> <em>alert_type, alert_status, created,</em> and <em>modified. </em>More complicated fields like <em>details</em> and <em>stack</em> might be too expensive for the backend (as they might require full-text search) and you would probably leave them out unless really required.</li>\n<li>\n<strong>Data formats: </strong>be very consistent in how you present and accept data across your API endpoints. This holds true especially for types such as numbers and dates, or complex structures. For example, to represent integers in JSON you can use numbers (i.e. \"fieldname\": 3) or strings (i.e. \"fieldname\": \"3\" ): no matter what you choose, you need to be consistent across all your API endpoints. And you should also use the same format when returning outputs and accepting inputs.</li>\n<li>\n<strong>Dates: </strong>dates and times can be represented in many ways: timestamps (in seconds, milliseconds, microseconds), strings (ISO8601 such as in the example above, or custom formats such as 20200101), with or without time zone information. This can easily become a problem for the developers. Again, the key is consistency: try to accept and return only a single date format (i.e. timestamp in milliseconds or ISO8601) and be explicit about whether you consider time zones or not: usually choosing to do everything in UTC is a good idea because removes reduce ambiguity. <strong>Make sure to document the date formats properly.</strong>\n</li>\n<li>\n<strong>Filter types: </strong>depending on the type of field, you should provide appropriate filters, not just <em>equals</em>. A good example is supporting <em>range filters </em>for dates that, in our example above, allow consumers to retrieve only the alerts created or modified in a specific interval. If some fields are enumerators with a limited number of possible values, it might be useful to support a <em>multi-select</em> filter (i.e. IN): in the example above it should be possible to filter by severity values and include only the High and Critical values using a single API call.</li>\n<li>\n<strong>Sorting: </strong>is your API consumer interested only in the older alerts or the newest? Supporting sorting in your data-driven API is extremely important. One field to sort by is generally enough, but sometimes (depending on the data) you might need more.</li>\n<li>\n<strong>Result limiting and pagination</strong>: you can’t expect all the entries to be returned at once (and your clients might not be interested or ready to ingest all of them anyway), so you should implement some logic where clients should retrieve a limited number of results and can get more when they need. If you are using pagination, clients should be able to specify the page size within a maximum allowed value. Defaults and maximums should be reasonable and properly documented.</li>\n<li>\n<strong>Field limiting</strong>: consider whether you really need to return all the fields of your results all the time, or if your clients usually just need a few. By letting the client decide what fields (or groups of fields) your API should return, you can reduce the network throughput and backend cost, and performance. You should provide and document some sane default. In the example above, you could decide to return by default all the fields except details and evidence, which can be requested by the client only if they explicitly ask, using an include parameter.</li>\n</ul>\n<p>Let’s put it all together. In the above example, you should be able to retrieve, using a single API call, something like this:</p>\n<blockquote>Up to 100 alerts that were created between 2020–04–01T00:00:00Z (April 1st 2020) and 2020–10–01T00:00:00Z (October 1st 2020) with severity “medium” or “high”, sorted by “modified” date, newest first, including all the fields but “evidence”.</blockquote>\n<p>There are multiple ways you can implement this; through REST, GraphQL, or custom query languages: in many cases, you don’t need something too complex as often data sets are fairly simple. The proper way depends on many design considerations that are outside the scope of this post. But having some, or most of these capabilities in your API will make it better and more future proof. By the way, if you’re into GraphQL, I recommend reading this post.</p>\n<h3>4. Plan for scale</h3>\n<p>A good API shouldn’t be presumptuous: it shouldn’t expect that clients are not doing anything else than waiting for its response, especially at scale, where performance is key.</p>\n<p>If your API requires more than a few milliseconds to produce a response, I recommend considering supporting jobs instead. The logic can be as follows:</p>\n<ul>\n<li>Implement an API endpoint to start an operation that is supposed to take some time. If accepted, it would return immediately with a <em>jobId</em><strong><em>.</em></strong>\n</li>\n<li>The client stores the jobId and periodically reaches out to a second endpoint that, when provided with the jobId, returns the completion status of the job (i.e. <em>running</em>, <em>completed</em>, <em>failed)</em>.</li>\n<li>Once results are available (or <em>some</em> are), the client can invoke a third endpoint to fetch the results.</li>\n</ul>\n<p>Other possible solutions include publisher/subscriber approaches or pushing data with <em>webhooks</em>, also depending on the size of the result set and the speed requirements<em>.</em> There isn’t a one-size-fits-all solution, but I strongly recommend avoiding a polling logic where API clients are kept waiting for the server to reply while it’s running long jobs in the backend.</p>\n<p>If your need high performance and throughput when in your APIs, consider gRPC, as its binary representation of data using protocol buffers has significant speed advantages over REST.</p>\n<p>Side note: if you want to learn more about REST, GraphQL, webhooks, or gRPC use cases, I recommend starting from this post.</p>\n<p>Finally, other considerations for scale include supporting batch operations on multiple entries at the same time (for example <em>mass updates</em>), but I recommend considering them only when you have a real use case in mind.</p>\n<h3>What’s next</h3>\n<p>In the next and last chapter, I’ll share some more suggestions about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c56c3091c7d\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 3 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<p>Welcome to the third part of my series on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. Part 1 covered some background context and the importance of design, while the second post was about security and backend protection; this chapter is on optimization and scale.</p>\n<h3>3. Optimize interactions</h3>\n<p>When dealing with scenarios that weren’t anticipated in the initial release of an API (for example when integrating with an external product from a new partner), developers often have to rely on data-driven APIs to extract information from the backend and process it externally. While use-case-driven APIs are generally considered more useful, sometimes there might not be one available that suits the requirements of the novel use case that you must implement.</p>\n<p>By considering the following guidelines when building your data-driven APIs, you can make them easier to consume and more efficient for the backend and the network, improving performance and reducing the operational cost (fewer data transfers, faster and cheaper queries on the DBs, etc.).</p>\n<p>I’ll use an example with sample data. Consider the following data as a representation of your backend database: an imaginary set of alertsthat your Enterprise product detected over time. Instead of just three, imagine the following JSON output with thousands of records:</p>\n<pre>{<br>    \"alerts\" : [<br>        {<br>            \"name\": \"Impossible Travel\",<br>            \"alert_type\": \"Behavior\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Critical\",<br>            \"created\": \"2020-09-27T09:27:33Z\",<br>            \"modified\": \"2020-09-28T14:34:44Z\",<br>            \"alert_id\": \"8493638e-af28-4a83-b1a9-12085fdbf5b3\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Malware Detected\",<br>            \"alert_type\": \"Endpoint\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"High\",<br>            \"created\": \"2020-10-04T11:22:01Z\",<br>            \"modified\": \"2020-10-08T08:45:33Z\",<br>            \"alert_id\": \"b1018a33-e30f-43b9-9d07-c12d42646bbe\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Large Upload\",<br>            \"alert_type\": \"Network\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Low\",<br>            \"created\": \"2020-11-01T07:04:42Z\",<br>            \"modified\": \"2020-12-01T11:13:24Z\",<br>            \"alert_id\": \"c79ed6a8-bba0-4177-a9c5-39e7f95c86f2\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        }<br>    ]<br>}</pre>\n<p>Imagine implementing a simple /v1/alerts REST API endpoint to retrieve the data and you can’t anticipate all the future needs. I recommend considering the following guidelines:</p>\n<ul>\n<li>\n<strong>Filters</strong>: allow your consumers to reduce the result set by offering filtering capabilities in your API on as many fields as possible without stressing the backend too much (if some filters are not indexed it could become expensive, so you must find the right compromise). In the example above, good filters might include: <em>name</em>, <em>severity,</em> <em>alert_type, alert_status, created,</em> and <em>modified. </em>More complicated fields like <em>details</em> and <em>stack</em> might be too expensive for the backend (as they might require full-text search) and you would probably leave them out unless really required.</li>\n<li>\n<strong>Data formats: </strong>be very consistent in how you present and accept data across your API endpoints. This holds true especially for types such as numbers and dates, or complex structures. For example, to represent integers in JSON you can use numbers (i.e. \"fieldname\": 3) or strings (i.e. \"fieldname\": \"3\" ): no matter what you choose, you need to be consistent across all your API endpoints. And you should also use the same format when returning outputs and accepting inputs.</li>\n<li>\n<strong>Dates: </strong>dates and times can be represented in many ways: timestamps (in seconds, milliseconds, microseconds), strings (ISO8601 such as in the example above, or custom formats such as 20200101), with or without time zone information. This can easily become a problem for the developers. Again, the key is consistency: try to accept and return only a single date format (i.e. timestamp in milliseconds or ISO8601) and be explicit about whether you consider time zones or not: usually choosing to do everything in UTC is a good idea because removes reduce ambiguity. <strong>Make sure to document the date formats properly.</strong>\n</li>\n<li>\n<strong>Filter types: </strong>depending on the type of field, you should provide appropriate filters, not just <em>equals</em>. A good example is supporting <em>range filters </em>for dates that, in our example above, allow consumers to retrieve only the alerts created or modified in a specific interval. If some fields are enumerators with a limited number of possible values, it might be useful to support a <em>multi-select</em> filter (i.e. IN): in the example above it should be possible to filter by severity values and include only the High and Critical values using a single API call.</li>\n<li>\n<strong>Sorting: </strong>is your API consumer interested only in the older alerts or the newest? Supporting sorting in your data-driven API is extremely important. One field to sort by is generally enough, but sometimes (depending on the data) you might need more.</li>\n<li>\n<strong>Result limiting and pagination</strong>: you can’t expect all the entries to be returned at once (and your clients might not be interested or ready to ingest all of them anyway), so you should implement some logic where clients should retrieve a limited number of results and can get more when they need. If you are using pagination, clients should be able to specify the page size within a maximum allowed value. Defaults and maximums should be reasonable and properly documented.</li>\n<li>\n<strong>Field limiting</strong>: consider whether you really need to return all the fields of your results all the time, or if your clients usually just need a few. By letting the client decide what fields (or groups of fields) your API should return, you can reduce the network throughput and backend cost, and performance. You should provide and document some sane default. In the example above, you could decide to return by default all the fields except details and evidence, which can be requested by the client only if they explicitly ask, using an include parameter.</li>\n</ul>\n<p>Let’s put it all together. In the above example, you should be able to retrieve, using a single API call, something like this:</p>\n<blockquote>Up to 100 alerts that were created between 2020–04–01T00:00:00Z (April 1st 2020) and 2020–10–01T00:00:00Z (October 1st 2020) with severity “medium” or “high”, sorted by “modified” date, newest first, including all the fields but “evidence”.</blockquote>\n<p>There are multiple ways you can implement this; through REST, GraphQL, or custom query languages: in many cases, you don’t need something too complex as often data sets are fairly simple. The proper way depends on many design considerations that are outside the scope of this post. But having some, or most of these capabilities in your API will make it better and more future proof. By the way, if you’re into GraphQL, I recommend reading this post.</p>\n<h3>4. Plan for scale</h3>\n<p>A good API shouldn’t be presumptuous: it shouldn’t expect that clients are not doing anything else than waiting for its response, especially at scale, where performance is key.</p>\n<p>If your API requires more than a few milliseconds to produce a response, I recommend considering supporting jobs instead. The logic can be as follows:</p>\n<ul>\n<li>Implement an API endpoint to start an operation that is supposed to take some time. If accepted, it would return immediately with a <em>jobId</em><strong><em>.</em></strong>\n</li>\n<li>The client stores the jobId and periodically reaches out to a second endpoint that, when provided with the jobId, returns the completion status of the job (i.e. <em>running</em>, <em>completed</em>, <em>failed)</em>.</li>\n<li>Once results are available (or <em>some</em> are), the client can invoke a third endpoint to fetch the results.</li>\n</ul>\n<p>Other possible solutions include publisher/subscriber approaches or pushing data with <em>webhooks</em>, also depending on the size of the result set and the speed requirements<em>.</em> There isn’t a one-size-fits-all solution, but I strongly recommend avoiding a polling logic where API clients are kept waiting for the server to reply while it’s running long jobs in the backend.</p>\n<p>If your need high performance and throughput when in your APIs, consider gRPC, as its binary representation of data using protocol buffers has significant speed advantages over REST.</p>\n<p>Side note: if you want to learn more about REST, GraphQL, webhooks, or gRPC use cases, I recommend starting from this post.</p>\n<p>Finally, other considerations for scale include supporting batch operations on multiple entries at the same time (for example <em>mass updates</em>), but I recommend considering them only when you have a real use case in mind.</p>\n<h3>What’s next</h3>\n<p>In the next and last chapter, I’ll share some more suggestions about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c56c3091c7d\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 3 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "api-design",
        "enterprise-software",
        "design-patterns",
        "development",
        "api"
      ]
    },
    {
      "title": "Enterprise API design practices: Part 2",
      "pubDate": "2021-02-23 09:53:10",
      "link": "https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/b316ca0c9724",
      "author": "Francesco Vigo",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*0DRNtTfd-C3bcw4NnpmIaw.jpeg",
      "description": "\n<p>Welcome to the second part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. In the first post, I covered some background context and the importance of design; this chapter is about security and protecting your backend.</p>\n<h3>1. Secure your APIs</h3>\n<p>Most of the projects I’ve been involved with revolve around Cybersecurity, but I don’t consider myself biased when I say that security should be a top priority for every Enterprise API. Misconfigurations and attacks are common and you should go long ways to protect your customers and their data.</p>\n<p>Besides the common table stakes (i.e. use TLS and some form of authentication), when it comes to API security I recommend considering at least the following:</p>\n<ul>\n<li>\n<strong>API Credentials</strong>: OAuth 2 should lead the way but, especially for on-premise products, API Keys are still commonly used. In any case, you should support multiple sets of API credentials in your product to allow for more granularity and proper auditing and compliance. For example, every individual service (or user) that consumes your API should have a dedicated set of credentials, which will make it easier to track down from the audit logs who/what performed an operation and apply proper rate limits.</li>\n<li>\n<strong>API RBAC (Role-Based Access Control)</strong>: another important reason to support multiple sets of API credentials is the ability to assign different <em>permissions</em> to each set. This way your customers won’t give administrative privileges to a set of credentials used, for example, to perform only read-only routine operations. Having RBAC is not enough: you need to guide your customers to use it properly: a good solution is to enforce a flow in your product UI that discourages users from creating API credentials with unnecessary permissions (assigning permissions in an additive way rather than subtractive, for example).</li>\n<li>\n<strong>Credentials shouldn’t last forever</strong>: it’s a good practice to create sets of API credentials that expire at some point or require a manual reauthorization. For how long a credential should stay valid is a good question, as you want to make sure that a business process doesn’t fail because someone forgot to reauthorize something that was set up 6 months ago. There are ways to mitigate this risk in your product UI. I recommend showing a list of the API credential sets (without showing the actual secrets!), when they expire, and how long ago they were used for the last time. This will help customers identify and delete stale credentials and refresh expiring ones.</li>\n</ul>\n<h3>2. Protect your backend</h3>\n<p>Don’t trust your API users: they are likely to break things when given a chance! Not maliciously, of course, but there are so many things that can go wrong when you grant external users/applications access to the heart of your product through an API. And yes, bad guys and Denial of Service attacks exist too.</p>\n<p>Similarly to Product UIs and CLI tools that validate user inputs and behavior, your APIs should do it too. But it goes beyond just making sure that all the required fields are present and correctly typed (if an API doesn’t do it properly, I would file a bug to get it fixed): include <em>cost</em> and <em>performance</em> in your design considerations<em>.</em></p>\n<p>Consider the following examples:</p>\n<ul>\n<li>Example 1: A proprietary API serving data from a Public Cloud database backend: in many cases such backends charge by the amount of data processed in each query. If your API allows consumers to freely search and return unlimited data from the backend, you might end up with an expensive bill.</li>\n<li>Example 2: An on-premise product with an API that allows users to provide files to be scanned for malicious behavior (i.e. a sandbox). A developer makes a mistake and forgets to add a <em>sleep() </em>in a cycle, hammering the API with tens of requests in a very short amount of time. If the API doesn’t protect the product from this situation, it will likely not just slow down the API service itself, but it can probably have significant performance impacts on the server and all the other services running on it, including your product and its UI.</li>\n</ul>\n<p>Both examples are not uncommon for APIs designed exclusively for internal consumption (i.e. when if something goes wrong you can send an IM to the developer that is breaking your API and resolve the problem right away).</p>\n<p>You should consider guidelines to mitigate these risk, such as:</p>\n<ul>\n<li>\n<strong>Shrink the result sets</strong>: try to support granularity when reading data from backends, for example by reducing the number of selected columns and slicing the results by time or other values. Set proper limits with sane defaults that don’t overload the backend. More on this in the next post.</li>\n<li>\n<strong>Rate limits</strong>: API rate limits are a practical way to tell your consumers that they need to slow down. Different consumers, identified with their sets of API credentials, should have their own limits, according to their strategic value, service level agreements, or monetization policy. Also, not all requests are equal: for example, some products implement rate limits using tokens. Each consumer has a limited number of tokens that replenish over time (or cost money) and each API call has a token <em>cost</em> that depends on a number of factors (i.e. impact on the backend load or business value of the service being consumed): once you run out of tokens you need to wait or buy more.</li>\n<li>\n<strong>Communicate errors properly</strong>: when hitting rate limits, API consumers should receive a proper error code from the server (i.e. 429 if using HTTP) and the error message should specify how long until new API calls can be accepted for this client: this won’t just improve DX, but will also allow the clients/SDKs to properly adapt and throttle their requests. Imagine that your API is used by a mobile app and they hit the rate limit: specifying how long they should wait before you are ready to accept a new request allows the app developer to show a popup in the UI to tell the end-user how long to wait. This will not just improve your API’s DX, but will empower the app developers to provide a better U<em>ser Experience</em> (UX) to their customers!</li>\n</ul>\n<h3>What’s next</h3>\n<p>In the next chapter, I’ll dive deep into some technical suggestions on data-driven APIs and scale. In the last part of the series, I’ll share some more suggestions about monitoring your API and supporting the developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b316ca0c9724\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 2 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<p>Welcome to the second part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. In the first post, I covered some background context and the importance of design; this chapter is about security and protecting your backend.</p>\n<h3>1. Secure your APIs</h3>\n<p>Most of the projects I’ve been involved with revolve around Cybersecurity, but I don’t consider myself biased when I say that security should be a top priority for every Enterprise API. Misconfigurations and attacks are common and you should go long ways to protect your customers and their data.</p>\n<p>Besides the common table stakes (i.e. use TLS and some form of authentication), when it comes to API security I recommend considering at least the following:</p>\n<ul>\n<li>\n<strong>API Credentials</strong>: OAuth 2 should lead the way but, especially for on-premise products, API Keys are still commonly used. In any case, you should support multiple sets of API credentials in your product to allow for more granularity and proper auditing and compliance. For example, every individual service (or user) that consumes your API should have a dedicated set of credentials, which will make it easier to track down from the audit logs who/what performed an operation and apply proper rate limits.</li>\n<li>\n<strong>API RBAC (Role-Based Access Control)</strong>: another important reason to support multiple sets of API credentials is the ability to assign different <em>permissions</em> to each set. This way your customers won’t give administrative privileges to a set of credentials used, for example, to perform only read-only routine operations. Having RBAC is not enough: you need to guide your customers to use it properly: a good solution is to enforce a flow in your product UI that discourages users from creating API credentials with unnecessary permissions (assigning permissions in an additive way rather than subtractive, for example).</li>\n<li>\n<strong>Credentials shouldn’t last forever</strong>: it’s a good practice to create sets of API credentials that expire at some point or require a manual reauthorization. For how long a credential should stay valid is a good question, as you want to make sure that a business process doesn’t fail because someone forgot to reauthorize something that was set up 6 months ago. There are ways to mitigate this risk in your product UI. I recommend showing a list of the API credential sets (without showing the actual secrets!), when they expire, and how long ago they were used for the last time. This will help customers identify and delete stale credentials and refresh expiring ones.</li>\n</ul>\n<h3>2. Protect your backend</h3>\n<p>Don’t trust your API users: they are likely to break things when given a chance! Not maliciously, of course, but there are so many things that can go wrong when you grant external users/applications access to the heart of your product through an API. And yes, bad guys and Denial of Service attacks exist too.</p>\n<p>Similarly to Product UIs and CLI tools that validate user inputs and behavior, your APIs should do it too. But it goes beyond just making sure that all the required fields are present and correctly typed (if an API doesn’t do it properly, I would file a bug to get it fixed): include <em>cost</em> and <em>performance</em> in your design considerations<em>.</em></p>\n<p>Consider the following examples:</p>\n<ul>\n<li>Example 1: A proprietary API serving data from a Public Cloud database backend: in many cases such backends charge by the amount of data processed in each query. If your API allows consumers to freely search and return unlimited data from the backend, you might end up with an expensive bill.</li>\n<li>Example 2: An on-premise product with an API that allows users to provide files to be scanned for malicious behavior (i.e. a sandbox). A developer makes a mistake and forgets to add a <em>sleep() </em>in a cycle, hammering the API with tens of requests in a very short amount of time. If the API doesn’t protect the product from this situation, it will likely not just slow down the API service itself, but it can probably have significant performance impacts on the server and all the other services running on it, including your product and its UI.</li>\n</ul>\n<p>Both examples are not uncommon for APIs designed exclusively for internal consumption (i.e. when if something goes wrong you can send an IM to the developer that is breaking your API and resolve the problem right away).</p>\n<p>You should consider guidelines to mitigate these risk, such as:</p>\n<ul>\n<li>\n<strong>Shrink the result sets</strong>: try to support granularity when reading data from backends, for example by reducing the number of selected columns and slicing the results by time or other values. Set proper limits with sane defaults that don’t overload the backend. More on this in the next post.</li>\n<li>\n<strong>Rate limits</strong>: API rate limits are a practical way to tell your consumers that they need to slow down. Different consumers, identified with their sets of API credentials, should have their own limits, according to their strategic value, service level agreements, or monetization policy. Also, not all requests are equal: for example, some products implement rate limits using tokens. Each consumer has a limited number of tokens that replenish over time (or cost money) and each API call has a token <em>cost</em> that depends on a number of factors (i.e. impact on the backend load or business value of the service being consumed): once you run out of tokens you need to wait or buy more.</li>\n<li>\n<strong>Communicate errors properly</strong>: when hitting rate limits, API consumers should receive a proper error code from the server (i.e. 429 if using HTTP) and the error message should specify how long until new API calls can be accepted for this client: this won’t just improve DX, but will also allow the clients/SDKs to properly adapt and throttle their requests. Imagine that your API is used by a mobile app and they hit the rate limit: specifying how long they should wait before you are ready to accept a new request allows the app developer to show a popup in the UI to tell the end-user how long to wait. This will not just improve your API’s DX, but will empower the app developers to provide a better U<em>ser Experience</em> (UX) to their customers!</li>\n</ul>\n<h3>What’s next</h3>\n<p>In the next chapter, I’ll dive deep into some technical suggestions on data-driven APIs and scale. In the last part of the series, I’ll share some more suggestions about monitoring your API and supporting the developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b316ca0c9724\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 2 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "api-security",
        "design-patterns",
        "enterprise-software",
        "api",
        "development"
      ]
    },
    {
      "title": "Enterprise API design practices: Part 1",
      "pubDate": "2021-02-23 09:48:22",
      "link": "https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/1c5a47717ed0",
      "author": "Francesco Vigo",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/0*UCzxQTkyzFZIc_ah.jpeg",
      "description": "\n<p>In this series, I will share some guidelines on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. In this initial post, I will cover some background context and a very important lesson, before getting more technical in parts two, three, and four.</p>\n<p>If you’re wondering why this matters, I recommend reading about the API Economy.</p>\n<p>In the last few years, thanks to my role as a Principal Solutions Architect at Palo Alto Networks, I’ve <em>dealt with</em> a lot of APIs. Most of these APIs were offered by Enterprise companies, predominately in Cybersecurity. By <em>dealt with </em>I mean that I’ve either written code to integrate with the APIs myself, or I’ve helped other developers do it. During some of these integrations, I had the privilege of discussing issues and potential improvements with the Engineers and Product Managers responsible for product APIs. Meeting with these stakeholders and providing feedback often lead to API improvements and a stronger partnership.</p>\n<p>In this blog post, I will share some of the things I’ve learned about good API design that can lead to better business outcomes and fewer headaches for API builders and consumers, including better Developer Experience (DX) and more efficient operations.</p>\n<blockquote>\n<em>There are tons of great resources out there about API best practices (such as the</em><em> Google Cloud API Design Guide</em><em>) and this post is not meant to be a comprehensive list, but rather present a set of common patterns that I ran into. Places where I usually find very interesting content are: </em><em>APIs you won’t hate</em><em> and </em><em>Nordic APIs</em><em>.</em>\n</blockquote>\n<h3>Background context</h3>\n<p>Before sharing the lessons I’ve learned, let me explain the context where most of my experience is based upon:</p>\n<ul>\n<li>\n<strong>B2B</strong>: Enterprise on-premise or SaaS products integrating with other Enterprise products (by partners) or custom applications/scripts (by customers).</li>\n<li>\n<strong>Top-down</strong>: some business mandate requires integrating with a specific product (i.e. because a joint customer was requesting an integration between two vendors), without the developers being able to choose an alternative that provided a better Developer Experience (DX). Sometimes we really had to bite the bullet and power through sub-optimal DX.</li>\n</ul>\n<p>And in most cases we had the following goals and constraints:</p>\n<ul>\n<li>\n<strong>Speed</strong>: build something quickly and move on to the next project. That’s how many Enterprise companies work when it comes to technology partnerships. While this doesn’t sound ideal, there are several good business reasons: sales objection handling, customer acquisition, marketing, etc. You know that only a few ideas can be successful: similarly, many product integrations don’t move the needle, so it’s good to test many hypotheses quickly and invest properly only in the few that are worth it.</li>\n<li>\n<strong>Build with what we have</strong>: because of the above requirement, we rarely had the opportunity to request API or Product changes before we had to ship something.</li>\n<li>\n<strong>Create value</strong>: doing things quickly doesn’t mean you need to check a box that provides no value to customers. Even when the integration use cases were limited in scope, we always had an outcome in mind that could benefit some end-user.</li>\n<li>\n<strong>Performance</strong>: don’t create things that create performance issues on the APIs or, even worse, on the backing products. Or that could end up in significantly higher public cloud spending because they trigger expensive, non-optimized backend queries. APIs should protect themselves and the backing products from misuse but, in my experience, it’s not always the case, especially with on-premise products.</li>\n</ul>\n<p>While I believe that a different context could lead to different conclusions, I think that the <em>guidelines</em> that follow are valid for most scenarios. Let’s begin with the most disruptive one.</p>\n<h3>0. Design first (if you can… but you really should)</h3>\n<p>Although it might sound like preaching to the choir, my first recommendation is to adopt the design-first approach when creating APIs. This also (especially!) holds true for Enterprise products even though it may sound hard because APIs are too often not perceived as first-class components of a product (or as products by themselves). I’ve seen a lot of different scenarios, including:</p>\n<ul>\n<li>APIs created to decouple the client logic from the server backend, without thinking about any other consumer than the product UI itself (a good example of a great idea made much less effective).</li>\n<li>APIs that basically just exposed the underlying database with some authentication in front of it.</li>\n<li>APIs created only to satisfy a very narrow/specific feature request coming from a customer and evolved in the same, very tactical, way, feature by feature.</li>\n<li>APIs meant to be internal-only that were later exposed to external consumers because of a business request. By the way, this breaks the so-called Bezos API Mandate and is usually a primary cause of technical debt.</li>\n</ul>\n<p>The examples above often performed poorly when we tried to consume those APIs in real-world integration use cases, leading to poor DX and reliance on client-side logic to work around the problems, which many times translated into lower quality results and developer fatigue (more on this in part 4).</p>\n<p>But such approaches aren’t necessarily wrong: arguably you can’t really know all the potential use cases of an API when you ship its first iteration and you gotta start somewhere but, <em>if you understand that you don’t know what you don’t know</em>, you can at least mitigate the risks. And the easiest and cheapest way to mitigate risks is to do it earlier on, hence I strongly recommend to embrace a <strong>design-first approach</strong> when creating APIs.</p>\n<p>However, if you really want or are forced to follow a <em>code-first approach</em>, you can still apply most of the guidelines presented in this series as you develop your APIs.</p>\n<h3>What’s next</h3>\n<p>In the next post, I’ll cover some security and backend considerations. In part three I’ll dive into some more technical considerations about data-driven APIs and scale. Finally, in the last chapter, I’ll present some more guidelines about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1c5a47717ed0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 1 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<p>In this series, I will share some guidelines on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. In this initial post, I will cover some background context and a very important lesson, before getting more technical in parts two, three, and four.</p>\n<p>If you’re wondering why this matters, I recommend reading about the API Economy.</p>\n<p>In the last few years, thanks to my role as a Principal Solutions Architect at Palo Alto Networks, I’ve <em>dealt with</em> a lot of APIs. Most of these APIs were offered by Enterprise companies, predominately in Cybersecurity. By <em>dealt with </em>I mean that I’ve either written code to integrate with the APIs myself, or I’ve helped other developers do it. During some of these integrations, I had the privilege of discussing issues and potential improvements with the Engineers and Product Managers responsible for product APIs. Meeting with these stakeholders and providing feedback often lead to API improvements and a stronger partnership.</p>\n<p>In this blog post, I will share some of the things I’ve learned about good API design that can lead to better business outcomes and fewer headaches for API builders and consumers, including better Developer Experience (DX) and more efficient operations.</p>\n<blockquote>\n<em>There are tons of great resources out there about API best practices (such as the</em><em> Google Cloud API Design Guide</em><em>) and this post is not meant to be a comprehensive list, but rather present a set of common patterns that I ran into. Places where I usually find very interesting content are: </em><em>APIs you won’t hate</em><em> and </em><em>Nordic APIs</em><em>.</em>\n</blockquote>\n<h3>Background context</h3>\n<p>Before sharing the lessons I’ve learned, let me explain the context where most of my experience is based upon:</p>\n<ul>\n<li>\n<strong>B2B</strong>: Enterprise on-premise or SaaS products integrating with other Enterprise products (by partners) or custom applications/scripts (by customers).</li>\n<li>\n<strong>Top-down</strong>: some business mandate requires integrating with a specific product (i.e. because a joint customer was requesting an integration between two vendors), without the developers being able to choose an alternative that provided a better Developer Experience (DX). Sometimes we really had to bite the bullet and power through sub-optimal DX.</li>\n</ul>\n<p>And in most cases we had the following goals and constraints:</p>\n<ul>\n<li>\n<strong>Speed</strong>: build something quickly and move on to the next project. That’s how many Enterprise companies work when it comes to technology partnerships. While this doesn’t sound ideal, there are several good business reasons: sales objection handling, customer acquisition, marketing, etc. You know that only a few ideas can be successful: similarly, many product integrations don’t move the needle, so it’s good to test many hypotheses quickly and invest properly only in the few that are worth it.</li>\n<li>\n<strong>Build with what we have</strong>: because of the above requirement, we rarely had the opportunity to request API or Product changes before we had to ship something.</li>\n<li>\n<strong>Create value</strong>: doing things quickly doesn’t mean you need to check a box that provides no value to customers. Even when the integration use cases were limited in scope, we always had an outcome in mind that could benefit some end-user.</li>\n<li>\n<strong>Performance</strong>: don’t create things that create performance issues on the APIs or, even worse, on the backing products. Or that could end up in significantly higher public cloud spending because they trigger expensive, non-optimized backend queries. APIs should protect themselves and the backing products from misuse but, in my experience, it’s not always the case, especially with on-premise products.</li>\n</ul>\n<p>While I believe that a different context could lead to different conclusions, I think that the <em>guidelines</em> that follow are valid for most scenarios. Let’s begin with the most disruptive one.</p>\n<h3>0. Design first (if you can… but you really should)</h3>\n<p>Although it might sound like preaching to the choir, my first recommendation is to adopt the design-first approach when creating APIs. This also (especially!) holds true for Enterprise products even though it may sound hard because APIs are too often not perceived as first-class components of a product (or as products by themselves). I’ve seen a lot of different scenarios, including:</p>\n<ul>\n<li>APIs created to decouple the client logic from the server backend, without thinking about any other consumer than the product UI itself (a good example of a great idea made much less effective).</li>\n<li>APIs that basically just exposed the underlying database with some authentication in front of it.</li>\n<li>APIs created only to satisfy a very narrow/specific feature request coming from a customer and evolved in the same, very tactical, way, feature by feature.</li>\n<li>APIs meant to be internal-only that were later exposed to external consumers because of a business request. By the way, this breaks the so-called Bezos API Mandate and is usually a primary cause of technical debt.</li>\n</ul>\n<p>The examples above often performed poorly when we tried to consume those APIs in real-world integration use cases, leading to poor DX and reliance on client-side logic to work around the problems, which many times translated into lower quality results and developer fatigue (more on this in part 4).</p>\n<p>But such approaches aren’t necessarily wrong: arguably you can’t really know all the potential use cases of an API when you ship its first iteration and you gotta start somewhere but, <em>if you understand that you don’t know what you don’t know</em>, you can at least mitigate the risks. And the easiest and cheapest way to mitigate risks is to do it earlier on, hence I strongly recommend to embrace a <strong>design-first approach</strong> when creating APIs.</p>\n<p>However, if you really want or are forced to follow a <em>code-first approach</em>, you can still apply most of the guidelines presented in this series as you develop your APIs.</p>\n<h3>What’s next</h3>\n<p>In the next post, I’ll cover some security and backend considerations. In part three I’ll dive into some more technical considerations about data-driven APIs and scale. Finally, in the last chapter, I’ll present some more guidelines about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1c5a47717ed0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Enterprise API design practices: Part 1 was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "api-economy",
        "api",
        "enterprise-software",
        "development",
        "developer-experience"
      ]
    },
    {
      "title": "Fingerprinting SSL Servers using JARM and Python",
      "pubDate": "2021-01-29 17:26:48",
      "link": "https://medium.com/palo-alto-networks-developer-blog/fingerprinting-ssl-servers-using-jarm-and-python-6d03f6d38dec?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/6d03f6d38dec",
      "author": "Andrew Scott",
      "thumbnail": "https://cdn-images-1.medium.com/max/668/1*INRqNCAsDsVqMadUX1n80A.png",
      "description": "\n<h4>Introducing pyJARM, the simple way to use JARM with Python</h4>\n<p>In early November 2020, SalesForce announced a new active TLS fingerprinting tool, inexplicably named JARM. The premise of JARM is that if a target server is scanned using a variety of different custom-crafted TCP “Client hello” packets, the resulting responses can be combined and hashed to provide a fingerprint that is indicative of a shared or similar application or server stack.</p>\n<p>Building on this basic premise, it’s asserted that different companies, platforms, or individual products will present similar fingerprints and therefore can allow researchers and security teams to identify possible C2 servers or group hosts with similar SSL configurations.</p>\n<h4>JARM Use Cases</h4>\n<p>I believe there are many interesting use-cases for JARM, and once proven there are opportunities to add JARM to a number of existing products and research initiatives. A short list of these use-cases include:</p>\n<ol>\n<li>Identifying malicious servers or malware C2 servers on the internet</li>\n<li>Identifying hosts owned by an organization that may not be using an approved or default configuration</li>\n<li>Grouping hosts that have similar configurations</li>\n<li>Identifying hosts that are/are not behind a firewall or WAF</li>\n</ol>\n<h4>How JARM Works</h4>\n<p>JARM works by crafting 10 custom TCP “Client hello” packets which it sends to the target server and records the responses. Each response is then parsed into a three piece record, with each piece being separated by a pipe. These raw results look like c02b|0303|h2|0017-0010. All of these raw scan results are combined into a single comma-separated string and hashed.</p>\n<p>The hash itself is a custom fuzzy hash that produces a 62 character fingerprint. The first 30 characters are made up of the cipher and TLS version chosen by the server for each of the 10 client hello’s sent (i.e. 3 characters per scan). The remaining 32 characters are a truncated SHA256 hash of the “Server hello” extensions returned by the server. A JARM fingerprint that is all 0’s indicates that the server did not respond.</p>\n<h4>Introducing pyJARM</h4>\n<p>When I saw the initial implementation of JARM in Python that was delivered by the Salesforce team I was happy that they’d made it available and open sourced it. However, with my developer hat on, when I looked at the code I couldn’t help but think that it could be more usable. I worked with Francesco Vigo, a colleague at Palo Alto Networks, to rewrite the original implementation into a more convenient library, known as pyJARM.</p>\n<p>We also proceeded to add some additional features to pyJARM that we thought would be convenient for developers and researchers, such as proxy and async support. We also made pyJARM available to install via pip and open-sourced it under the ISC license.</p>\n<h4>Using pyJARM</h4>\n<p>Installation of pyJARM is simple and can be accomplished with a simple pip install pyjarm using any machine with access to pypi and Python 3.7+ installed.</p>\n<p>pyJARM can be run either as a command line tool or utilized programatically. When running as a command line tool it can accept a single target, or a file with a list of inputs. It can output to stdout or write directly to a csv file.</p>\n<p>You can also use pyJARM programmatically by importing the Scanner class and calling Scanner.scan with an IP/Domain and port number.</p>\n<h4>What’s Next?</h4>\n<p>We’re eager to see wider adoption of pyJARM and JARM generally in the infosec community and within vendor tools and solutions. If you have any questions, issues, or enhancements for pyJARM, we’d encourage you to open an Issue or PR on Github.</p>\n<p>PaloAltoNetworks/pyjarm</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d03f6d38dec\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Fingerprinting SSL Servers using JARM and Python was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<h4>Introducing pyJARM, the simple way to use JARM with Python</h4>\n<p>In early November 2020, SalesForce announced a new active TLS fingerprinting tool, inexplicably named JARM. The premise of JARM is that if a target server is scanned using a variety of different custom-crafted TCP “Client hello” packets, the resulting responses can be combined and hashed to provide a fingerprint that is indicative of a shared or similar application or server stack.</p>\n<p>Building on this basic premise, it’s asserted that different companies, platforms, or individual products will present similar fingerprints and therefore can allow researchers and security teams to identify possible C2 servers or group hosts with similar SSL configurations.</p>\n<h4>JARM Use Cases</h4>\n<p>I believe there are many interesting use-cases for JARM, and once proven there are opportunities to add JARM to a number of existing products and research initiatives. A short list of these use-cases include:</p>\n<ol>\n<li>Identifying malicious servers or malware C2 servers on the internet</li>\n<li>Identifying hosts owned by an organization that may not be using an approved or default configuration</li>\n<li>Grouping hosts that have similar configurations</li>\n<li>Identifying hosts that are/are not behind a firewall or WAF</li>\n</ol>\n<h4>How JARM Works</h4>\n<p>JARM works by crafting 10 custom TCP “Client hello” packets which it sends to the target server and records the responses. Each response is then parsed into a three piece record, with each piece being separated by a pipe. These raw results look like c02b|0303|h2|0017-0010. All of these raw scan results are combined into a single comma-separated string and hashed.</p>\n<p>The hash itself is a custom fuzzy hash that produces a 62 character fingerprint. The first 30 characters are made up of the cipher and TLS version chosen by the server for each of the 10 client hello’s sent (i.e. 3 characters per scan). The remaining 32 characters are a truncated SHA256 hash of the “Server hello” extensions returned by the server. A JARM fingerprint that is all 0’s indicates that the server did not respond.</p>\n<h4>Introducing pyJARM</h4>\n<p>When I saw the initial implementation of JARM in Python that was delivered by the Salesforce team I was happy that they’d made it available and open sourced it. However, with my developer hat on, when I looked at the code I couldn’t help but think that it could be more usable. I worked with Francesco Vigo, a colleague at Palo Alto Networks, to rewrite the original implementation into a more convenient library, known as pyJARM.</p>\n<p>We also proceeded to add some additional features to pyJARM that we thought would be convenient for developers and researchers, such as proxy and async support. We also made pyJARM available to install via pip and open-sourced it under the ISC license.</p>\n<h4>Using pyJARM</h4>\n<p>Installation of pyJARM is simple and can be accomplished with a simple pip install pyjarm using any machine with access to pypi and Python 3.7+ installed.</p>\n<p>pyJARM can be run either as a command line tool or utilized programatically. When running as a command line tool it can accept a single target, or a file with a list of inputs. It can output to stdout or write directly to a csv file.</p>\n<p>You can also use pyJARM programmatically by importing the Scanner class and calling Scanner.scan with an IP/Domain and port number.</p>\n<h4>What’s Next?</h4>\n<p>We’re eager to see wider adoption of pyJARM and JARM generally in the infosec community and within vendor tools and solutions. If you have any questions, issues, or enhancements for pyJARM, we’d encourage you to open an Issue or PR on Github.</p>\n<p>PaloAltoNetworks/pyjarm</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d03f6d38dec\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Fingerprinting SSL Servers using JARM and Python was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "jarm",
        "python",
        "programming",
        "malware",
        "cybersecurity"
      ]
    },
    {
      "title": "Continuous Scraping of Cloud Services Endpoints",
      "pubDate": "2021-01-25 18:21:54",
      "link": "https://medium.com/palo-alto-networks-developer-blog/continuous-scraping-of-cloud-services-endpoints-978920fd27ca?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/978920fd27ca",
      "author": "Luigi Mori",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*8MiuCQ4qNzvBFPcISAwLKw.png",
      "description": "\n<p><strong><em>TL;DR</em></strong><em>: The addition of GitHub Actions to the already feature rich GitHub platform created an interesting and powerful framework for tracking URLs/IP Ranges used by Cloud Services. Live Example: </em><em>jtschichold/mm-cloud-services-endpoints-test</em></p>\n<p>Not too long after GitHub announced their Continuous Integration framework feature, GitHub Actions, some smart people had the idea of using GitHub Actions scheduling mechanism to periodically scrape web sites and track changes using git.</p>\n<p>If you think for a moment with GitHub Actions you have:</p>\n<ul>\n<li>a scheduling engine to periodically run (almost) arbitrary code (GitHub Actions)</li>\n<li>a storage mechanism (git repo)</li>\n<li>a super powerful change tracking mechanism (git itself)</li>\n<li>a CDN to publish the results of your activities (GitHub)</li>\n<li>a mechanism to interact with users (GitHub Pull Requests)</li>\n<li>a mechanism to notify users (GitHub repo notifications)</li>\n</ul>\n<p><strong>This looks like a perfect fit for tracking low volume, slow changing feed — like list of URLs/IPs used by Cloud Services</strong>. You can already implement trackers in hundreds of ways, but in most architectures some of the requirements listed above require ad-hoc coding, are painful to implement (looking at you, “<em>user interaction</em>”) or expensive (<em>CDN</em>). <strong>With GitHub Actions instead you have all the requirements offered as services in a nice package, ready to use.</strong></p>\n<p>The following lines guide you thru the process of setting up a Cloud Services tracker, using some simple code we implemented to smooth some of the tasks. Thanks to the GitHub Actions Pricing structure you can test this idea for free (as long as you keep your repo public).</p>\n<h3>Quick introduction to GitHub Actions</h3>\n<p>GitHub Actions is the Continuous Integration framework offered by GitHub and tightly integrated with their GitHub platform. GitHub Actions follows the same patterns used by other Continuous Integration services — to configure GitHub Actions:</p>\n<ul>\n<li>create a workflow definition file inside the directory .github/workflows of your GitHub repo, using YAML</li>\n<li>specify in the workflow file one or more list of <em>steps</em> to be executed and when they should be executed (the <em>triggers</em>)</li>\n<li>commit &amp; push</li>\n</ul>\n<p>Let’s look at a real world example to give you a better idea on how GitHub Actions workflows work and how to use them for scraping. The following is a commented excerpt taken from the source of my inspiration — the repo alex/nyt-2020-election-scraper. This workflow has a single <em>job</em> called scheduled that downloads the data of the US2020 Presidential Election and updates local tables, pushing the results back into the repo:</p>\n<p>As you can see the scraping workflow is extremely easy to understand and flexible. For tracking Cloud Services we need some additional steps for processing and double checking the scraping results, but not too many.</p>\n<p>You can find the full documentation with a more in-depth Quickstart on the official GitHub Action webpage: https://docs.github.com/en/actions</p>\n<h3>Tracking Cloud Services</h3>\n<p>To track Cloud Services endpoints we want a GitHub Action workflow looking like this:</p>\n<ol>\n<li>\n<strong>Checkout</strong> the repo</li>\n<li>\n<strong>Fetch</strong> the latest version of list of endpoints used by a Cloud Service from their feed/API/webpage</li>\n<li>\n<strong>Filter</strong> the list (if we want to exclude some IP ranges or URLs)</li>\n<li>\n<strong>Transform</strong> the list in a format we can consume</li>\n<li>\n<strong>Check</strong> the change in the latest version of the list against the one stored in the repo</li>\n<li>\n<strong>Update</strong>. If the change is not <em>big</em>, commit the changes &amp; push. If the change is <em>big</em>, open a PR and ask for manual approval.</li>\n</ol>\n<p><strong>Why do we want check the changes before pushing?</strong> as an additional safeguard against breaking changes. Breaking changes could happen for a variety of reasons, example:</p>\n<ul>\n<li>formatting errors in the endpoint list provided by the Cloud Service</li>\n<li>structural changes in the webpage where the endpoints are listed, this for Cloud Services that provides the endpoint lists on webpages instead of JSON files</li>\n</ul>\n<p>The proposed solution here is to detect <em>big</em> changes (for some definition of <em>big</em>) and ask for manual approval before updating the list if a <em>big</em> change is detected.</p>\n<p>Let’s follow the steps one by one and see how we can implement our workflow. You can find a live example in jtschichold/mm-cloud-services-endpoints-tests</p>\n<h4>1. Checkout the repo</h4>\n<p>This is simple, there is a GitHub Action for that — action/checkout:</p>\n<p>Without arguments, the action checkouts the repo in the current directory from the default branch, or the default branch for the triggering event.</p>\n<h4>2. Fetch</h4>\n<p>Many endpoint lists are published as JSON files and easy to fetch using curl and post process using jq. But not all of them. To simplify this process we have a created a GitHub Action called jtschichold\\mm-cloud-services-miners that can be used to fetch endpoint lists from a number of Cloud Services. The action has been designed to be easy to extend. If you need a Cloud Service that is not supported by the action feel free to open a PR :-)</p>\n<p>We can use the action in the workflow in the following way:</p>\n<p>The config input of the action points to a config file in the repo where the action finds the details about what to fetch and where to save the results:</p>\n<p>The resulting files are JSON files with the list of the select IP Ranges. As an example, public-cloud-azuread-ips.txt looks like this:</p>\n<pre>[<br>  \"13.64.151.161/32\",<br>  \"13.66.141.64/27\",<br>  \"13.67.9.224/27\",<br>...<br>  \"2603:1006:2000::/48\",<br>  \"2603:1007:200::/48\",<br>  \"2603:1016:1400::/48\",<br>  \"2603:1017::/48\",<br>  \"2603:1026:3000::/48\",<br>  \"2603:1027:1::/48\",<br>  \"2603:1036:3000::/48\",<br>  \"2603:1037:1::/48\",<br>  \"2603:1046:2000::/48\",<br>  \"2603:1047:1::/48\",<br>  \"2603:1056:2000::/48\",<br>  \"2603:1057:2::/48\"<br>]</pre>\n<p>You can read the full documentation of the action on the homepage jtschichold/mm-cloud-services-miners</p>\n<h4>3 &amp; 4. Filter &amp; Transform</h4>\n<p>Now we have fetched the list of the endpoints, next step is filtering the lists for extra security and transform the lists in a format that can be consumed.</p>\n<p>We have created 2 GitHub Actions to simplify this step, both of them do filter &amp; transform lists — one for IPs and one for URLs:</p>\n<p>jtschichold/mm-process-ip-list is used to filter IP lists, transform the lists in plain text format and (optionally) aggregate multiple lists. It can filter IP lists:</p>\n<ul>\n<li>to exclude IP ranges overlapping a list of CIDRS read from a file</li>\n<li>to exclude IP ranges with a small subnet mask (like 0.0.0.0/0…)</li>\n<li>to exclude IP ranges overlapping Reserved IP addresses (see Wikipedia)</li>\n</ul>\n<p>jtschichold/mm-process-urls-list is instead used to filter URL lists, transform the lists in other formats and (optionally again) aggregate multiple lists. It can filter URL lists to exclude URLs matching a list of regular expressions read from a file.</p>\n<p>Let’s see how can we use jtschichold/mm-process-ip-list to filter the lists we have created during the fetch step:</p>\n<p>After this step, public-cloud-azuread-ips.txt looks like this:</p>\n<pre>13.64.151.161/32<br>13.66.141.64/27<br>13.67.9.224/27<br>...<br>2603:1006:2000::/48<br>2603:1007:200::/48<br>2603:1016:1400::/48<br>2603:1017::/48<br>2603:1026:3000::/48<br>2603:1027:1::/48<br>2603:1036:3000::/48<br>2603:1037:1::/48<br>2603:1046:2000::/48<br>2603:1047:1::/48<br>2603:1056:2000::/48<br>2603:1057:2::/48</pre>\n<h4>5 &amp; 6. Check &amp; Update</h4>\n<p>Before pushing the update to the repo, we want to make sure the new lists didn’t change <em>much</em>. As git is the perfect tool track versions of text files, we can just use its abilities to compute some metrics over the changes in the new version of the feeds. We have embedded the logic in a GitHub Action, jtschichold/mm-check-changes:</p>\n<p>Now we can use this action to decide if we want to automatically push the changes or rather open a PR to ask for manual approval:</p>\n<h4>Final workflow</h4>\n<h3>Consume the results</h3>\n<p>Nothing new here, thanks to GitHub once the results are pushed into the repo you can directly access the files using the classic GitHub URL schema:</p>\n<pre>https://raw.githubusercontent.com/&lt;repo name&gt;/&lt;branch&gt;/&lt;path to the file&gt;</pre>\n<p>Example from jtschichold/mm-cloud-services-endpoints-test:</p>\n<pre>https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt</pre>\n<p>The URL can be directly used in your scripts or in configuring an EDL on PAN-OS: https://docs.paloaltonetworks.com/pan-os/10-0/pan-os-admin/policy/use-an-external-dynamic-list-in-policy/external-dynamic-list.html</p>\n<h3>Summary</h3>\n<p>In this post we have covered the benefits of implementing a tracker of Cloud Services endpoints using GitHub Actions and how to do it:</p>\n<ul>\n<li>Basic GitHub Actions workflow structure</li>\n<li>How to fetch the endpoints</li>\n<li>How to filter the fetched endpoints for additional security</li>\n<li>How to transform the endpoints in the desired format</li>\n<li>How to prevent breaking changes and ask for manual approval</li>\n</ul>\n<h3>Shut up and give me the code</h3>\n<p>Here you are:</p>\n<ul>\n<li>Live Example: jtschichold/mm-cloud-services-endpoints-test\n</li>\n<li>Fetch action: jtschichold/mm-cloud-services-miners\n</li>\n<li>Filter &amp; Transform action for IPs: jtschichold/mm-process-ip-list\n</li>\n<li>Filter &amp; Transform action for URLs: jtschichold/mm-process-url-list\n</li>\n<li>Check changes action: jtschichold/mm-check-changes\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=978920fd27ca\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Continuous Scraping of Cloud Services Endpoints was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<p><strong><em>TL;DR</em></strong><em>: The addition of GitHub Actions to the already feature rich GitHub platform created an interesting and powerful framework for tracking URLs/IP Ranges used by Cloud Services. Live Example: </em><em>jtschichold/mm-cloud-services-endpoints-test</em></p>\n<p>Not too long after GitHub announced their Continuous Integration framework feature, GitHub Actions, some smart people had the idea of using GitHub Actions scheduling mechanism to periodically scrape web sites and track changes using git.</p>\n<p>If you think for a moment with GitHub Actions you have:</p>\n<ul>\n<li>a scheduling engine to periodically run (almost) arbitrary code (GitHub Actions)</li>\n<li>a storage mechanism (git repo)</li>\n<li>a super powerful change tracking mechanism (git itself)</li>\n<li>a CDN to publish the results of your activities (GitHub)</li>\n<li>a mechanism to interact with users (GitHub Pull Requests)</li>\n<li>a mechanism to notify users (GitHub repo notifications)</li>\n</ul>\n<p><strong>This looks like a perfect fit for tracking low volume, slow changing feed — like list of URLs/IPs used by Cloud Services</strong>. You can already implement trackers in hundreds of ways, but in most architectures some of the requirements listed above require ad-hoc coding, are painful to implement (looking at you, “<em>user interaction</em>”) or expensive (<em>CDN</em>). <strong>With GitHub Actions instead you have all the requirements offered as services in a nice package, ready to use.</strong></p>\n<p>The following lines guide you thru the process of setting up a Cloud Services tracker, using some simple code we implemented to smooth some of the tasks. Thanks to the GitHub Actions Pricing structure you can test this idea for free (as long as you keep your repo public).</p>\n<h3>Quick introduction to GitHub Actions</h3>\n<p>GitHub Actions is the Continuous Integration framework offered by GitHub and tightly integrated with their GitHub platform. GitHub Actions follows the same patterns used by other Continuous Integration services — to configure GitHub Actions:</p>\n<ul>\n<li>create a workflow definition file inside the directory .github/workflows of your GitHub repo, using YAML</li>\n<li>specify in the workflow file one or more list of <em>steps</em> to be executed and when they should be executed (the <em>triggers</em>)</li>\n<li>commit &amp; push</li>\n</ul>\n<p>Let’s look at a real world example to give you a better idea on how GitHub Actions workflows work and how to use them for scraping. The following is a commented excerpt taken from the source of my inspiration — the repo alex/nyt-2020-election-scraper. This workflow has a single <em>job</em> called scheduled that downloads the data of the US2020 Presidential Election and updates local tables, pushing the results back into the repo:</p>\n<p>As you can see the scraping workflow is extremely easy to understand and flexible. For tracking Cloud Services we need some additional steps for processing and double checking the scraping results, but not too many.</p>\n<p>You can find the full documentation with a more in-depth Quickstart on the official GitHub Action webpage: https://docs.github.com/en/actions</p>\n<h3>Tracking Cloud Services</h3>\n<p>To track Cloud Services endpoints we want a GitHub Action workflow looking like this:</p>\n<ol>\n<li>\n<strong>Checkout</strong> the repo</li>\n<li>\n<strong>Fetch</strong> the latest version of list of endpoints used by a Cloud Service from their feed/API/webpage</li>\n<li>\n<strong>Filter</strong> the list (if we want to exclude some IP ranges or URLs)</li>\n<li>\n<strong>Transform</strong> the list in a format we can consume</li>\n<li>\n<strong>Check</strong> the change in the latest version of the list against the one stored in the repo</li>\n<li>\n<strong>Update</strong>. If the change is not <em>big</em>, commit the changes &amp; push. If the change is <em>big</em>, open a PR and ask for manual approval.</li>\n</ol>\n<p><strong>Why do we want check the changes before pushing?</strong> as an additional safeguard against breaking changes. Breaking changes could happen for a variety of reasons, example:</p>\n<ul>\n<li>formatting errors in the endpoint list provided by the Cloud Service</li>\n<li>structural changes in the webpage where the endpoints are listed, this for Cloud Services that provides the endpoint lists on webpages instead of JSON files</li>\n</ul>\n<p>The proposed solution here is to detect <em>big</em> changes (for some definition of <em>big</em>) and ask for manual approval before updating the list if a <em>big</em> change is detected.</p>\n<p>Let’s follow the steps one by one and see how we can implement our workflow. You can find a live example in jtschichold/mm-cloud-services-endpoints-tests</p>\n<h4>1. Checkout the repo</h4>\n<p>This is simple, there is a GitHub Action for that — action/checkout:</p>\n<p>Without arguments, the action checkouts the repo in the current directory from the default branch, or the default branch for the triggering event.</p>\n<h4>2. Fetch</h4>\n<p>Many endpoint lists are published as JSON files and easy to fetch using curl and post process using jq. But not all of them. To simplify this process we have a created a GitHub Action called jtschichold\\mm-cloud-services-miners that can be used to fetch endpoint lists from a number of Cloud Services. The action has been designed to be easy to extend. If you need a Cloud Service that is not supported by the action feel free to open a PR :-)</p>\n<p>We can use the action in the workflow in the following way:</p>\n<p>The config input of the action points to a config file in the repo where the action finds the details about what to fetch and where to save the results:</p>\n<p>The resulting files are JSON files with the list of the select IP Ranges. As an example, public-cloud-azuread-ips.txt looks like this:</p>\n<pre>[<br>  \"13.64.151.161/32\",<br>  \"13.66.141.64/27\",<br>  \"13.67.9.224/27\",<br>...<br>  \"2603:1006:2000::/48\",<br>  \"2603:1007:200::/48\",<br>  \"2603:1016:1400::/48\",<br>  \"2603:1017::/48\",<br>  \"2603:1026:3000::/48\",<br>  \"2603:1027:1::/48\",<br>  \"2603:1036:3000::/48\",<br>  \"2603:1037:1::/48\",<br>  \"2603:1046:2000::/48\",<br>  \"2603:1047:1::/48\",<br>  \"2603:1056:2000::/48\",<br>  \"2603:1057:2::/48\"<br>]</pre>\n<p>You can read the full documentation of the action on the homepage jtschichold/mm-cloud-services-miners</p>\n<h4>3 &amp; 4. Filter &amp; Transform</h4>\n<p>Now we have fetched the list of the endpoints, next step is filtering the lists for extra security and transform the lists in a format that can be consumed.</p>\n<p>We have created 2 GitHub Actions to simplify this step, both of them do filter &amp; transform lists — one for IPs and one for URLs:</p>\n<p>jtschichold/mm-process-ip-list is used to filter IP lists, transform the lists in plain text format and (optionally) aggregate multiple lists. It can filter IP lists:</p>\n<ul>\n<li>to exclude IP ranges overlapping a list of CIDRS read from a file</li>\n<li>to exclude IP ranges with a small subnet mask (like 0.0.0.0/0…)</li>\n<li>to exclude IP ranges overlapping Reserved IP addresses (see Wikipedia)</li>\n</ul>\n<p>jtschichold/mm-process-urls-list is instead used to filter URL lists, transform the lists in other formats and (optionally again) aggregate multiple lists. It can filter URL lists to exclude URLs matching a list of regular expressions read from a file.</p>\n<p>Let’s see how can we use jtschichold/mm-process-ip-list to filter the lists we have created during the fetch step:</p>\n<p>After this step, public-cloud-azuread-ips.txt looks like this:</p>\n<pre>13.64.151.161/32<br>13.66.141.64/27<br>13.67.9.224/27<br>...<br>2603:1006:2000::/48<br>2603:1007:200::/48<br>2603:1016:1400::/48<br>2603:1017::/48<br>2603:1026:3000::/48<br>2603:1027:1::/48<br>2603:1036:3000::/48<br>2603:1037:1::/48<br>2603:1046:2000::/48<br>2603:1047:1::/48<br>2603:1056:2000::/48<br>2603:1057:2::/48</pre>\n<h4>5 &amp; 6. Check &amp; Update</h4>\n<p>Before pushing the update to the repo, we want to make sure the new lists didn’t change <em>much</em>. As git is the perfect tool track versions of text files, we can just use its abilities to compute some metrics over the changes in the new version of the feeds. We have embedded the logic in a GitHub Action, jtschichold/mm-check-changes:</p>\n<p>Now we can use this action to decide if we want to automatically push the changes or rather open a PR to ask for manual approval:</p>\n<h4>Final workflow</h4>\n<h3>Consume the results</h3>\n<p>Nothing new here, thanks to GitHub once the results are pushed into the repo you can directly access the files using the classic GitHub URL schema:</p>\n<pre>https://raw.githubusercontent.com/&lt;repo name&gt;/&lt;branch&gt;/&lt;path to the file&gt;</pre>\n<p>Example from jtschichold/mm-cloud-services-endpoints-test:</p>\n<pre>https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt</pre>\n<p>The URL can be directly used in your scripts or in configuring an EDL on PAN-OS: https://docs.paloaltonetworks.com/pan-os/10-0/pan-os-admin/policy/use-an-external-dynamic-list-in-policy/external-dynamic-list.html</p>\n<h3>Summary</h3>\n<p>In this post we have covered the benefits of implementing a tracker of Cloud Services endpoints using GitHub Actions and how to do it:</p>\n<ul>\n<li>Basic GitHub Actions workflow structure</li>\n<li>How to fetch the endpoints</li>\n<li>How to filter the fetched endpoints for additional security</li>\n<li>How to transform the endpoints in the desired format</li>\n<li>How to prevent breaking changes and ask for manual approval</li>\n</ul>\n<h3>Shut up and give me the code</h3>\n<p>Here you are:</p>\n<ul>\n<li>Live Example: jtschichold/mm-cloud-services-endpoints-test\n</li>\n<li>Fetch action: jtschichold/mm-cloud-services-miners\n</li>\n<li>Filter &amp; Transform action for IPs: jtschichold/mm-process-ip-list\n</li>\n<li>Filter &amp; Transform action for URLs: jtschichold/mm-process-url-list\n</li>\n<li>Check changes action: jtschichold/mm-check-changes\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=978920fd27ca\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Continuous Scraping of Cloud Services Endpoints was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "github-actions",
        "developer-relations",
        "palo-alto-networks",
        "security"
      ]
    },
    {
      "title": "Custom incidents in Cortex XDR",
      "pubDate": "2020-06-05 13:18:21",
      "link": "https://medium.com/palo-alto-networks-developer-blog/custom-incidents-in-cortex-xdr-ef22a05ff753?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/ef22a05ff753",
      "author": "Xavier Homs",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/0*dfFpXD27IHz9SQwO.gif",
      "description": "\n<p>You might have heard that Cortex XDR can boost your Security Operations productivity by learning how your network, endpoints and cloud components behave. Cortex XDR does that by taking into consideration the billions of data points generated by sensors deployed everywhere in your organization.</p>\n<p>As a result, only a small set of incidents that require supervision is generated. Hundreds or even thousands of alerts are conveniently collected inside these incidents for the security engineer to analyze and take action.</p>\n<p>But what’s an incident? Well, there are many angles to that question. I would classify them into the <em>known</em>, <em>custom</em> and <em>third party</em> categories.</p>\n<p>There are, of course, known threats well described by models like the MITRE ATT&amp;CK or malicious pieces of code no-one would like to see running on their endpoints. Cortex XDR provides an out-of-the-box collection of +350 behavior indicators of compromise (BIOCs) as well as analytics engines capable of creating baselines and detecting anomalies. So you’re well covered for the <em>known</em> angle of the response.</p>\n<p>Cortex XDR provides, as well, capabilities to allow Security Engineers model their own BIOCs to raise custom incidents. It allows, for instance, to describe the process of starting a FTP client and allowing it to open PDF files in a small collection of endpoints as an exfiltration attempt. So you’re covered, as well, with the <em>custom</em> angle of the response.</p>\n<p>And, finally, Cortex XDR trust by default High and Medium severity alerts generated by network sensors like the PAN-OS NGFW’s or endpoint sensors like the Cortex XDR Agent. Alerts from third party systems are supported as well. Anyone or anything capable of triggering a High or Medium severity alert is a candidate to generate an incident inside Cortex XDR.</p>\n<h3>The value of Third Party alerts</h3>\n<p>Cortex XDR features a very easy to consume REST API covering many of the product features. One of the API endpoints exposes a POST method that allows the system ingest alerts from third parties.</p>\n<p>Instead of describing the API endpoint I would rather prefer to imagine a use case and walk you through to completing an integration for it.</p>\n<p>Let’s imagine there is a critical system (System-X) in our data center in which we’re not allowed to install a Cortex XDR Agent (endpoint sensor). Imagine, as well, that any successful SSH Login to that system should tigger a Cortex XDR incident to let a Security Engineer report on it. And, finally, imagine that such a system generates syslog messages we can collect in a Linux box with a syntax similar to the following lines</p>\n<pre>Apr  2 09:09:28 mimic sshd[8580]: Accepted password for xhoms from 192.168.1.18 port 49290 ssh2</pre>\n<pre>Apr  2 09:09:28 mimic sshd[8582]: pam_unix(sshd:session): session opened for user xhoms by (uid=0)</pre>\n<p>Our objective for this article will be to parse these lines (in fact the first one) and generate a POST request to trigger a new Cortex XDR incident.</p>\n<h4>How does it looks like without a Third Party alert?</h4>\n<p>Cortex XDR provides a Query Builder feature that allows the security engineer analyze the connection described in the previous syslog message</p>\n<p>It can, for example, look for the endpoint and process responsible for such a connection.</p>\n<p>But, can you answer the question whether the login was successful or not? Which username was used for the connection? Or if the connection used password or public key material? The response is that the target system is better positioned to provide such details. And, in fact, it is doing so in the syslog message we saw before.</p>\n<h3>Cortex XDR API</h3>\n<p>Let’s try to leverage the Cortex XDR API and the syslog message generated by the fictitious System-X described before to trigger an incident with the data we want to present to our security engineers.</p>\n<h4>Insert Parsed Alerts API Endpoint</h4>\n<p>Details for this endpoint are available publicly in the Palo Alto Networks TechDoc site. It is basically a POST request to the URI /public_api/v1/alerts/insert_parsed_alerts/ with a JSON body consisting of an array of objects featuring the following object model.</p>\n<h4>API Authorization Headers</h4>\n<p>Cortex XDR API are protected by a RBAC API Key model. Administrator must create the API Key and assign access privileges to it using the management console.</p>\n<p>For our case (ingestion of alerts) the API Key must have <em>Administrator</em> privileges. Choosing between the <em>Advanced</em> or <em>Standard</em> security level will depend on the client used to POST the message. In the case of a cURL-based script then the right choice is the <em>Standard</em> security level. If you plan to use client libraries like the <em>pan-cortex-xdr-nodejs</em> then you better opt for the <em>Advanced</em> security level.</p>\n<p>Details on how to format the HTTP Authorization headers can be found in the Get Started with Cortex XDR APIs document. But, in summary:</p>\n<ul>\n<li>For <em>Standard</em> security level API Key’s you must set the headers authorization and x-xdr-auth-id</li>\n<li>For <em>Advanced</em> security level API Key’s you must set the headers authorization, x-xdr-auth-id, x-xdr-timestamp and x-xdr-nonce</li>\n</ul>\n<h3>The integration code</h3>\n<p>The tasks we need to complete are the following ones:</p>\n<ol>\n<li>Create a parser function that will receive a syslog message as single argument and return a Cortex XDR compliant parsed object (or null if the line can’t be parsed)</li>\n<li>A context that will instantiate a Cortex XDR API connection (load the API Key material), listen for new syslog messages calling the previous parser every time a new one is received and, finally, push the returned object to the Ingestion Alert endpoint (in case it is not null)</li>\n</ol>\n<p>Hooking this code into a pipeline (i.e. to the syslog receiver) is out of the scope of this article.</p>\n<p>For this example I’ll use TypeScript and leverage the pan-cortex-xdr-nodejs client library.</p>\n<h4>The message parser function</h4>\n<p>This can be easily achieved with a JS RegExp object.</p>\n<pre>// Example message: \"Apr  2 09:09:28 mimic sshd[8580]: Accepted password for xhoms from 192.168.1.18 port 49290 ssh2\"</pre>\n<pre>const msgRegExp = /^(.*[0-9]{2}:[0-9]{2}:[0-9]{2}).*Accepted ([^ ]+) for ([^ ]+) from ([^ ]+) port ([0-9]+) ssh2$/</pre>\n<p>This object contains five capturing groups:</p>\n<ol>\n<li>Date (Apr 2 09:09:28)</li>\n<li>Authentication type (password)</li>\n<li>Username (xhoms)</li>\n<li>Source IP (192.168.1.18) and</li>\n<li>Source Port (49290)</li>\n</ol>\n<p>Let’s leverage this RegExp object and return a valid Cortex XDR Parsed Alert object if the match is successful. Notice this piece of code uses the <em>MomentJS</em> library to parse the Time Date string.</p>\n<h4>The application context</h4>\n<p>We’re only missing a code context to initialize the connection to the XDR API (load the API Key material) and create a stream.Writable object that could be leveraged to build the pipeline. That’s quite easy using the @paloaltonetworks/pan-cortex-xdr NodeJS client library.</p>\n<p>In this case we’re passing, explicitly, the API Key material as options to the createXdrApi factory method. If these options are not passed then the factory method will try to find them in the environmental variables PAN_BASIC_API_KEY (or PAN_ADV_API_KEY), PAN_API_KEY_ID and PAN_XDR_FQDN</p>\n<p>We’re ready to go. We just need to hook this code into the syslog stream and call xdrAlertPipe.write(line) with every message we receive from the System-X</p>\n<h3>Incidents triggered by the integration</h3>\n<p>You might have noticed that our integration is pushing alerts with the Medium severity level. Alerts with levels Informational and Low are considered by Cortex XDR as insights. Alerts with levels Medium and High will be promoted to incidents instead.</p>\n<p>Now, in our XDR dashboard, we have a new incident triggered by our integration code. Analyzing the incident will provide the security engineer with details like username and ssh authentication type.</p>\n<p>The nice thing about Cortex XDR is that it automatically pulls additional information to enrich our incident. In this case, it provides additional insights to the Security Engineer like process being used in the client (putty.exe) parent process, command line arguments and, obviously, all details on the endpoint being used for the connection.</p>\n<h3>Summary</h3>\n<ul>\n<li>In some corner cases (i.e. threat data generated by closed systems) your security engineer might have some blind spots. The Cortex XDR API enables, among many other uses cases, the construction of integrations that can extract information from these systems (i.e. through Syslog) and push them as insights or incidents to the management console.</li>\n<li>The REST Cortex XDR API is very easy to consume with API Keys that can be tailored to every use case (access control) as well as security requirement.</li>\n<li>A NodeJS / TypeScript library exists for quick starting integrations.</li>\n</ul>\n<h4>References</h4>\n<ul>\n<li>How does Cortex XDR work? (YouTube Video)</li>\n<li>Cortex XDR API Reference</li>\n<li>NodeJS Client Library</li>\n<li>The MITRE ATT&amp;CK™ Evaluation Guide</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ef22a05ff753\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Custom incidents in Cortex XDR was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<p>You might have heard that Cortex XDR can boost your Security Operations productivity by learning how your network, endpoints and cloud components behave. Cortex XDR does that by taking into consideration the billions of data points generated by sensors deployed everywhere in your organization.</p>\n<p>As a result, only a small set of incidents that require supervision is generated. Hundreds or even thousands of alerts are conveniently collected inside these incidents for the security engineer to analyze and take action.</p>\n<p>But what’s an incident? Well, there are many angles to that question. I would classify them into the <em>known</em>, <em>custom</em> and <em>third party</em> categories.</p>\n<p>There are, of course, known threats well described by models like the MITRE ATT&amp;CK or malicious pieces of code no-one would like to see running on their endpoints. Cortex XDR provides an out-of-the-box collection of +350 behavior indicators of compromise (BIOCs) as well as analytics engines capable of creating baselines and detecting anomalies. So you’re well covered for the <em>known</em> angle of the response.</p>\n<p>Cortex XDR provides, as well, capabilities to allow Security Engineers model their own BIOCs to raise custom incidents. It allows, for instance, to describe the process of starting a FTP client and allowing it to open PDF files in a small collection of endpoints as an exfiltration attempt. So you’re covered, as well, with the <em>custom</em> angle of the response.</p>\n<p>And, finally, Cortex XDR trust by default High and Medium severity alerts generated by network sensors like the PAN-OS NGFW’s or endpoint sensors like the Cortex XDR Agent. Alerts from third party systems are supported as well. Anyone or anything capable of triggering a High or Medium severity alert is a candidate to generate an incident inside Cortex XDR.</p>\n<h3>The value of Third Party alerts</h3>\n<p>Cortex XDR features a very easy to consume REST API covering many of the product features. One of the API endpoints exposes a POST method that allows the system ingest alerts from third parties.</p>\n<p>Instead of describing the API endpoint I would rather prefer to imagine a use case and walk you through to completing an integration for it.</p>\n<p>Let’s imagine there is a critical system (System-X) in our data center in which we’re not allowed to install a Cortex XDR Agent (endpoint sensor). Imagine, as well, that any successful SSH Login to that system should tigger a Cortex XDR incident to let a Security Engineer report on it. And, finally, imagine that such a system generates syslog messages we can collect in a Linux box with a syntax similar to the following lines</p>\n<pre>Apr  2 09:09:28 mimic sshd[8580]: Accepted password for xhoms from 192.168.1.18 port 49290 ssh2</pre>\n<pre>Apr  2 09:09:28 mimic sshd[8582]: pam_unix(sshd:session): session opened for user xhoms by (uid=0)</pre>\n<p>Our objective for this article will be to parse these lines (in fact the first one) and generate a POST request to trigger a new Cortex XDR incident.</p>\n<h4>How does it looks like without a Third Party alert?</h4>\n<p>Cortex XDR provides a Query Builder feature that allows the security engineer analyze the connection described in the previous syslog message</p>\n<p>It can, for example, look for the endpoint and process responsible for such a connection.</p>\n<p>But, can you answer the question whether the login was successful or not? Which username was used for the connection? Or if the connection used password or public key material? The response is that the target system is better positioned to provide such details. And, in fact, it is doing so in the syslog message we saw before.</p>\n<h3>Cortex XDR API</h3>\n<p>Let’s try to leverage the Cortex XDR API and the syslog message generated by the fictitious System-X described before to trigger an incident with the data we want to present to our security engineers.</p>\n<h4>Insert Parsed Alerts API Endpoint</h4>\n<p>Details for this endpoint are available publicly in the Palo Alto Networks TechDoc site. It is basically a POST request to the URI /public_api/v1/alerts/insert_parsed_alerts/ with a JSON body consisting of an array of objects featuring the following object model.</p>\n<h4>API Authorization Headers</h4>\n<p>Cortex XDR API are protected by a RBAC API Key model. Administrator must create the API Key and assign access privileges to it using the management console.</p>\n<p>For our case (ingestion of alerts) the API Key must have <em>Administrator</em> privileges. Choosing between the <em>Advanced</em> or <em>Standard</em> security level will depend on the client used to POST the message. In the case of a cURL-based script then the right choice is the <em>Standard</em> security level. If you plan to use client libraries like the <em>pan-cortex-xdr-nodejs</em> then you better opt for the <em>Advanced</em> security level.</p>\n<p>Details on how to format the HTTP Authorization headers can be found in the Get Started with Cortex XDR APIs document. But, in summary:</p>\n<ul>\n<li>For <em>Standard</em> security level API Key’s you must set the headers authorization and x-xdr-auth-id</li>\n<li>For <em>Advanced</em> security level API Key’s you must set the headers authorization, x-xdr-auth-id, x-xdr-timestamp and x-xdr-nonce</li>\n</ul>\n<h3>The integration code</h3>\n<p>The tasks we need to complete are the following ones:</p>\n<ol>\n<li>Create a parser function that will receive a syslog message as single argument and return a Cortex XDR compliant parsed object (or null if the line can’t be parsed)</li>\n<li>A context that will instantiate a Cortex XDR API connection (load the API Key material), listen for new syslog messages calling the previous parser every time a new one is received and, finally, push the returned object to the Ingestion Alert endpoint (in case it is not null)</li>\n</ol>\n<p>Hooking this code into a pipeline (i.e. to the syslog receiver) is out of the scope of this article.</p>\n<p>For this example I’ll use TypeScript and leverage the pan-cortex-xdr-nodejs client library.</p>\n<h4>The message parser function</h4>\n<p>This can be easily achieved with a JS RegExp object.</p>\n<pre>// Example message: \"Apr  2 09:09:28 mimic sshd[8580]: Accepted password for xhoms from 192.168.1.18 port 49290 ssh2\"</pre>\n<pre>const msgRegExp = /^(.*[0-9]{2}:[0-9]{2}:[0-9]{2}).*Accepted ([^ ]+) for ([^ ]+) from ([^ ]+) port ([0-9]+) ssh2$/</pre>\n<p>This object contains five capturing groups:</p>\n<ol>\n<li>Date (Apr 2 09:09:28)</li>\n<li>Authentication type (password)</li>\n<li>Username (xhoms)</li>\n<li>Source IP (192.168.1.18) and</li>\n<li>Source Port (49290)</li>\n</ol>\n<p>Let’s leverage this RegExp object and return a valid Cortex XDR Parsed Alert object if the match is successful. Notice this piece of code uses the <em>MomentJS</em> library to parse the Time Date string.</p>\n<h4>The application context</h4>\n<p>We’re only missing a code context to initialize the connection to the XDR API (load the API Key material) and create a stream.Writable object that could be leveraged to build the pipeline. That’s quite easy using the @paloaltonetworks/pan-cortex-xdr NodeJS client library.</p>\n<p>In this case we’re passing, explicitly, the API Key material as options to the createXdrApi factory method. If these options are not passed then the factory method will try to find them in the environmental variables PAN_BASIC_API_KEY (or PAN_ADV_API_KEY), PAN_API_KEY_ID and PAN_XDR_FQDN</p>\n<p>We’re ready to go. We just need to hook this code into the syslog stream and call xdrAlertPipe.write(line) with every message we receive from the System-X</p>\n<h3>Incidents triggered by the integration</h3>\n<p>You might have noticed that our integration is pushing alerts with the Medium severity level. Alerts with levels Informational and Low are considered by Cortex XDR as insights. Alerts with levels Medium and High will be promoted to incidents instead.</p>\n<p>Now, in our XDR dashboard, we have a new incident triggered by our integration code. Analyzing the incident will provide the security engineer with details like username and ssh authentication type.</p>\n<p>The nice thing about Cortex XDR is that it automatically pulls additional information to enrich our incident. In this case, it provides additional insights to the Security Engineer like process being used in the client (putty.exe) parent process, command line arguments and, obviously, all details on the endpoint being used for the connection.</p>\n<h3>Summary</h3>\n<ul>\n<li>In some corner cases (i.e. threat data generated by closed systems) your security engineer might have some blind spots. The Cortex XDR API enables, among many other uses cases, the construction of integrations that can extract information from these systems (i.e. through Syslog) and push them as insights or incidents to the management console.</li>\n<li>The REST Cortex XDR API is very easy to consume with API Keys that can be tailored to every use case (access control) as well as security requirement.</li>\n<li>A NodeJS / TypeScript library exists for quick starting integrations.</li>\n</ul>\n<h4>References</h4>\n<ul>\n<li>How does Cortex XDR work? (YouTube Video)</li>\n<li>Cortex XDR API Reference</li>\n<li>NodeJS Client Library</li>\n<li>The MITRE ATT&amp;CK™ Evaluation Guide</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ef22a05ff753\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Custom incidents in Cortex XDR was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": [
        "cortex-xdr",
        "palo-alto-networks",
        "typescript",
        "cortex",
        "nodejs"
      ]
    },
    {
      "title": "Re-introducing {API} Explorer (for the first time)",
      "pubDate": "2020-04-15 21:53:08",
      "link": "https://medium.com/palo-alto-networks-developer-blog/re-introducing-api-explorer-for-the-first-time-3c81903fa700?source=rss----7f77455ad9a7---4",
      "guid": "https://medium.com/p/3c81903fa700",
      "author": "Steven Serrata",
      "thumbnail": "https://cdn-images-1.medium.com/max/1024/1*Bp4XA0_V9bEz3gxjZSYU9g.jpeg",
      "description": "\n<p>The story of how a Cortex™ hub sample application morphed into a developer onboarding tool.</p>\n<p><strong><em>Note</em></strong><em>: At this time, API Explorer is available only to select partners. If you are interested in developing a Cortex™ app or integration please register </em><em>here</em><em>.</em></p>\n<p>It was February 20, 2018, when a team of engineers, product managers, (and a rather lonely but sincere Developer Advocate) gathered for the first demo of the Cortex™ hub “reference application.” The demo (which itself was a walkthrough of API Explorer) sparked the first of many discussions around OAuth 2.0, schemas and APIs — discussions that would ultimately help shape the final product. What product you ask? Well, the Cortex™ 3rd-party application platform and ecosystem. The ask from product management was simple — develop a sample application that can be used to test the entire Cortex™ 3rd-party app lifecycle, end-to-end.</p>\n<blockquote>What isn’t so obvious in the recorded meeting, is that API Explorer would eventually become the de facto tool for developer onboarding and enablement.</blockquote>\n<p>In retrospect, this particular demo also highlighted the early stages of the “Developer Zero” journey I had unknowingly embarked on. It’s a little strange now, re-visiting the demo recording. It’s not just listening to the sound of my own voice. There’s an air of insecurity and it’s obvious the platform is in its nascent stages. What isn’t so obvious in the recorded meeting, is that API Explorer would eventually become the de facto tool for developer onboarding and enablement.</p>\n<h3>“Suck Valley” — Population Everyone?</h3>\n<blockquote>What’s challenging about this particular flow (or ritual) is that it requires a user agent and an OAuth 2.0 client capable of performing several browser redirects, culminating with the IdP returning a refresh_token — not exactly “quick start” material.</blockquote>\n<p>With over 30 apps published to Cortex™ hub (to date) there’s been plenty of time to collect and curate developer feedback. Without a doubt, the most salient feedback Developer Relations has received is about how challenging it is to implement our OAuth 2.0 authorization flow, and it’s easy to understand why. The particular flavor of OAuth 2.0 supported by Cortex™ hub is referred to as “3-legged OAuth” or the authorization code grant flow. What’s challenging about this particular flow (or ritual) is that it requires a user agent and an OAuth 2.0 client capable of facilitating several browser redirects, culminating with the IdP returning a refresh_token — not exactly “quick start” material.</p>\n<p>Don’t get me wrong, it’s not like we wanted to set our developer community up for failure (or worse, sit back and enjoy watching them struggle). There was a good reason why 3-legged OAuth was chosen. It’s a widely used authorization flow that is especially good at empowering end users to consent to 3rd-party apps accessing <em>their</em> data. If you’ve ever installed an app on your phone that prompted for permission to access your camera, or track your location, then you’re quite familiar with this concept.</p>\n<h3>Enter API Explorer</h3>\n<blockquote>API Explorer had its own evolutionary process to undergo and it would take several months before it became apparent that an alternative developer onboarding process was needed.</blockquote>\n<p>So, API Explorer comes in and saves the day, right? Well, it’s not that simple. API Explorer had its own evolutionary process to undergo and it would take several months before it became apparent that an alternative developer onboarding process was needed. I’ll get to that part, but let’s first take a moment to understand the purpose behind API Explorer and what it’s capable of today.</p>\n<p>(WARNING, some light marketing stuff follows)</p>\n<p>In short, API Explorer is a simple, yet powerful, Cortex™ Data Lake application, created to empower developers at any stage of their journey.</p>\n<p>It can be used to:</p>\n<ul>\n<li>Explore the data available in <em>your</em> Cortex Data Lake</li>\n<li>Rapidly test and refine SQL queries against the Query Service API</li>\n<li>Learn how to parse API responses</li>\n<li>Play in a developer sandbox environment</li>\n</ul>\n<p>More than anything, API Explorer reduces the initial friction introduced by having to implement 3-legged OAuth. Basically, the OAuth 2.0 stuff is offloaded to API Explorer so that you can dive straight into exploring and understanding the APIs and the data.</p>\n<h3>Greasing the Wheels</h3>\n<p>Ok, since you made it this far, I suppose I’ll share the singular feature that morphed API Explorer into a developer onboarding tool. You ready?</p>\n<p>Ok…so what exactly is a “developer token”? And, what makes it so special? In a nutshell, a Developer Token is used to authenticate with API Explorer’s built-in token redemption service. It’s intended to eliminate the need to acquire and store a client_id, client_secret and refresh_token in your own credentials store. Moreover, when combined with one of our SDKs, it can be used in place of these credentials in order to quickly gain API access to <em>your</em> Cortex Data Lake.</p>\n<p>So, API Explorer performs the authorization code grant flow, during which you grant it access to your CDL. Then, you generate a developer_token which can be used like an “API key” to send API requests programmatically. Don’t worry — after you’re finished working on your prototype, you’ll have plenty of time left to tackle the 3-legged OAuth flow.</p>\n<p>You’re welcome.</p>\n<h3>Example Usage</h3>\n<p>Behold! A Cortex™ Data Lake Query Service API request in a few lines of code!</p>\n<pre># Export developer token as environment variable</pre>\n<pre>export PAN_DEVELOPER_TOKEN=&lt;your_developer_token&gt;</pre>\n<p>To learn more about API Explorer and developer tokens, visit our Cortex for Developers site.</p>\n<p>Please note that API Explorer is currently available only to select partners. If you’re interested in developing a Cortex™ integration or third-party app register here: https://cortex.pan.dev/register.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3c81903fa700\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Re-introducing {API} Explorer (for the first time) was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "content": "\n<p>The story of how a Cortex™ hub sample application morphed into a developer onboarding tool.</p>\n<p><strong><em>Note</em></strong><em>: At this time, API Explorer is available only to select partners. If you are interested in developing a Cortex™ app or integration please register </em><em>here</em><em>.</em></p>\n<p>It was February 20, 2018, when a team of engineers, product managers, (and a rather lonely but sincere Developer Advocate) gathered for the first demo of the Cortex™ hub “reference application.” The demo (which itself was a walkthrough of API Explorer) sparked the first of many discussions around OAuth 2.0, schemas and APIs — discussions that would ultimately help shape the final product. What product you ask? Well, the Cortex™ 3rd-party application platform and ecosystem. The ask from product management was simple — develop a sample application that can be used to test the entire Cortex™ 3rd-party app lifecycle, end-to-end.</p>\n<blockquote>What isn’t so obvious in the recorded meeting, is that API Explorer would eventually become the de facto tool for developer onboarding and enablement.</blockquote>\n<p>In retrospect, this particular demo also highlighted the early stages of the “Developer Zero” journey I had unknowingly embarked on. It’s a little strange now, re-visiting the demo recording. It’s not just listening to the sound of my own voice. There’s an air of insecurity and it’s obvious the platform is in its nascent stages. What isn’t so obvious in the recorded meeting, is that API Explorer would eventually become the de facto tool for developer onboarding and enablement.</p>\n<h3>“Suck Valley” — Population Everyone?</h3>\n<blockquote>What’s challenging about this particular flow (or ritual) is that it requires a user agent and an OAuth 2.0 client capable of performing several browser redirects, culminating with the IdP returning a refresh_token — not exactly “quick start” material.</blockquote>\n<p>With over 30 apps published to Cortex™ hub (to date) there’s been plenty of time to collect and curate developer feedback. Without a doubt, the most salient feedback Developer Relations has received is about how challenging it is to implement our OAuth 2.0 authorization flow, and it’s easy to understand why. The particular flavor of OAuth 2.0 supported by Cortex™ hub is referred to as “3-legged OAuth” or the authorization code grant flow. What’s challenging about this particular flow (or ritual) is that it requires a user agent and an OAuth 2.0 client capable of facilitating several browser redirects, culminating with the IdP returning a refresh_token — not exactly “quick start” material.</p>\n<p>Don’t get me wrong, it’s not like we wanted to set our developer community up for failure (or worse, sit back and enjoy watching them struggle). There was a good reason why 3-legged OAuth was chosen. It’s a widely used authorization flow that is especially good at empowering end users to consent to 3rd-party apps accessing <em>their</em> data. If you’ve ever installed an app on your phone that prompted for permission to access your camera, or track your location, then you’re quite familiar with this concept.</p>\n<h3>Enter API Explorer</h3>\n<blockquote>API Explorer had its own evolutionary process to undergo and it would take several months before it became apparent that an alternative developer onboarding process was needed.</blockquote>\n<p>So, API Explorer comes in and saves the day, right? Well, it’s not that simple. API Explorer had its own evolutionary process to undergo and it would take several months before it became apparent that an alternative developer onboarding process was needed. I’ll get to that part, but let’s first take a moment to understand the purpose behind API Explorer and what it’s capable of today.</p>\n<p>(WARNING, some light marketing stuff follows)</p>\n<p>In short, API Explorer is a simple, yet powerful, Cortex™ Data Lake application, created to empower developers at any stage of their journey.</p>\n<p>It can be used to:</p>\n<ul>\n<li>Explore the data available in <em>your</em> Cortex Data Lake</li>\n<li>Rapidly test and refine SQL queries against the Query Service API</li>\n<li>Learn how to parse API responses</li>\n<li>Play in a developer sandbox environment</li>\n</ul>\n<p>More than anything, API Explorer reduces the initial friction introduced by having to implement 3-legged OAuth. Basically, the OAuth 2.0 stuff is offloaded to API Explorer so that you can dive straight into exploring and understanding the APIs and the data.</p>\n<h3>Greasing the Wheels</h3>\n<p>Ok, since you made it this far, I suppose I’ll share the singular feature that morphed API Explorer into a developer onboarding tool. You ready?</p>\n<p>Ok…so what exactly is a “developer token”? And, what makes it so special? In a nutshell, a Developer Token is used to authenticate with API Explorer’s built-in token redemption service. It’s intended to eliminate the need to acquire and store a client_id, client_secret and refresh_token in your own credentials store. Moreover, when combined with one of our SDKs, it can be used in place of these credentials in order to quickly gain API access to <em>your</em> Cortex Data Lake.</p>\n<p>So, API Explorer performs the authorization code grant flow, during which you grant it access to your CDL. Then, you generate a developer_token which can be used like an “API key” to send API requests programmatically. Don’t worry — after you’re finished working on your prototype, you’ll have plenty of time left to tackle the 3-legged OAuth flow.</p>\n<p>You’re welcome.</p>\n<h3>Example Usage</h3>\n<p>Behold! A Cortex™ Data Lake Query Service API request in a few lines of code!</p>\n<pre># Export developer token as environment variable</pre>\n<pre>export PAN_DEVELOPER_TOKEN=&lt;your_developer_token&gt;</pre>\n<p>To learn more about API Explorer and developer tokens, visit our Cortex for Developers site.</p>\n<p>Please note that API Explorer is currently available only to select partners. If you’re interested in developing a Cortex™ integration or third-party app register here: https://cortex.pan.dev/register.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3c81903fa700\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p>Re-introducing {API} Explorer (for the first time) was originally published in Palo Alto Networks Developer Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
      "enclosure": {},
      "categories": ["oauth2", "palo-alto-networks", "cortex"]
    }
  ]
}
