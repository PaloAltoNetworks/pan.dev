{"status":"ok","feed":{"url":"https://medium.com/feed/palo-alto-networks-developer-blog","title":"Palo Alto Networks Developer Blog - Medium","link":"https://medium.com/palo-alto-networks-developer-blog?source=rss----7f77455ad9a7---4","author":"","description":"All things API, DevOps, SecOps, Security, Automation - Medium","image":"https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png"},"items":[{"title":"Dynamic Firewalling with Palo Alto Networks NGFWs and Consul-Terraform-Sync","pubDate":"2021-11-29 16:16:06","link":"https://medium.com/palo-alto-networks-developer-blog/dynamic-firewalling-with-palo-alto-networks-ngfws-and-consul-terraform-sync-1e7798c843ee?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/1e7798c843ee","author":"Migara Ekanayake","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*NWjt1kbXbjOvtBVrMftUhQ.png","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NWjt1kbXbjOvtBVrMftUhQ.png\"></figure><blockquote>I cannot reach www.isitthefirewall.com\u2026!! Must be the firewall!!</blockquote>\n<p>Sounds familiar? We\u2019ve all been there. If you were that lonely firewall administrator who tried to defend the good ol\u2019 firewall, congratulations for making it this far. Life was tough back then when you only had a handful of firewall change requests a day, static data centres and monolithic applications.</p>\n<p>Fast forward to the present day, we are moving away from traditional static datacentres to modern dynamic datacentres. Applications are no longer maintained as monoliths, they are arranged into several smaller functional components (aka Microservices) to gain development agility. As you gain development agility you introduce new operational challenges.</p>\n<h3>What are the Operational Challenges?</h3>\n<p>Now that you have split your monolithic application into a dozen of microservices, most likely you are going to have multiple instances of each microservice to fully realise the benefits of this app transformation exercise. Every time you want to bring up a new instance, you open a ticket and wait for the firewall administrator to allow traffic to that new node, this could take days if not\u00a0weeks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*IIxryI0M9W1oLqxf\"><figcaption>Photo by <a href=\"https://unsplash.com/@introspectivedsgn?utm_source=medium&amp;utm_medium=referral\">Erik Mclean</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>When you add autoscaling into the mix (so that the number of instances can dynamically scale in and out based on your traffic demands) having to wait days or weeks before traffic can reach those new instances defeats the whole point of having the autoscaling in the first\u00a0place.</p>\n<p>Long live\u00a0agility!</p>\n<h3>The Disjoint</h3>\n<p>Traditionally, firewall administrators used to retrieve the requests from their ticketing system and implement those changes via UI or CLI during a maintenance window. This results in creating an impedance mismatch with the application teams and overall slower delivery of the solutions and can also introduce an element of human error during the ticket creation process as well as the implementation of this request. If you are a network/security administrator who has recognised these problems, the likelihood is that you have already written some custom scripts and/or leveraged a configuration management platform to automate some of these tasks. Yes, this solves the problem to a certain extent, still, there is a manual handoff task between Continuous Delivery and Continuous Deployment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*DHsRg0yS7RguQyi7AxddDg.gif\"><figcaption>The DevOps Shortfall</figcaption></figure><h3>Consul-Terraform-Sync</h3>\n<p>Network and security teams can solve these challenges by enabling dynamic service-driven network automation with self-service capabilities using an automation tool that supports multiple networking technologies.</p>\n<p>HashiCorp and Palo Alto Networks recently collaborated on a strategy for this using HashiCorp\u2019s <a href=\"https://www.consul.io/docs/nia\">Network Infrastructure Automation</a> (NIA). This works by triggering a Terraform workflow that automatically updates Palo Alto Networks NGFWs or Panorama based on changes it detects from the <a href=\"https://github.com/hashicorp/consul\">Consul</a>\u00a0catalog.</p>\n<p>Under the hood, we are leveraging <a href=\"https://docs.paloaltonetworks.com/pan-os/9-1/pan-os-admin/policy/register-ip-addresses-and-tags-dynamically.html\">Dynamic Address Groups</a> (DAGs). In PAN-OS it is possible to dynamically associate (and remove) tags from IP addresses, using several ways, including the XML API. A tag is simply a string that can be used as match criteria in Dynamic Address Groups, allowing the Firewall to dynamically allow/block traffic without requiring a configuration commit.</p>\n<p>If you need a refresher on DAGs here is a great <a href=\"https://panos.pan.dev/docs/apis/panos_dag_qs/\">DAG Quickstart</a> guide.</p>\n<h3>Scaling Up with\u00a0Panorama</h3>\n<p>The challenge for large-scale networks is ensuring every firewall that enforces policies has the IP address-to-tag mappings for your entire user\u00a0base.</p>\n<p>If you are managing your Palo Alto Networks NGFWs with Panorama you can redistribute IP address-to-tag mappings to your entire firewall estate within a matter of seconds. This could be your VM-Series NGFWs deployed in the public cloud, private cloud, hybrid cloud or hardware NGFWs in your datacentre.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9PJShizQ-ZVMPHDrVtVQXw.png\"><figcaption>IP address-to-tag mappings redistribution</figcaption></figure><h3>What\u2019s Next?</h3>\n<p>If you have read this far, why not give this <a href=\"https://learn.hashicorp.com/tutorials/consul/consul-terraform-sync-pan?in=consul/network-infrastructure-automation\">step-by-step guide on how you can automate the configuration management process for Palo Alto Networks NGFWs with Consul-Terraform-Sync</a> a\u00a0try?.</p>\n<p>For more resources, check\u00a0out:</p>\n<ul>\n<li>Webinar: <a href=\"https://www.hashicorp.com/events/webinars/palo-alto-networks-and-hashicorp-cts\">Network Security Automation with HashiCorp Consul-Terraform-Sync and Palo Alto\u00a0Networks</a>\n</li>\n<li>Whitepaper: <a href=\"https://www.datocms-assets.com/2885/1621464663-consulpaloaltosolutionbriefdigital-3.pdf\">Enabling Dynamic Firewalling with Palo Alto and HashiCorp</a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1e7798c843ee\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/dynamic-firewalling-with-palo-alto-networks-ngfws-and-consul-terraform-sync-1e7798c843ee\">Dynamic Firewalling with Palo Alto Networks NGFWs and Consul-Terraform-Sync</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NWjt1kbXbjOvtBVrMftUhQ.png\"></figure><blockquote>I cannot reach www.isitthefirewall.com\u2026!! Must be the firewall!!</blockquote>\n<p>Sounds familiar? We\u2019ve all been there. If you were that lonely firewall administrator who tried to defend the good ol\u2019 firewall, congratulations for making it this far. Life was tough back then when you only had a handful of firewall change requests a day, static data centres and monolithic applications.</p>\n<p>Fast forward to the present day, we are moving away from traditional static datacentres to modern dynamic datacentres. Applications are no longer maintained as monoliths, they are arranged into several smaller functional components (aka Microservices) to gain development agility. As you gain development agility you introduce new operational challenges.</p>\n<h3>What are the Operational Challenges?</h3>\n<p>Now that you have split your monolithic application into a dozen of microservices, most likely you are going to have multiple instances of each microservice to fully realise the benefits of this app transformation exercise. Every time you want to bring up a new instance, you open a ticket and wait for the firewall administrator to allow traffic to that new node, this could take days if not\u00a0weeks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*IIxryI0M9W1oLqxf\"><figcaption>Photo by <a href=\"https://unsplash.com/@introspectivedsgn?utm_source=medium&amp;utm_medium=referral\">Erik Mclean</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>When you add autoscaling into the mix (so that the number of instances can dynamically scale in and out based on your traffic demands) having to wait days or weeks before traffic can reach those new instances defeats the whole point of having the autoscaling in the first\u00a0place.</p>\n<p>Long live\u00a0agility!</p>\n<h3>The Disjoint</h3>\n<p>Traditionally, firewall administrators used to retrieve the requests from their ticketing system and implement those changes via UI or CLI during a maintenance window. This results in creating an impedance mismatch with the application teams and overall slower delivery of the solutions and can also introduce an element of human error during the ticket creation process as well as the implementation of this request. If you are a network/security administrator who has recognised these problems, the likelihood is that you have already written some custom scripts and/or leveraged a configuration management platform to automate some of these tasks. Yes, this solves the problem to a certain extent, still, there is a manual handoff task between Continuous Delivery and Continuous Deployment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*DHsRg0yS7RguQyi7AxddDg.gif\"><figcaption>The DevOps Shortfall</figcaption></figure><h3>Consul-Terraform-Sync</h3>\n<p>Network and security teams can solve these challenges by enabling dynamic service-driven network automation with self-service capabilities using an automation tool that supports multiple networking technologies.</p>\n<p>HashiCorp and Palo Alto Networks recently collaborated on a strategy for this using HashiCorp\u2019s <a href=\"https://www.consul.io/docs/nia\">Network Infrastructure Automation</a> (NIA). This works by triggering a Terraform workflow that automatically updates Palo Alto Networks NGFWs or Panorama based on changes it detects from the <a href=\"https://github.com/hashicorp/consul\">Consul</a>\u00a0catalog.</p>\n<p>Under the hood, we are leveraging <a href=\"https://docs.paloaltonetworks.com/pan-os/9-1/pan-os-admin/policy/register-ip-addresses-and-tags-dynamically.html\">Dynamic Address Groups</a> (DAGs). In PAN-OS it is possible to dynamically associate (and remove) tags from IP addresses, using several ways, including the XML API. A tag is simply a string that can be used as match criteria in Dynamic Address Groups, allowing the Firewall to dynamically allow/block traffic without requiring a configuration commit.</p>\n<p>If you need a refresher on DAGs here is a great <a href=\"https://panos.pan.dev/docs/apis/panos_dag_qs/\">DAG Quickstart</a> guide.</p>\n<h3>Scaling Up with\u00a0Panorama</h3>\n<p>The challenge for large-scale networks is ensuring every firewall that enforces policies has the IP address-to-tag mappings for your entire user\u00a0base.</p>\n<p>If you are managing your Palo Alto Networks NGFWs with Panorama you can redistribute IP address-to-tag mappings to your entire firewall estate within a matter of seconds. This could be your VM-Series NGFWs deployed in the public cloud, private cloud, hybrid cloud or hardware NGFWs in your datacentre.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9PJShizQ-ZVMPHDrVtVQXw.png\"><figcaption>IP address-to-tag mappings redistribution</figcaption></figure><h3>What\u2019s Next?</h3>\n<p>If you have read this far, why not give this <a href=\"https://learn.hashicorp.com/tutorials/consul/consul-terraform-sync-pan?in=consul/network-infrastructure-automation\">step-by-step guide on how you can automate the configuration management process for Palo Alto Networks NGFWs with Consul-Terraform-Sync</a> a\u00a0try?.</p>\n<p>For more resources, check\u00a0out:</p>\n<ul>\n<li>Webinar: <a href=\"https://www.hashicorp.com/events/webinars/palo-alto-networks-and-hashicorp-cts\">Network Security Automation with HashiCorp Consul-Terraform-Sync and Palo Alto\u00a0Networks</a>\n</li>\n<li>Whitepaper: <a href=\"https://www.datocms-assets.com/2885/1621464663-consulpaloaltosolutionbriefdigital-3.pdf\">Enabling Dynamic Firewalling with Palo Alto and HashiCorp</a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1e7798c843ee\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/dynamic-firewalling-with-palo-alto-networks-ngfws-and-consul-terraform-sync-1e7798c843ee\">Dynamic Firewalling with Palo Alto Networks NGFWs and Consul-Terraform-Sync</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["palo-alto-networks","network-automation","consul-terraform-sync","hashicorp","pano"]},{"title":"PAN.dev and the Rise of Docs-as-Code","pubDate":"2021-11-09 16:08:29","link":"https://medium.com/palo-alto-networks-developer-blog/pan-dev-and-the-rise-of-docs-as-code-9ac544e0ff3?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/9ac544e0ff3","author":"Steven Serrata","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*dNBwueBXCX9bjIq06F8Ysw.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dNBwueBXCX9bjIq06F8Ysw.jpeg\"></figure><p>It started out as a simple idea\u200a\u2014\u200ato improve the documentation experience for the many open-source projects at <a href=\"https://github.com/PaloAltoNetworks\">Palo Alto Networks</a>. Up until that point (and still to this day) our open source projects were documented in the traditional fashion: the ubiquitous README.md and the occasional <a href=\"https://readthedocs.org/\">Read the Docs</a> or <a href=\"https://pages.github.com/\">GitHub Pages</a> site. In truth, there is nothing inherently \u201cwrong\u201d with this approach but there was a sense that we could do more to improve the developer experience of adopting our APIs, tools and SDKs. So began our exploration into understanding how companies like Google, Microsoft, Facebook, Amazon, Twilio, Stripe, et al., approached developer docs. Outside of visual aesthetics, it was clear that these companies invested a great deal of time into streamlining their developer onboarding experiences, something we like to refer to as \u201ctime-to-joy.\u201d The hard truth was that our <a href=\"https://docs.paloaltonetworks.com/\">product documentation site</a> was catering to an altogether different audience and it was evident in the fact that features like quick starts (\u201cHello World\u201d), code blocks, and interactive API reference docs were noticeably absent.</p>\n<blockquote>After some tinkering we had our first, git-backed developer portal deployed using Gatsby and AWS Amplify, but that was only the beginning.</blockquote>\n<h3>Great Gatsby!</h3>\n<p>Circa March 2019. Although we technically weren\u2019t new to Developer Relations as a practice, it was the first time we made an honest attempt to solve for the lack of developer-style documentation at Palo Alto Networks. We quickly got to work researching how other companies delivered their developer sites and received a tip from a colleague to look at <a href=\"https://www.gatsbyjs.com/\">Gatsby</a>\u200a\u2014\u200aa static-site generator based on GraphQL and ReactJS. After some tinkering, we had our first git-backed developer portal deployed using <a href=\"https://aws.amazon.com/amplify/\">AWS Amplify</a>, but that was only the beginning. It was a revelation that git-backed, web/CDN hosting services existed and that rich, interactive, documentation sites could be managed like code. It wasn\u2019t long before we found <a href=\"https://www.netlify.com/\">Netlify</a>, <a href=\"https://firebase.google.com/\">Firebase</a>, and, eventually, <a href=\"https://v2.docusaurus.io/\">Docusaurus</a>\u200a\u2014\u200aa static-site generator specializing in the documentation use case. It was easy to pick up and run with this toolchain as it seamlessly weaved together the open-source git flows we were accustomed to with the ability to rapidly author developer docs using markdown and <a href=\"https://mdxjs.com/\">MDX</a>\u200a\u2014\u200aa veritable match made in heaven. We had the stack\u200a\u2014\u200anow all we needed was a strategy.</p>\n<blockquote>At a deeper level, it\u2019s a fresh, bold new direction for a disruptive, next-generation security company that, through strategic acquisitions and innovation, has found itself on the doorsteps of the <a href=\"https://www.forbes.com/sites/tomtaulli/2020/01/18/api-economy--is-it-the-next-big-thing/?sh=149de2642ffd\">API\u00a0Economy</a>.</blockquote>\n<h3>For Developers, by Developers</h3>\n<p>At the surface, <a href=\"https://developers.paloaltonetworks.com/\">pan.dev</a> is a family of sites dedicated to documentation for developers, by developers. At a deeper level, it\u2019s a fresh, bold, new direction for a disruptive, next-generation security company that, through strategic acquisitions and innovation, has found itself on the doorsteps of the <a href=\"https://www.forbes.com/sites/tomtaulli/2020/01/18/api-economy--is-it-the-next-big-thing/?sh=149de2642ffd\">API Economy</a> (more on that topic coming\u00a0soon!).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gX2Me3pxWAyLJZe2LoATkQ.png\"><figcaption>pan.dev overview</figcaption></figure><p>The content published on pan.dev is <strong><em>for developers</em></strong><em> </em>because it supports and includes the features that developer audiences have come to expect, such as code blocks and API reference docs (and dark\u00a0mode!):</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*r66kFUpDjTRkMkdycL57gA.gif\"><figcaption>Code blocks on\u00a0pan.dev</figcaption></figure><p>It\u2019s <strong><em>by developers </em></strong>since the contributing, review and publishing flows are backed by git and version control systems like GitLab and GitHub. What\u2019s more, by adopting git as the underlying collaboration tool, pan.dev is capable of supporting both closed and open-source contributions. Spot a typo or inaccuracy? <em>Open a GitHub issue.</em> Got a cool <a href=\"https://panos.pan.dev/docs/apis/pandevice_qs\">pan-os-python</a> tutorial you\u2019d like to contribute? <em>Follow our </em><a href=\"https://panos.pan.dev/docs/contributing\"><em>contributing guidelines</em></a><em> and we\u2019ll start reviewing your submission.</em> By adopting developer workflows, pan.dev is able to move developer docs closer to the source (no pun intended). That means we can deliver high-quality content straight from the minds of the leading automation, scripting, DevOps/SecOps practitioners in the cybersecurity world.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wdh-41YrISa7h76VfkkGMA.gif\"><figcaption>Markdown demo</figcaption></figure><h3>So, What\u2019s\u00a0Next?</h3>\n<p>If we\u2019ve learned anything, over the years, it\u2019s that coming up with technical solutions might actually be the easier aspect of working on the pan.dev project. What can be more difficult (and arguably more critical) is garnering buy-in from stakeholders, partners and leadership, while adapting to the ever-evolving needs of the developer audience we\u2019re trying to serve. All this while doing what we can to ensure the reliability and scalability of the underlying infrastructure (not to mention SEO, analytics, accessibility, performance, i18n, etc.). The <em>real</em> work has only recently begun and we\u2019re ecstatic to see what the future holds. Already, we\u2019ve seen a meteoric rise in monthly active users (MAUs) across sites (over 40K!) and pan.dev is well-positioned to be the de facto platform for delivering developer documentation at Palo Alto Networks.</p>\n<p>To learn more and experience it for yourself, feel free to tour our <a href=\"https://pan.dev/\">family of developer sites</a> and peruse the <a href=\"https://gallery.pan.dev/\">GitHub Gallery</a> of open-source projects at Palo Alto Networks.</p>\n<p>For a deeper dive into the pan.dev toolchain/stack, stay tuned for part\u00a02!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9ac544e0ff3\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/pan-dev-and-the-rise-of-docs-as-code-9ac544e0ff3\">PAN.dev and the Rise of Docs-as-Code</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dNBwueBXCX9bjIq06F8Ysw.jpeg\"></figure><p>It started out as a simple idea\u200a\u2014\u200ato improve the documentation experience for the many open-source projects at <a href=\"https://github.com/PaloAltoNetworks\">Palo Alto Networks</a>. Up until that point (and still to this day) our open source projects were documented in the traditional fashion: the ubiquitous README.md and the occasional <a href=\"https://readthedocs.org/\">Read the Docs</a> or <a href=\"https://pages.github.com/\">GitHub Pages</a> site. In truth, there is nothing inherently \u201cwrong\u201d with this approach but there was a sense that we could do more to improve the developer experience of adopting our APIs, tools and SDKs. So began our exploration into understanding how companies like Google, Microsoft, Facebook, Amazon, Twilio, Stripe, et al., approached developer docs. Outside of visual aesthetics, it was clear that these companies invested a great deal of time into streamlining their developer onboarding experiences, something we like to refer to as \u201ctime-to-joy.\u201d The hard truth was that our <a href=\"https://docs.paloaltonetworks.com/\">product documentation site</a> was catering to an altogether different audience and it was evident in the fact that features like quick starts (\u201cHello World\u201d), code blocks, and interactive API reference docs were noticeably absent.</p>\n<blockquote>After some tinkering we had our first, git-backed developer portal deployed using Gatsby and AWS Amplify, but that was only the beginning.</blockquote>\n<h3>Great Gatsby!</h3>\n<p>Circa March 2019. Although we technically weren\u2019t new to Developer Relations as a practice, it was the first time we made an honest attempt to solve for the lack of developer-style documentation at Palo Alto Networks. We quickly got to work researching how other companies delivered their developer sites and received a tip from a colleague to look at <a href=\"https://www.gatsbyjs.com/\">Gatsby</a>\u200a\u2014\u200aa static-site generator based on GraphQL and ReactJS. After some tinkering, we had our first git-backed developer portal deployed using <a href=\"https://aws.amazon.com/amplify/\">AWS Amplify</a>, but that was only the beginning. It was a revelation that git-backed, web/CDN hosting services existed and that rich, interactive, documentation sites could be managed like code. It wasn\u2019t long before we found <a href=\"https://www.netlify.com/\">Netlify</a>, <a href=\"https://firebase.google.com/\">Firebase</a>, and, eventually, <a href=\"https://v2.docusaurus.io/\">Docusaurus</a>\u200a\u2014\u200aa static-site generator specializing in the documentation use case. It was easy to pick up and run with this toolchain as it seamlessly weaved together the open-source git flows we were accustomed to with the ability to rapidly author developer docs using markdown and <a href=\"https://mdxjs.com/\">MDX</a>\u200a\u2014\u200aa veritable match made in heaven. We had the stack\u200a\u2014\u200anow all we needed was a strategy.</p>\n<blockquote>At a deeper level, it\u2019s a fresh, bold new direction for a disruptive, next-generation security company that, through strategic acquisitions and innovation, has found itself on the doorsteps of the <a href=\"https://www.forbes.com/sites/tomtaulli/2020/01/18/api-economy--is-it-the-next-big-thing/?sh=149de2642ffd\">API\u00a0Economy</a>.</blockquote>\n<h3>For Developers, by Developers</h3>\n<p>At the surface, <a href=\"https://developers.paloaltonetworks.com/\">pan.dev</a> is a family of sites dedicated to documentation for developers, by developers. At a deeper level, it\u2019s a fresh, bold, new direction for a disruptive, next-generation security company that, through strategic acquisitions and innovation, has found itself on the doorsteps of the <a href=\"https://www.forbes.com/sites/tomtaulli/2020/01/18/api-economy--is-it-the-next-big-thing/?sh=149de2642ffd\">API Economy</a> (more on that topic coming\u00a0soon!).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gX2Me3pxWAyLJZe2LoATkQ.png\"><figcaption>pan.dev overview</figcaption></figure><p>The content published on pan.dev is <strong><em>for developers</em></strong><em> </em>because it supports and includes the features that developer audiences have come to expect, such as code blocks and API reference docs (and dark\u00a0mode!):</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*r66kFUpDjTRkMkdycL57gA.gif\"><figcaption>Code blocks on\u00a0pan.dev</figcaption></figure><p>It\u2019s <strong><em>by developers </em></strong>since the contributing, review and publishing flows are backed by git and version control systems like GitLab and GitHub. What\u2019s more, by adopting git as the underlying collaboration tool, pan.dev is capable of supporting both closed and open-source contributions. Spot a typo or inaccuracy? <em>Open a GitHub issue.</em> Got a cool <a href=\"https://panos.pan.dev/docs/apis/pandevice_qs\">pan-os-python</a> tutorial you\u2019d like to contribute? <em>Follow our </em><a href=\"https://panos.pan.dev/docs/contributing\"><em>contributing guidelines</em></a><em> and we\u2019ll start reviewing your submission.</em> By adopting developer workflows, pan.dev is able to move developer docs closer to the source (no pun intended). That means we can deliver high-quality content straight from the minds of the leading automation, scripting, DevOps/SecOps practitioners in the cybersecurity world.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wdh-41YrISa7h76VfkkGMA.gif\"><figcaption>Markdown demo</figcaption></figure><h3>So, What\u2019s\u00a0Next?</h3>\n<p>If we\u2019ve learned anything, over the years, it\u2019s that coming up with technical solutions might actually be the easier aspect of working on the pan.dev project. What can be more difficult (and arguably more critical) is garnering buy-in from stakeholders, partners and leadership, while adapting to the ever-evolving needs of the developer audience we\u2019re trying to serve. All this while doing what we can to ensure the reliability and scalability of the underlying infrastructure (not to mention SEO, analytics, accessibility, performance, i18n, etc.). The <em>real</em> work has only recently begun and we\u2019re ecstatic to see what the future holds. Already, we\u2019ve seen a meteoric rise in monthly active users (MAUs) across sites (over 40K!) and pan.dev is well-positioned to be the de facto platform for delivering developer documentation at Palo Alto Networks.</p>\n<p>To learn more and experience it for yourself, feel free to tour our <a href=\"https://pan.dev/\">family of developer sites</a> and peruse the <a href=\"https://gallery.pan.dev/\">GitHub Gallery</a> of open-source projects at Palo Alto Networks.</p>\n<p>For a deeper dive into the pan.dev toolchain/stack, stay tuned for part\u00a02!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9ac544e0ff3\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/pan-dev-and-the-rise-of-docs-as-code-9ac544e0ff3\">PAN.dev and the Rise of Docs-as-Code</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["cybersecurity","documentation","static-site-generator","developer","palo-alto-networks"]},{"title":"User-ID / EDL\u00a0\u2026 better both of them","pubDate":"2021-04-13 16:39:40","link":"https://medium.com/palo-alto-networks-developer-blog/user-id-edl-better-both-of-them-65b6a9d6c424?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/65b6a9d6c424","author":"Xavier Homs","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*J8LUAx85I7Hf-SKk47MZfQ.jpeg","description":"\n<h3>User-ID or EDL\u00a0\u2026 why not\u00a0both?</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*J8LUAx85I7Hf-SKk47MZfQ.jpeg\"><figcaption>Original Photo by <a href=\"https://unsplash.com/@jontyson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Jon\u00a0Tyson</a></figcaption></figure><p>User-ID and External Dynamic Lists (EDL\u2019s) are probably the most commonly used PAN-OS features to share external IP metadata with the NGFW. The Use cases for them are enormous: from blocking known DoS sources to forward traffic from specific IP addresses on low latency\u00a0links.</p>\n<p>Let\u2019s take for instance the following logs generated by the SSH daemon in a linux\u00a0host.</p>\n<a href=\"https://medium.com/media/6a8bdd451748be2a7326a77029a6a200/href\">https://medium.com/media/6a8bdd451748be2a7326a77029a6a200/href</a><p>Won\u2019t it be great to have a way to share the IP addresses used by the user <em>xhoms</em> with the NGFW so specific security policies are applied to traffic sourced by that address just because we now know the endpoint is being managed by that\u00a0user?</p>\n<p>The following Shell CGI script could be used to create an External Dynamic List (EDL) that would list all known IP addresses the account <em>xhoms</em> was used to login into this\u00a0server.</p>\n<a href=\"https://medium.com/media/ec08fd38c48c8a005909d5b502e521cc/href\">https://medium.com/media/ec08fd38c48c8a005909d5b502e521cc/href</a><p>That is great\u00a0but:</p>\n<ul>\n<li>When would addresses be removed from the list? At the daily log\u00a0rollout?</li>\n<li>Are all listed IP addresses still being operated by the user\u00a0<em>xhoms</em>?</li>\n<li>Is there any way to remove addresses from the list at user\u00a0logout?</li>\n</ul>\n<p>To overcome these (and many other) limitations Palo Alto Networks NGFW feature a very powerful IP tagging API called User-ID. Although EDL and User-ID cover similar objectives there are fundamental technical differences between\u00a0them:</p>\n<ul>\n<li>User-ID is \u201casynchronous\u201d (push mode), supports bot \u201cset\u201d and \u201cdelete\u201d operations and provides a very flexible way to create groupings. Either by tagging address objects (Dynamic Address Group\u200a\u2014\u200aDAG ) or by mapping users to addresses and then tagging these users (Dynamic User Group\u200a\u2014\u200aDUG). User-ID allows for these tags to have a timeout in order for the PAN-OS device to take care of removing them at their due\u00a0time.</li>\n<li>EDL is \u201cuniversal\u201d. Almost all network appliance vendors provide a feature to fetch an IP address list from a\u00a0URL.</li>\n</ul>\n<p>Years ago it was up to the end customer to create its own connectors. Just like the CGI script we shared before. But as presence of PAN-OS powered NGFW\u2019s grown among enterprise customers more application vendors decided to leverage the User-ID value by providing API clients out-of-the-box. Although this is great (for Palo Alto Network customers) losing the option to fetch a list from a URL (EDL mode) makes it more difficult to integrate legacy technologies that might be out there still in the\u00a0network.</p>\n<h4>Creating EDL\u2019s from User-ID enabled applications</h4>\n<p>Let\u2019s assume you have an application that features a PAN-OS User-ID API client and that is capable of pushing log entries to the PAN-OS NGFW in an asynchronous way. Let\u2019s assume, as well, that there are still some legacy network devices in your network that need to fetch that address metadata from a URL. How difficult it would be to create a micro-service for\u00a0that?</p>\n<p>One option would be to leverage a PAN-OS XML SDK like <a href=\"https://panos.pan.dev/docs/apis/panpython_qs\">PAN Python</a> or <a href=\"https://panos.pan.dev/docs/apis/pango_qs\">PAN GO</a> and to create a REST API that extracts the current state from the PAN-OS device using its operations API.</p>\n<pre>GET https://&lt;pan-os-device&gt;/api<br>    ?key=&lt;API-KEY&gt;<br>    &amp;type=op<br>    &amp;cmd=&lt;show&gt;&lt;object&gt;&lt;registered-ip&gt;&lt;all&gt;&lt;/all&gt;&lt;/registered-ip&gt;&lt;/object&gt;&lt;/show&gt;</pre>\n<pre>HTTP/1.1 200 OK</pre>\n<pre>&lt;response status=\"success\"&gt;<br>  &lt;result&gt;<br>    &lt;entry ip=\"10.10.10.11\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;entry ip=\"10.10.10.10\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;count&gt;2&lt;/count&gt;<br>  &lt;/result&gt;<br>&lt;/response&gt;</pre>\n<p>It shouldn\u2019t be that difficult to parse the response and provide a plain list out of it, right? Come back to me if you\u2019re thinking on using a XSLT processor to pipe it at the output of a cURL command packing everything as a CGI script because we might have some common grounds\u00a0;-)</p>\n<p>I\u2019d love to propose a different approach though: to hijack User-ID messages by implementing a micro-service that behaves as a <strong>reverse proxy</strong> between the application that features the User-ID client and the PAN-OS device. I like this approach because it opens the door to other interesting use cases like enforcing timeout (adding timeout to entries that do not have it), converting DAG messages into DUG equivalents or adding additional tags based on the source application.</p>\n<h4>Components for a User-ID to EDL reverse\u00a0proxy</h4>\n<p>The ingredients for such a receipt would\u00a0be:</p>\n<ol>\n<li>a light http web server featuring routing (hijack messages sent to /api and let other requests pass through). GO <a href=\"https://pkg.go.dev/net/http\">http package</a> and the type <a href=\"https://pkg.go.dev/net/http/httputil@go1.16.2#ReverseProxy\">ReverseType</a> fits like a glove in this\u00a0case.</li>\n<li>a collection of XML-enabled GO structs to unmarshal captured User-ID messages.</li>\n<li>a library featuring User-ID message payload processing capable of keeping a valid (non-expired) state of User-to-IP, Group-to-IP and Tag-to-IP maps.</li>\n</ol>\n<p>Fortunately for us there is the <a href=\"https://pkg.go.dev/github.com/xhoms/panoslib\">xhoms/panoslib</a> GO module that covers 2 and 3 (OK, let\u2019s be honest, I purpose-built the library to support this\u00a0article)</p>\n<p>With all these components it won\u2019t be that difficult to reverse proxy a PAN-OS https service monitoring User-ID messages and exposing the state of the corresponding entries on a virtual endpoint named /edl that won\u2019t conflict at all with the PAN-OS\u00a0schema.</p>\n<p>I wanted to prove the point and ended up coding the receipt as a micro-service. You can either check the code in the the GitHub repository <a href=\"https://github.com/xhoms/uidmonitor\">xhoms/uidmonitor</a> or run it from a Docker-enabled host. Take a look to the related <a href=\"https://panos.pan.dev/docs/apis/panos_tutorials_uid_monitor\">panos.pan.dev</a> tutorial for insights in the source code rationale</p>\n<pre>docker run --rm -p 8080:8080 -e TARGET=&lt;pan-os-device&gt; ghcr.io/xhoms/uidmonitor</pre>\n<h4>Some payload\u00a0examples</h4>\n<p>To get started you can check the following User-ID payload that registers the tag test to the IP addresses 10.10.10.10 and 10.10.20.20. Notice the different timeout values (100 vs 10\u00a0seconds)</p>\n<pre>POST <a href=\"http://127.0.0.1:8080/api/\">http://127.0.0.1:8080/api/</a> HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;register&gt;<br>            &lt;entry ip=\"10.10.10.10\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry ip=\"10.10.20.20\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register&gt;<br>    &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Just after pushing the payload (before the 10 second tag expires) perform the following GET request on the /edl endpoint and verify the provided list contains both IP addresses.</p>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:07:50 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<p>A few seconds (+10) later the same transaction should return only the IP address whose tag has not\u00a0expired.</p>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:08:55 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<p>Now let\u2019s use a bit more complex payload. The following one register (login) two users mapped to IP addresses and then apply the user tag admin to both of them. As in the previous case, each tag has a different expiration value.</p>\n<pre>POST <a href=\"http://127.0.0.1:8080/api/\">http://127.0.0.1:8080/api/</a> HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;login&gt;<br>            &lt;entry name=\"bar@test.local\" ip=\"10.10.20.20\" timeout=\"100\"&gt;&lt;/entry&gt;<br>            &lt;entry name=\"foo@test.local\" ip=\"10.10.10.10\" timeout=\"100\"&gt;&lt;/entry&gt;<br>        &lt;/login&gt;<br>        &lt;register-user&gt;<br>            &lt;entry user=\"foo@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry user=\"bar@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register-user&gt;<br>   &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Although the login transaction has a longer timeout on both cases, the group membership tag for the user \u201cbar@test.local\u201d is expected to timeout in 10\u00a0seconds.</p>\n<p>Calling the /edl endpoint at specific times would demonstrate the first tag expiration.</p>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=group<br>    &amp;key=admin</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:16 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<pre>...</pre>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=group<br>    &amp;key=admin<br><br>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:32 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<h3>Summary</h3>\n<p>Both EDL (because it provides \u201cuniversal\u201d coverage) and User-ID (because its advanced feature set) have their place in a large network. Implementing a reverse proxy between User-ID enabled applications and the managed PAN-OS device capable of hijacking User-ID messages have many different use cases: from basic User-ID to EDL conversion (as in the demo application) to advanced cases like applying policies to User-ID messages, enforcing maximum timeouts or transposing Address Groups into User\u00a0Groups.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=65b6a9d6c424\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/user-id-edl-better-both-of-them-65b6a9d6c424\">User-ID / EDL\u00a0\u2026 better both of them</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>User-ID or EDL\u00a0\u2026 why not\u00a0both?</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*J8LUAx85I7Hf-SKk47MZfQ.jpeg\"><figcaption>Original Photo by <a href=\"https://unsplash.com/@jontyson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Jon\u00a0Tyson</a></figcaption></figure><p>User-ID and External Dynamic Lists (EDL\u2019s) are probably the most commonly used PAN-OS features to share external IP metadata with the NGFW. The Use cases for them are enormous: from blocking known DoS sources to forward traffic from specific IP addresses on low latency\u00a0links.</p>\n<p>Let\u2019s take for instance the following logs generated by the SSH daemon in a linux\u00a0host.</p>\n<a href=\"https://medium.com/media/6a8bdd451748be2a7326a77029a6a200/href\">https://medium.com/media/6a8bdd451748be2a7326a77029a6a200/href</a><p>Won\u2019t it be great to have a way to share the IP addresses used by the user <em>xhoms</em> with the NGFW so specific security policies are applied to traffic sourced by that address just because we now know the endpoint is being managed by that\u00a0user?</p>\n<p>The following Shell CGI script could be used to create an External Dynamic List (EDL) that would list all known IP addresses the account <em>xhoms</em> was used to login into this\u00a0server.</p>\n<a href=\"https://medium.com/media/ec08fd38c48c8a005909d5b502e521cc/href\">https://medium.com/media/ec08fd38c48c8a005909d5b502e521cc/href</a><p>That is great\u00a0but:</p>\n<ul>\n<li>When would addresses be removed from the list? At the daily log\u00a0rollout?</li>\n<li>Are all listed IP addresses still being operated by the user\u00a0<em>xhoms</em>?</li>\n<li>Is there any way to remove addresses from the list at user\u00a0logout?</li>\n</ul>\n<p>To overcome these (and many other) limitations Palo Alto Networks NGFW feature a very powerful IP tagging API called User-ID. Although EDL and User-ID cover similar objectives there are fundamental technical differences between\u00a0them:</p>\n<ul>\n<li>User-ID is \u201casynchronous\u201d (push mode), supports bot \u201cset\u201d and \u201cdelete\u201d operations and provides a very flexible way to create groupings. Either by tagging address objects (Dynamic Address Group\u200a\u2014\u200aDAG ) or by mapping users to addresses and then tagging these users (Dynamic User Group\u200a\u2014\u200aDUG). User-ID allows for these tags to have a timeout in order for the PAN-OS device to take care of removing them at their due\u00a0time.</li>\n<li>EDL is \u201cuniversal\u201d. Almost all network appliance vendors provide a feature to fetch an IP address list from a\u00a0URL.</li>\n</ul>\n<p>Years ago it was up to the end customer to create its own connectors. Just like the CGI script we shared before. But as presence of PAN-OS powered NGFW\u2019s grown among enterprise customers more application vendors decided to leverage the User-ID value by providing API clients out-of-the-box. Although this is great (for Palo Alto Network customers) losing the option to fetch a list from a URL (EDL mode) makes it more difficult to integrate legacy technologies that might be out there still in the\u00a0network.</p>\n<h4>Creating EDL\u2019s from User-ID enabled applications</h4>\n<p>Let\u2019s assume you have an application that features a PAN-OS User-ID API client and that is capable of pushing log entries to the PAN-OS NGFW in an asynchronous way. Let\u2019s assume, as well, that there are still some legacy network devices in your network that need to fetch that address metadata from a URL. How difficult it would be to create a micro-service for\u00a0that?</p>\n<p>One option would be to leverage a PAN-OS XML SDK like <a href=\"https://panos.pan.dev/docs/apis/panpython_qs\">PAN Python</a> or <a href=\"https://panos.pan.dev/docs/apis/pango_qs\">PAN GO</a> and to create a REST API that extracts the current state from the PAN-OS device using its operations API.</p>\n<pre>GET https://&lt;pan-os-device&gt;/api<br>    ?key=&lt;API-KEY&gt;<br>    &amp;type=op<br>    &amp;cmd=&lt;show&gt;&lt;object&gt;&lt;registered-ip&gt;&lt;all&gt;&lt;/all&gt;&lt;/registered-ip&gt;&lt;/object&gt;&lt;/show&gt;</pre>\n<pre>HTTP/1.1 200 OK</pre>\n<pre>&lt;response status=\"success\"&gt;<br>  &lt;result&gt;<br>    &lt;entry ip=\"10.10.10.11\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;entry ip=\"10.10.10.10\" from_agent=\"0\" persistent=\"1\"&gt;<br>      &lt;tag&gt;<br>        &lt;member&gt;tag10&lt;/member&gt;<br>      &lt;/tag&gt;<br>    &lt;/entry&gt;<br>    &lt;count&gt;2&lt;/count&gt;<br>  &lt;/result&gt;<br>&lt;/response&gt;</pre>\n<p>It shouldn\u2019t be that difficult to parse the response and provide a plain list out of it, right? Come back to me if you\u2019re thinking on using a XSLT processor to pipe it at the output of a cURL command packing everything as a CGI script because we might have some common grounds\u00a0;-)</p>\n<p>I\u2019d love to propose a different approach though: to hijack User-ID messages by implementing a micro-service that behaves as a <strong>reverse proxy</strong> between the application that features the User-ID client and the PAN-OS device. I like this approach because it opens the door to other interesting use cases like enforcing timeout (adding timeout to entries that do not have it), converting DAG messages into DUG equivalents or adding additional tags based on the source application.</p>\n<h4>Components for a User-ID to EDL reverse\u00a0proxy</h4>\n<p>The ingredients for such a receipt would\u00a0be:</p>\n<ol>\n<li>a light http web server featuring routing (hijack messages sent to /api and let other requests pass through). GO <a href=\"https://pkg.go.dev/net/http\">http package</a> and the type <a href=\"https://pkg.go.dev/net/http/httputil@go1.16.2#ReverseProxy\">ReverseType</a> fits like a glove in this\u00a0case.</li>\n<li>a collection of XML-enabled GO structs to unmarshal captured User-ID messages.</li>\n<li>a library featuring User-ID message payload processing capable of keeping a valid (non-expired) state of User-to-IP, Group-to-IP and Tag-to-IP maps.</li>\n</ol>\n<p>Fortunately for us there is the <a href=\"https://pkg.go.dev/github.com/xhoms/panoslib\">xhoms/panoslib</a> GO module that covers 2 and 3 (OK, let\u2019s be honest, I purpose-built the library to support this\u00a0article)</p>\n<p>With all these components it won\u2019t be that difficult to reverse proxy a PAN-OS https service monitoring User-ID messages and exposing the state of the corresponding entries on a virtual endpoint named /edl that won\u2019t conflict at all with the PAN-OS\u00a0schema.</p>\n<p>I wanted to prove the point and ended up coding the receipt as a micro-service. You can either check the code in the the GitHub repository <a href=\"https://github.com/xhoms/uidmonitor\">xhoms/uidmonitor</a> or run it from a Docker-enabled host. Take a look to the related <a href=\"https://panos.pan.dev/docs/apis/panos_tutorials_uid_monitor\">panos.pan.dev</a> tutorial for insights in the source code rationale</p>\n<pre>docker run --rm -p 8080:8080 -e TARGET=&lt;pan-os-device&gt; ghcr.io/xhoms/uidmonitor</pre>\n<h4>Some payload\u00a0examples</h4>\n<p>To get started you can check the following User-ID payload that registers the tag test to the IP addresses 10.10.10.10 and 10.10.20.20. Notice the different timeout values (100 vs 10\u00a0seconds)</p>\n<pre>POST <a href=\"http://127.0.0.1:8080/api/\">http://127.0.0.1:8080/api/</a> HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;register&gt;<br>            &lt;entry ip=\"10.10.10.10\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry ip=\"10.10.20.20\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;test&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register&gt;<br>    &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Just after pushing the payload (before the 10 second tag expires) perform the following GET request on the /edl endpoint and verify the provided list contains both IP addresses.</p>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:07:50 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<p>A few seconds (+10) later the same transaction should return only the IP address whose tag has not\u00a0expired.</p>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=tag<br>    &amp;key=test</pre>\n<pre>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:08:55 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<p>Now let\u2019s use a bit more complex payload. The following one register (login) two users mapped to IP addresses and then apply the user tag admin to both of them. As in the previous case, each tag has a different expiration value.</p>\n<pre>POST <a href=\"http://127.0.0.1:8080/api/\">http://127.0.0.1:8080/api/</a> HTTP/1.1<br>Content-Type: application/x-www-form-urlencoded</pre>\n<pre>key=&lt;my-api-key&gt;<br>&amp;type=user-id<br>&amp;cmd=&lt;uid-message&gt;<br>    &lt;type&gt;update&lt;/type&gt;<br>    &lt;payload&gt;<br>        &lt;login&gt;<br>            &lt;entry name=\"bar@test.local\" ip=\"10.10.20.20\" timeout=\"100\"&gt;&lt;/entry&gt;<br>            &lt;entry name=\"foo@test.local\" ip=\"10.10.10.10\" timeout=\"100\"&gt;&lt;/entry&gt;<br>        &lt;/login&gt;<br>        &lt;register-user&gt;<br>            &lt;entry user=\"foo@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"100\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>            &lt;entry user=\"bar@test.local\"&gt;<br>                &lt;tag&gt;<br>                    &lt;member timeout=\"10\"&gt;admin&lt;/member&gt;<br>                &lt;/tag&gt;<br>            &lt;/entry&gt;<br>        &lt;/register-user&gt;<br>   &lt;/payload&gt;<br>&lt;/uid-message&gt;</pre>\n<p>Although the login transaction has a longer timeout on both cases, the group membership tag for the user \u201cbar@test.local\u201d is expected to timeout in 10\u00a0seconds.</p>\n<p>Calling the /edl endpoint at specific times would demonstrate the first tag expiration.</p>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=group<br>    &amp;key=admin</pre>\n<pre>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:16 GMT<br>Content-Length: 24<br>Connection: close</pre>\n<pre>10.10.20.20<br>10.10.10.10</pre>\n<pre>...</pre>\n<pre>GET <a href=\"http://127.0.0.1:8080/edl/\">http://127.0.0.1:8080/edl/</a><br>    ?list=group<br>    &amp;key=admin<br><br>HTTP/1.1 200 OK<br>Content-Type: text/plain<br>Date: Mon, 12 Apr 2021 10:28:32 GMT<br>Content-Length: 12<br>Connection: close</pre>\n<pre>10.10.10.10</pre>\n<h3>Summary</h3>\n<p>Both EDL (because it provides \u201cuniversal\u201d coverage) and User-ID (because its advanced feature set) have their place in a large network. Implementing a reverse proxy between User-ID enabled applications and the managed PAN-OS device capable of hijacking User-ID messages have many different use cases: from basic User-ID to EDL conversion (as in the demo application) to advanced cases like applying policies to User-ID messages, enforcing maximum timeouts or transposing Address Groups into User\u00a0Groups.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=65b6a9d6c424\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/user-id-edl-better-both-of-them-65b6a9d6c424\">User-ID / EDL\u00a0\u2026 better both of them</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["pano","go","palo-alto-networks","user-id"]},{"title":"Ingest PAN-OS Alerts into Cortex XDR Pro Endpoint","pubDate":"2021-03-19 14:49:33","link":"https://medium.com/palo-alto-networks-developer-blog/ingest-pan-os-alerts-into-cortex-xdr-pro-endpoint-8eef3e59b264?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/8eef3e59b264","author":"Amine Basli","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*_LxpVyZVZbbKSPOIjGvJdA.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_LxpVyZVZbbKSPOIjGvJdA.jpeg\"><figcaption>Photo by callmefred.com</figcaption></figure><blockquote>In this blog I\u2019m sharing my experience leveraging Cortex XDR API\u2019s to tailor-fit this product into specific customer requirements<strong><em>.</em></strong>\n</blockquote>\n<p>I bet that if you\u2019re a Systems Engineer operating in EMEA you\u2019ve argued more than once about the way products and licenses are packaged from an opportunity size point of view (too big for your customer base). There are many reasons that explain why a given vendor would like to have a reduced set of SKUs. But, no doubt, the fewer SKUs you have the lower the chances are for a given product to fit exactly into the customer\u2019s need from a sizing standpoint.</p>\n<p>At Palo Alto Networks we\u2019re no different, but we execute on two principles that help bridge the\u00a0gaps:</p>\n<ul>\n<li>Provide APIs for our\u00a0products</li>\n<li>Encourage the developer community to use them through our Developer Relations team</li>\n</ul>\n<h3>The use\u00a0case</h3>\n<p>A <a href=\"https://www.paloaltonetworks.com/cortex/cortex-xdr\">Cortex XDR Pro Endpoint</a> customer was interested in ingesting threat logs from their PAN-OS NGFW into his tenant to stitch them with the agent\u2019s alerts and incidents. The customer was aware that this requirement could be achieved by sharing the NGFW state into the cloud-based Cortex Data Lake. But the corresponding SKU was overkill from a feature point of view (it provides not only alert stitching but ML baselining of traffic behaviour as well) and an unreachable cost from a budgeting perspective.</p>\n<p>Cortex XDR Pro provides a <a href=\"https://docs.paloaltonetworks.com/cortex/cortex-xdr/cortex-xdr-api.html\">REST API</a> to ingest third-party alerts to cover this specific use case. It is rate limited to only 600 alerts per minute per tenant but was more than enough for my customer because they were only interested in medium to critical alerts that appeared at a much lower frequency in their network. What about leveraging this API to provide an exact match to my customer\u2019s requirement?</p>\n<p>On the other end, PAN-OS can be configured to forward these filtered alerts natively to any REST API with a very flexible payload templating feature. So at first it looked like we could \u201cconnect\u201d the PAN-OS HTTP Log Forwarding feature with the <a href=\"https://docs.paloaltonetworks.com/cortex/cortex-xdr/cortex-xdr-api/cortex-xdr-apis/incident-management/insert-parsed-alerts.html#insert-parsed-alerts\">Cortex XDR Insert Parsed Alert API</a>. But a deep dive analysis revealed inconsistencies between the mandatory timestamp field format (it must be presented as UNIX milliseconds for XDR to accept\u00a0it)</p>\n<p>It was too close to give up. I just needed a transformation pipeline that could be used as a middleman between the PAN-OS HTTP log forwarding feature and the Cortex XDR Insert Parsed Alert API. Provided I was ready for a middleman, I\u2019d like it to enforce the XDR API quota limitations (600 alerts per minute / up to 60 alerts per\u00a0update)</p>\n<h3>Developers to the\u00a0rescue</h3>\n<p>I engaged our Developer Relations team in Palo Alto Networks because they\u2019re always eager to discuss new use cases for our APIs. A quick discussion ended up with the following architecture proposal:</p>\n<ul>\n<li>An HTTP server would be implemented to be used as the endpoint for PAN-OS alert ingestion. Basic authentication would be implemented by providing a pre-shared key in the Authentication header.</li>\n<li>A transformation pipeline would convert the PAN-OS payload into a ready-to-consume XDR parsed alert API payload. The pipeline would take care, as well, to enforce the XDR API quota limits buffering bursts as\u00a0needed.</li>\n<li>The XDR API client implementation would use an Advanced API Key for authentication.</li>\n<li>Everything would be packaged in a Docker container to ease its deployment. Configuration parameters would be provided to the container as environment variables.</li>\n</ul>\n<p>Implementation details ended up shared in a document available in <a href=\"https://cortex.pan.dev/docs/tutorials/xdr_alert_ingestion\">cortex.pan.dev</a>. If you\u2019re in the mood of creating your own implementation I highly recommend you taking the time to go over the whole tutorial. If you just want to get to the point then you can use this <a href=\"https://github.com/users/xhoms/packages/container/package/xdrgateway\">ready-to-go container image</a>.</p>\n<h3>Let\u2019s get\u00a0started</h3>\n<p>I opted for using the available container image. Let me guide you through my experience using it to fulfil my customer\u2019s request.</p>\n<p>First of all the application requires some configuration data that must be provided as a set of environmental variables. A few of them are mandatory, others are just optional with default\u00a0values.</p>\n<p>The following are the required variables (the application will refuse to start without\u00a0them)</p>\n<ul>\n<li>API_KEY:<strong> </strong>XDR API Key (Advanced)</li>\n<li>API_KEY_ID:<strong> </strong>The XDR API Key identifier (its sequence\u00a0number)</li>\n<li>FQDN:<strong> </strong>Fully Qualified Domain Name of the corresponding XDR Instance (i.e. myxdr.xdr.us.paloaltonetworks.com)</li>\n</ul>\n<p>The following are optional variables</p>\n<ul>\n<li>PSK: the server will check the value in the Authorization header to accept the request (default to no authentication)</li>\n<li>DEBUG: if it exists then the engine will be more verbose (defaults to\u00a0false)</li>\n<li>PORT: TCP port to bind the HTTP server to (defaults to\u00a08080)</li>\n<li>OFFSET: PAN-OS timestamp does not include time zone. By default, they will be considered in UTC (defaults to +0\u00a0hours)</li>\n<li>QUOTA_SIZE: XDR ingestion alert quota (defaults to\u00a0600)</li>\n<li>QUOTA_SECONDS: XDR ingestion alert quota refresh period (defaults to 60\u00a0seconds)</li>\n<li>UPDATE_SIZE: XDR ingestion alert max number of alerts per update (defaults to\u00a060)</li>\n<li>BUFFER_SIZE: size of the pipe buffer (defaults to 6000 = 10\u00a0minutes)</li>\n<li>T1: how often the pipe buffer polled for new alerts (defaults to 2\u00a0seconds)</li>\n</ul>\n<p>For more details, you can check public repository <a href=\"https://github.com/xhoms/xdrgateway\">documentation</a></p>\n<h4>Step 1: <strong>Generate an Advanced API key on Cortex\u00a0XDR</strong>\n</h4>\n<p>Connect to your Cortex XDR instance and navigate to Setting &gt; API Keys Generate an API Key of type Advanced granting the Administrator role to it (that role is required for Alert ingestion)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*l6_ws7RtoivHjme5\"></figure><h4><strong>Step 2: Run the container image</strong></h4>\n<p>Assuming you have Docker installed on your computer the following command line command would pull the image and run the micro-service with configuration options passed as environmental variables.</p>\n<pre>docker run -rm -p 8080:8080 -e PSK=hello -e FQDN=xxx.xdr.us.paloaltonetworks.com -e API_KEY=&lt;my-api-key&gt; -e API_KEY_ID=&lt;my-key-id&gt; -e DEBUG=yes ghcr.io/xhoms/xdrgateway</pre>\n<p>The Debug option provides more verbosity and it is recommended for initial experimentation. If everything goes as expected you\u2019ll see a log message like the following one.</p>\n<pre>2021/03/17 21:32:25 starting http service on port 8080</pre>\n<h4>Step 3: Perform a quick test to verify the micro-service is\u00a0running</h4>\n<p>Use any API test tool (I like good-old Curl) to push a test\u00a0payload.</p>\n<pre>curl -X POST -H \"Authorization: hello\" \"<a href=\"http://rancher:8081/in\">http://127.0.0.1:8080/in</a>\" -d '{\"src\":\"1.2.3.4\",\"sport\":1234,\"dst\":\"4.3.2.1\",\"dport\": 4231,\"time_generated\":\"2021/01/06 20:44:34\",\"rule\":\"test_rule\",\"serial\":\"9999\",\"sender_sw_version\":\"10.0.4\",\"subtype\":\"spyware\",\"threat_name\":\"bad-bad-c2c\",\"severity\":\"critical\",\"action\":\"alert\"}---annex---\"Mozi Command and Control Traffic Detection\"'</pre>\n<p>You should receive an empty status 200 response and see log messages in the server console like the following ones:</p>\n<pre>2021/03/17 21:52:16 api - successfully parsed alert<br>2021/03/17 21:52:17 xdrclient - successful call to insert_parsed_alerts</pre>\n<h4><strong>Step 4: Configure HTTP log Forward on the Firewall:</strong></h4>\n<p>PAN-OS NGFW can forward alerts to HTTP/S endpoints. The feature configuration is available in Device &gt; Server Profiles &gt; HTTP and, for this case, you should use the following parameters</p>\n<ul>\n<li>IP\u00a0: IP address of the container host</li>\n<li>Protocol\u00a0:\u00a0HTTP</li>\n<li>Port: 8080(default)</li>\n<li>HTTP Method\u00a0POST</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ikHuhU9BUhhzBjZP\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/847/1*6kZ_SHkUIcW6vsH_LMml0Q.png\"></figure><p>Some notes regarding the payload\u00a0format</p>\n<ul>\n<li>URI Format:\u201c/in\u201d is the endpoint where the micro-service listen for POST requests containing new\u00a0alerts</li>\n<li>Headers: Notice we\u2019re setting the value hello in the Authorization header to match the -e PSK=hello configuration variable we passed to the micro-service</li>\n<li>Payload: The variable $threat_name was introduced with PAN-OS 10.0. If you\u2019re using older versions of PAN-OS the you can use the variable $threatid instead</li>\n</ul>\n<p>The last action to complete this job is to create a new Log Forwarding profile that will consume our recently created HTTP server and to use it in all security rules we want their alerts being ingested into XDR. The configuration object is available at Objects &gt; Log Forwarding</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*9ZrWMDpMosw9baso\"></figure><h4>Step 5: Final\u00a0checks</h4>\n<p>As your PAN-OS NGFW starts generating alerts, you should see activity in the micro-service\u2019s output log and alerts being ingested into the Cortex XDR instance. The source tag Palo Alto Networks\u200a\u2014\u200aPAN-OS clearly indicates these alerts were pushed by our deployment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LntWo1GBlfTHKCYWG1afEA.png\"></figure><h3>Production-ready version</h3>\n<p>There a couple of additional steps to perform before considering the micro-service production-ready.</p>\n<ul>\n<li>The service should be exposed over a secure channel (TLS). Best option is to leverage the preferred forward-proxy environment available in our container environment. Notice the image honours the PORT env variable and should work out-of-the-box almost everywhere</li>\n<li>A restart-policy should be in place to restart the container in case of a crash. It is not a good idea to run more than one instance in a load balancing group because the quota enforcement won\u2019t be synchronised between\u00a0them</li>\n</ul>\n<h3>Summary</h3>\n<p>Palo Alto Networks products and services are built with automation and customisation in mind. They feature rich API\u2019s that can be used to tailor them to specific customer\u00a0needs.</p>\n<p>In my case these APIs provided the customer with a\u00a0choice:</p>\n<ul>\n<li>Either wait for the scope of the project (and its budget) to increase in order to accommodate additional products like Cortex <a href=\"https://www.paloaltonetworks.com/cortex/network-traffic-analysis\">Network Traffic Analysis</a>\u00a0<strong>or</strong>\n</li>\n<li>Deploy a compact \u201cmiddle-man\u201d that would connect the PAN-OS and Cortex XDR\u00a0API\u2019s</li>\n</ul>\n<p>This experience empowered me by acquiring DevOps knowledge (building micro-services) I\u2019m sure I\u2019ll use in many opportunities to come. Special thanks to The Developer Relations team at Palo Alto Networks who provided documentation and examples, and were eager to explore this new use case. I couldn\u2019t have done it without their help and the work they\u2019ve been putting into improving our developer experience.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8eef3e59b264\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/ingest-pan-os-alerts-into-cortex-xdr-pro-endpoint-8eef3e59b264\">Ingest PAN-OS Alerts into Cortex XDR Pro Endpoint</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_LxpVyZVZbbKSPOIjGvJdA.jpeg\"><figcaption>Photo by callmefred.com</figcaption></figure><blockquote>In this blog I\u2019m sharing my experience leveraging Cortex XDR API\u2019s to tailor-fit this product into specific customer requirements<strong><em>.</em></strong>\n</blockquote>\n<p>I bet that if you\u2019re a Systems Engineer operating in EMEA you\u2019ve argued more than once about the way products and licenses are packaged from an opportunity size point of view (too big for your customer base). There are many reasons that explain why a given vendor would like to have a reduced set of SKUs. But, no doubt, the fewer SKUs you have the lower the chances are for a given product to fit exactly into the customer\u2019s need from a sizing standpoint.</p>\n<p>At Palo Alto Networks we\u2019re no different, but we execute on two principles that help bridge the\u00a0gaps:</p>\n<ul>\n<li>Provide APIs for our\u00a0products</li>\n<li>Encourage the developer community to use them through our Developer Relations team</li>\n</ul>\n<h3>The use\u00a0case</h3>\n<p>A <a href=\"https://www.paloaltonetworks.com/cortex/cortex-xdr\">Cortex XDR Pro Endpoint</a> customer was interested in ingesting threat logs from their PAN-OS NGFW into his tenant to stitch them with the agent\u2019s alerts and incidents. The customer was aware that this requirement could be achieved by sharing the NGFW state into the cloud-based Cortex Data Lake. But the corresponding SKU was overkill from a feature point of view (it provides not only alert stitching but ML baselining of traffic behaviour as well) and an unreachable cost from a budgeting perspective.</p>\n<p>Cortex XDR Pro provides a <a href=\"https://docs.paloaltonetworks.com/cortex/cortex-xdr/cortex-xdr-api.html\">REST API</a> to ingest third-party alerts to cover this specific use case. It is rate limited to only 600 alerts per minute per tenant but was more than enough for my customer because they were only interested in medium to critical alerts that appeared at a much lower frequency in their network. What about leveraging this API to provide an exact match to my customer\u2019s requirement?</p>\n<p>On the other end, PAN-OS can be configured to forward these filtered alerts natively to any REST API with a very flexible payload templating feature. So at first it looked like we could \u201cconnect\u201d the PAN-OS HTTP Log Forwarding feature with the <a href=\"https://docs.paloaltonetworks.com/cortex/cortex-xdr/cortex-xdr-api/cortex-xdr-apis/incident-management/insert-parsed-alerts.html#insert-parsed-alerts\">Cortex XDR Insert Parsed Alert API</a>. But a deep dive analysis revealed inconsistencies between the mandatory timestamp field format (it must be presented as UNIX milliseconds for XDR to accept\u00a0it)</p>\n<p>It was too close to give up. I just needed a transformation pipeline that could be used as a middleman between the PAN-OS HTTP log forwarding feature and the Cortex XDR Insert Parsed Alert API. Provided I was ready for a middleman, I\u2019d like it to enforce the XDR API quota limitations (600 alerts per minute / up to 60 alerts per\u00a0update)</p>\n<h3>Developers to the\u00a0rescue</h3>\n<p>I engaged our Developer Relations team in Palo Alto Networks because they\u2019re always eager to discuss new use cases for our APIs. A quick discussion ended up with the following architecture proposal:</p>\n<ul>\n<li>An HTTP server would be implemented to be used as the endpoint for PAN-OS alert ingestion. Basic authentication would be implemented by providing a pre-shared key in the Authentication header.</li>\n<li>A transformation pipeline would convert the PAN-OS payload into a ready-to-consume XDR parsed alert API payload. The pipeline would take care, as well, to enforce the XDR API quota limits buffering bursts as\u00a0needed.</li>\n<li>The XDR API client implementation would use an Advanced API Key for authentication.</li>\n<li>Everything would be packaged in a Docker container to ease its deployment. Configuration parameters would be provided to the container as environment variables.</li>\n</ul>\n<p>Implementation details ended up shared in a document available in <a href=\"https://cortex.pan.dev/docs/tutorials/xdr_alert_ingestion\">cortex.pan.dev</a>. If you\u2019re in the mood of creating your own implementation I highly recommend you taking the time to go over the whole tutorial. If you just want to get to the point then you can use this <a href=\"https://github.com/users/xhoms/packages/container/package/xdrgateway\">ready-to-go container image</a>.</p>\n<h3>Let\u2019s get\u00a0started</h3>\n<p>I opted for using the available container image. Let me guide you through my experience using it to fulfil my customer\u2019s request.</p>\n<p>First of all the application requires some configuration data that must be provided as a set of environmental variables. A few of them are mandatory, others are just optional with default\u00a0values.</p>\n<p>The following are the required variables (the application will refuse to start without\u00a0them)</p>\n<ul>\n<li>API_KEY:<strong> </strong>XDR API Key (Advanced)</li>\n<li>API_KEY_ID:<strong> </strong>The XDR API Key identifier (its sequence\u00a0number)</li>\n<li>FQDN:<strong> </strong>Fully Qualified Domain Name of the corresponding XDR Instance (i.e. myxdr.xdr.us.paloaltonetworks.com)</li>\n</ul>\n<p>The following are optional variables</p>\n<ul>\n<li>PSK: the server will check the value in the Authorization header to accept the request (default to no authentication)</li>\n<li>DEBUG: if it exists then the engine will be more verbose (defaults to\u00a0false)</li>\n<li>PORT: TCP port to bind the HTTP server to (defaults to\u00a08080)</li>\n<li>OFFSET: PAN-OS timestamp does not include time zone. By default, they will be considered in UTC (defaults to +0\u00a0hours)</li>\n<li>QUOTA_SIZE: XDR ingestion alert quota (defaults to\u00a0600)</li>\n<li>QUOTA_SECONDS: XDR ingestion alert quota refresh period (defaults to 60\u00a0seconds)</li>\n<li>UPDATE_SIZE: XDR ingestion alert max number of alerts per update (defaults to\u00a060)</li>\n<li>BUFFER_SIZE: size of the pipe buffer (defaults to 6000 = 10\u00a0minutes)</li>\n<li>T1: how often the pipe buffer polled for new alerts (defaults to 2\u00a0seconds)</li>\n</ul>\n<p>For more details, you can check public repository <a href=\"https://github.com/xhoms/xdrgateway\">documentation</a></p>\n<h4>Step 1: <strong>Generate an Advanced API key on Cortex\u00a0XDR</strong>\n</h4>\n<p>Connect to your Cortex XDR instance and navigate to Setting &gt; API Keys Generate an API Key of type Advanced granting the Administrator role to it (that role is required for Alert ingestion)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*l6_ws7RtoivHjme5\"></figure><h4><strong>Step 2: Run the container image</strong></h4>\n<p>Assuming you have Docker installed on your computer the following command line command would pull the image and run the micro-service with configuration options passed as environmental variables.</p>\n<pre>docker run -rm -p 8080:8080 -e PSK=hello -e FQDN=xxx.xdr.us.paloaltonetworks.com -e API_KEY=&lt;my-api-key&gt; -e API_KEY_ID=&lt;my-key-id&gt; -e DEBUG=yes ghcr.io/xhoms/xdrgateway</pre>\n<p>The Debug option provides more verbosity and it is recommended for initial experimentation. If everything goes as expected you\u2019ll see a log message like the following one.</p>\n<pre>2021/03/17 21:32:25 starting http service on port 8080</pre>\n<h4>Step 3: Perform a quick test to verify the micro-service is\u00a0running</h4>\n<p>Use any API test tool (I like good-old Curl) to push a test\u00a0payload.</p>\n<pre>curl -X POST -H \"Authorization: hello\" \"<a href=\"http://rancher:8081/in\">http://127.0.0.1:8080/in</a>\" -d '{\"src\":\"1.2.3.4\",\"sport\":1234,\"dst\":\"4.3.2.1\",\"dport\": 4231,\"time_generated\":\"2021/01/06 20:44:34\",\"rule\":\"test_rule\",\"serial\":\"9999\",\"sender_sw_version\":\"10.0.4\",\"subtype\":\"spyware\",\"threat_name\":\"bad-bad-c2c\",\"severity\":\"critical\",\"action\":\"alert\"}---annex---\"Mozi Command and Control Traffic Detection\"'</pre>\n<p>You should receive an empty status 200 response and see log messages in the server console like the following ones:</p>\n<pre>2021/03/17 21:52:16 api - successfully parsed alert<br>2021/03/17 21:52:17 xdrclient - successful call to insert_parsed_alerts</pre>\n<h4><strong>Step 4: Configure HTTP log Forward on the Firewall:</strong></h4>\n<p>PAN-OS NGFW can forward alerts to HTTP/S endpoints. The feature configuration is available in Device &gt; Server Profiles &gt; HTTP and, for this case, you should use the following parameters</p>\n<ul>\n<li>IP\u00a0: IP address of the container host</li>\n<li>Protocol\u00a0:\u00a0HTTP</li>\n<li>Port: 8080(default)</li>\n<li>HTTP Method\u00a0POST</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ikHuhU9BUhhzBjZP\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/847/1*6kZ_SHkUIcW6vsH_LMml0Q.png\"></figure><p>Some notes regarding the payload\u00a0format</p>\n<ul>\n<li>URI Format:\u201c/in\u201d is the endpoint where the micro-service listen for POST requests containing new\u00a0alerts</li>\n<li>Headers: Notice we\u2019re setting the value hello in the Authorization header to match the -e PSK=hello configuration variable we passed to the micro-service</li>\n<li>Payload: The variable $threat_name was introduced with PAN-OS 10.0. If you\u2019re using older versions of PAN-OS the you can use the variable $threatid instead</li>\n</ul>\n<p>The last action to complete this job is to create a new Log Forwarding profile that will consume our recently created HTTP server and to use it in all security rules we want their alerts being ingested into XDR. The configuration object is available at Objects &gt; Log Forwarding</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*9ZrWMDpMosw9baso\"></figure><h4>Step 5: Final\u00a0checks</h4>\n<p>As your PAN-OS NGFW starts generating alerts, you should see activity in the micro-service\u2019s output log and alerts being ingested into the Cortex XDR instance. The source tag Palo Alto Networks\u200a\u2014\u200aPAN-OS clearly indicates these alerts were pushed by our deployment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LntWo1GBlfTHKCYWG1afEA.png\"></figure><h3>Production-ready version</h3>\n<p>There a couple of additional steps to perform before considering the micro-service production-ready.</p>\n<ul>\n<li>The service should be exposed over a secure channel (TLS). Best option is to leverage the preferred forward-proxy environment available in our container environment. Notice the image honours the PORT env variable and should work out-of-the-box almost everywhere</li>\n<li>A restart-policy should be in place to restart the container in case of a crash. It is not a good idea to run more than one instance in a load balancing group because the quota enforcement won\u2019t be synchronised between\u00a0them</li>\n</ul>\n<h3>Summary</h3>\n<p>Palo Alto Networks products and services are built with automation and customisation in mind. They feature rich API\u2019s that can be used to tailor them to specific customer\u00a0needs.</p>\n<p>In my case these APIs provided the customer with a\u00a0choice:</p>\n<ul>\n<li>Either wait for the scope of the project (and its budget) to increase in order to accommodate additional products like Cortex <a href=\"https://www.paloaltonetworks.com/cortex/network-traffic-analysis\">Network Traffic Analysis</a>\u00a0<strong>or</strong>\n</li>\n<li>Deploy a compact \u201cmiddle-man\u201d that would connect the PAN-OS and Cortex XDR\u00a0API\u2019s</li>\n</ul>\n<p>This experience empowered me by acquiring DevOps knowledge (building micro-services) I\u2019m sure I\u2019ll use in many opportunities to come. Special thanks to The Developer Relations team at Palo Alto Networks who provided documentation and examples, and were eager to explore this new use case. I couldn\u2019t have done it without their help and the work they\u2019ve been putting into improving our developer experience.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8eef3e59b264\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/ingest-pan-os-alerts-into-cortex-xdr-pro-endpoint-8eef3e59b264\">Ingest PAN-OS Alerts into Cortex XDR Pro Endpoint</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["cortex-xdr","containers","pan-os-alerts","api","ingestion"]},{"title":"Enterprise API design practices: Part 4","pubDate":"2021-02-23 10:10:38","link":"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/b94d9e8532f5","author":"Francesco Vigo","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*SmWJPvCF1Y5OMz_Wvxe5Vg.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SmWJPvCF1Y5OMz_Wvxe5Vg.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@dallimonti?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Andr\u00e9s Dallimonti</a> on\u00a0<a href=\"https://unsplash.com/s/photos/cockpit?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Welcome to the last part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. I <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">initially</a> covered some background context and the importance of design, <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">then</a> I presented security and backend protection and in the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">third chapter</a> I explored optimizations and scale; this post is about monitoring and Developer Experience (DX).</p>\n<h3>5. Monitor your\u00a0APIs</h3>\n<p>Don\u2019t fly blind: you must make sure that your APIs are properly instrumented so you can monitor what\u2019s going on. This is important for a lot of good reasons, such\u00a0as:</p>\n<ul>\n<li>\n<strong>Security: </strong>is your service under attack or being used maliciously? You should always be able to figure out if there are anomalies and react swiftly to prevent incidents.</li>\n<li>\n<strong>Auditing</strong>: depending on the nature of your business, you might be required by compliance or investigative reasons to produce an audit trail of user activity of your product, which requires proper instrumentation of your APIs as well as logging\u00a0events.</li>\n<li>\n<strong>Performance and scale</strong>: you should be aware of how your API and backend are performing and if the key metrics are within the acceptable ranges, way before your users start complaining and your business is impacted. It\u2019s always better to optimize before performance becomes a real\u00a0problem.</li>\n<li>\n<strong>Cost:</strong> similarly, with proper instrumentation, you can be aware of your infrastructure costs sustained to serve a certain amount of traffic. That can help you with capacity planning and cost modeling if you\u2019re monetizing your service. And to avoid unexpected bills at the end of the month that might force you to disrupt the\u00a0service.</li>\n<li>\n<strong>Feedback</strong>: with proper telemetry data to look at when you deploy a change, you can understand how it\u2019s performing and whether it was a good idea to implement it. It also allows you to implement prototyping techniques such as A/B\u00a0testing.</li>\n<li>\n<strong>Learn</strong>: analyzing how developers use your API can be a great source of learning. It will give you useful insights on how your service is being consumed and is a valuable source of ideas that you can evaluate for new features (i.e. new use-case driven API endpoints).</li>\n</ul>\n<p>Proper instrumentation of services is a vast topic and here I just want to summarize a few items that are usually easy to implement:</p>\n<ul>\n<li>Every API request should have a unique operation identifier that is stored in your logs and can help your ops team figure out what happened. This identifier should also be reported back to clients somewhere (usually in API response the headers) especially, but not exclusively, in case of\u00a0errors.</li>\n<li>Keep an eye on API requests that fail with server errors (i.e. the HTTP 5xx ones) and, if the number is non-negligible, try to pinpoint the root cause: is it a bug, or some request that is causing timeouts in the backend? Can you fix it or make it\u00a0faster?</li>\n<li>Keep a reasonable log history to allow tracking errors and auditing user activity for at least several days back in\u00a0time.</li>\n<li>Create dashboards that help you monitor user activity, API usage, security metrics, etc. And make sure to <strong>check them\u00a0often</strong>.</li>\n</ul>\n<h3>6. Make Developer Experience a\u00a0priority</h3>\n<p>Last principle, but definitely not the least important. Even if you are lucky and the developers that use your APIs are required to do so because of an external mandate, you shouldn\u2019t make their experience less positive.</p>\n<blockquote>In fact, the Developer Experience of your product should be\u00a0awesome.</blockquote>\n<p>If developers properly understand how to work with your APIs, and enjoy doing so, they will likely run into fewer issues and be more patient when trying to overcome them. They will also provide you with good quality feedback. It will reduce the number of requests they generate on your support teams. They will give you insights and use case ideas for building a better\u00a0product.</p>\n<blockquote>And, more importantly, happy developers will ultimately build better products for their own customers which, in turn, will act as a force multiplier for the value and success of your own product. Everybody wins in the API Economy model: customers, business stakeholders, partners, product teams, engineers, support\u00a0teams.</blockquote>\n<p>I\u2019ve witnessed several situations where developers were so exhausted and frustrated with working against a bad API that, as soon as they saw the light at the end of the tunnel, they stopped thinking creatively and just powered their way through to an MVP that was far from viable and valuable. But it checked the box they needed so they were allowed to move on. I consider this a very specific scenario of <em>developer fatigue</em>: let\u2019s call it \u201c<strong>API Fatigue</strong>\u201d.</p>\n<p>Luckily, I\u2019ve also experienced the other way around, where new, unplanned, great features were added because the DX was good and we had fun integrating things together.</p>\n<p>There are many resources out there that describe how to make APIs with great developer experience: the most obvious one is to create APIs that are clear and simple to consume. Apply the <a href=\"https://en.wikipedia.org/wiki/Principle_of_least_astonishment\">Principle of least astonishment</a>.</p>\n<p>I recommend considering the following when shipping an\u00a0API:</p>\n<ul>\n<li>\n<strong>Document your API properly</strong>: it\u2019s almost certain that the time you spend creating proper documentation at the beginning is saved later on when developers start consuming it. Specification files, such as <a href=\"https://swagger.io/specification/\">OpenAPI Specification</a> (OAS) tremendously help here. Also, if you follow the design-first approach that we discussed in the first post of this series, you\u2019ll probably already have a specification file ready to\u00a0use.</li>\n<li>\n<strong>Support different learning paths</strong>: some developers like to read all the documentation from top to bottom before writing a single line of code, others will start with the code editor right away. Instead of forcing a learning path, try to embrace the different mindsets and provide tools for everyone: specification files, <a href=\"https://www.postman.com/collection/\">Postman collections</a>, examples in different programming languages, an easy-to-access API sandbox (i.e. something that doesn\u2019t require installing an Enterprise product and waiting 2 weeks to get a license to make your first API call), a developer portal, tutorials and, why not, video walkthroughs. This might sound overwhelming but pays off in the end. With the proper design done first a lot of this content can be autogenerated from the specification files.</li>\n<li>\n<strong>Document the data structure</strong>: if you can, don\u2019t just add lists of properties to your specs and docs: strive to provide proper descriptions of the fields that your API uses so that developers that are not fully familiar with your product can understand them. Remove ambiguity from the documentation as much as possible. This can go a long way, as developers can make mental models by understanding the data properly that often leads to better use cases than \u201c<em>just pull some data from this\u00a0API\u201d.</em>\n</li>\n<li>\n<strong>Use verbs, status codes, and error descriptions properly</strong>: leverage the power of the protocol you are using (i.e. HTTP when using REST APIs) to define how to do things and what responses mean. Proper usage of status codes and good error messages will dramatically reduce the number of requests to your support team. Developers are smart and want to solve problems quickly so if you provide them with the right information to do so, they won\u2019t bother you. Also, if you are properly logging and monitoring your API behavior, it will be easier for your support team to troubleshoot if your errors are not all \u201c500 Internal Server Error\u201d without any other\u00a0detail.</li>\n</ul>\n<p>Finally,<strong> stay close to the developers</strong>: especially if your API is being used by people external from your organization, it\u2019s incredibly important to be close to them as much as you can to support them, learn and gather feedback on your API and its documentation. Allow everyone that is responsible for designing and engineering your API to be in that feedback loop, so they can share the learnings. Consider creating a community where people can ask questions and can expect quick answers (Slack, Reddit, Stack Overflow, etc.). I\u2019ve made great friendships this\u00a0way!</p>\n<h3>A few\u00a0examples</h3>\n<p>There are many great APIs out there. Here is a small, not complete, list of products from other companies that, for one reason or another, are strong examples of what I described in this blog\u00a0series:</p>\n<ul>\n<li><a href=\"https://developer.microsoft.com/en-us/graph\">Microsoft Graph</a></li>\n<li><a href=\"https://cloud.google.com/apis\">Google Cloud</a></li>\n<li><a href=\"https://developers.virustotal.com/\">VirusTotal</a></li>\n<li>GitHub <a href=\"https://docs.github.com/en/rest\">REST</a> and <a href=\"https://docs.github.com/en/graphql\">GraphQL</a>\u00a0APIs</li>\n</ul>\n<h3>Conclusion</h3>\n<p>And that\u2019s a wrap, thanks for reading! There are more things that I\u2019ve learned that I might share in future posts, but in my opinion, these are the most relevant ones. I hope you found this series useful: looking forward to hearing your feedback and comments!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b94d9e8532f5\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">Enterprise API design practices: Part 4</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SmWJPvCF1Y5OMz_Wvxe5Vg.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@dallimonti?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Andr\u00e9s Dallimonti</a> on\u00a0<a href=\"https://unsplash.com/s/photos/cockpit?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Welcome to the last part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. I <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">initially</a> covered some background context and the importance of design, <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">then</a> I presented security and backend protection and in the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">third chapter</a> I explored optimizations and scale; this post is about monitoring and Developer Experience (DX).</p>\n<h3>5. Monitor your\u00a0APIs</h3>\n<p>Don\u2019t fly blind: you must make sure that your APIs are properly instrumented so you can monitor what\u2019s going on. This is important for a lot of good reasons, such\u00a0as:</p>\n<ul>\n<li>\n<strong>Security: </strong>is your service under attack or being used maliciously? You should always be able to figure out if there are anomalies and react swiftly to prevent incidents.</li>\n<li>\n<strong>Auditing</strong>: depending on the nature of your business, you might be required by compliance or investigative reasons to produce an audit trail of user activity of your product, which requires proper instrumentation of your APIs as well as logging\u00a0events.</li>\n<li>\n<strong>Performance and scale</strong>: you should be aware of how your API and backend are performing and if the key metrics are within the acceptable ranges, way before your users start complaining and your business is impacted. It\u2019s always better to optimize before performance becomes a real\u00a0problem.</li>\n<li>\n<strong>Cost:</strong> similarly, with proper instrumentation, you can be aware of your infrastructure costs sustained to serve a certain amount of traffic. That can help you with capacity planning and cost modeling if you\u2019re monetizing your service. And to avoid unexpected bills at the end of the month that might force you to disrupt the\u00a0service.</li>\n<li>\n<strong>Feedback</strong>: with proper telemetry data to look at when you deploy a change, you can understand how it\u2019s performing and whether it was a good idea to implement it. It also allows you to implement prototyping techniques such as A/B\u00a0testing.</li>\n<li>\n<strong>Learn</strong>: analyzing how developers use your API can be a great source of learning. It will give you useful insights on how your service is being consumed and is a valuable source of ideas that you can evaluate for new features (i.e. new use-case driven API endpoints).</li>\n</ul>\n<p>Proper instrumentation of services is a vast topic and here I just want to summarize a few items that are usually easy to implement:</p>\n<ul>\n<li>Every API request should have a unique operation identifier that is stored in your logs and can help your ops team figure out what happened. This identifier should also be reported back to clients somewhere (usually in API response the headers) especially, but not exclusively, in case of\u00a0errors.</li>\n<li>Keep an eye on API requests that fail with server errors (i.e. the HTTP 5xx ones) and, if the number is non-negligible, try to pinpoint the root cause: is it a bug, or some request that is causing timeouts in the backend? Can you fix it or make it\u00a0faster?</li>\n<li>Keep a reasonable log history to allow tracking errors and auditing user activity for at least several days back in\u00a0time.</li>\n<li>Create dashboards that help you monitor user activity, API usage, security metrics, etc. And make sure to <strong>check them\u00a0often</strong>.</li>\n</ul>\n<h3>6. Make Developer Experience a\u00a0priority</h3>\n<p>Last principle, but definitely not the least important. Even if you are lucky and the developers that use your APIs are required to do so because of an external mandate, you shouldn\u2019t make their experience less positive.</p>\n<blockquote>In fact, the Developer Experience of your product should be\u00a0awesome.</blockquote>\n<p>If developers properly understand how to work with your APIs, and enjoy doing so, they will likely run into fewer issues and be more patient when trying to overcome them. They will also provide you with good quality feedback. It will reduce the number of requests they generate on your support teams. They will give you insights and use case ideas for building a better\u00a0product.</p>\n<blockquote>And, more importantly, happy developers will ultimately build better products for their own customers which, in turn, will act as a force multiplier for the value and success of your own product. Everybody wins in the API Economy model: customers, business stakeholders, partners, product teams, engineers, support\u00a0teams.</blockquote>\n<p>I\u2019ve witnessed several situations where developers were so exhausted and frustrated with working against a bad API that, as soon as they saw the light at the end of the tunnel, they stopped thinking creatively and just powered their way through to an MVP that was far from viable and valuable. But it checked the box they needed so they were allowed to move on. I consider this a very specific scenario of <em>developer fatigue</em>: let\u2019s call it \u201c<strong>API Fatigue</strong>\u201d.</p>\n<p>Luckily, I\u2019ve also experienced the other way around, where new, unplanned, great features were added because the DX was good and we had fun integrating things together.</p>\n<p>There are many resources out there that describe how to make APIs with great developer experience: the most obvious one is to create APIs that are clear and simple to consume. Apply the <a href=\"https://en.wikipedia.org/wiki/Principle_of_least_astonishment\">Principle of least astonishment</a>.</p>\n<p>I recommend considering the following when shipping an\u00a0API:</p>\n<ul>\n<li>\n<strong>Document your API properly</strong>: it\u2019s almost certain that the time you spend creating proper documentation at the beginning is saved later on when developers start consuming it. Specification files, such as <a href=\"https://swagger.io/specification/\">OpenAPI Specification</a> (OAS) tremendously help here. Also, if you follow the design-first approach that we discussed in the first post of this series, you\u2019ll probably already have a specification file ready to\u00a0use.</li>\n<li>\n<strong>Support different learning paths</strong>: some developers like to read all the documentation from top to bottom before writing a single line of code, others will start with the code editor right away. Instead of forcing a learning path, try to embrace the different mindsets and provide tools for everyone: specification files, <a href=\"https://www.postman.com/collection/\">Postman collections</a>, examples in different programming languages, an easy-to-access API sandbox (i.e. something that doesn\u2019t require installing an Enterprise product and waiting 2 weeks to get a license to make your first API call), a developer portal, tutorials and, why not, video walkthroughs. This might sound overwhelming but pays off in the end. With the proper design done first a lot of this content can be autogenerated from the specification files.</li>\n<li>\n<strong>Document the data structure</strong>: if you can, don\u2019t just add lists of properties to your specs and docs: strive to provide proper descriptions of the fields that your API uses so that developers that are not fully familiar with your product can understand them. Remove ambiguity from the documentation as much as possible. This can go a long way, as developers can make mental models by understanding the data properly that often leads to better use cases than \u201c<em>just pull some data from this\u00a0API\u201d.</em>\n</li>\n<li>\n<strong>Use verbs, status codes, and error descriptions properly</strong>: leverage the power of the protocol you are using (i.e. HTTP when using REST APIs) to define how to do things and what responses mean. Proper usage of status codes and good error messages will dramatically reduce the number of requests to your support team. Developers are smart and want to solve problems quickly so if you provide them with the right information to do so, they won\u2019t bother you. Also, if you are properly logging and monitoring your API behavior, it will be easier for your support team to troubleshoot if your errors are not all \u201c500 Internal Server Error\u201d without any other\u00a0detail.</li>\n</ul>\n<p>Finally,<strong> stay close to the developers</strong>: especially if your API is being used by people external from your organization, it\u2019s incredibly important to be close to them as much as you can to support them, learn and gather feedback on your API and its documentation. Allow everyone that is responsible for designing and engineering your API to be in that feedback loop, so they can share the learnings. Consider creating a community where people can ask questions and can expect quick answers (Slack, Reddit, Stack Overflow, etc.). I\u2019ve made great friendships this\u00a0way!</p>\n<h3>A few\u00a0examples</h3>\n<p>There are many great APIs out there. Here is a small, not complete, list of products from other companies that, for one reason or another, are strong examples of what I described in this blog\u00a0series:</p>\n<ul>\n<li><a href=\"https://developer.microsoft.com/en-us/graph\">Microsoft Graph</a></li>\n<li><a href=\"https://cloud.google.com/apis\">Google Cloud</a></li>\n<li><a href=\"https://developers.virustotal.com/\">VirusTotal</a></li>\n<li>GitHub <a href=\"https://docs.github.com/en/rest\">REST</a> and <a href=\"https://docs.github.com/en/graphql\">GraphQL</a>\u00a0APIs</li>\n</ul>\n<h3>Conclusion</h3>\n<p>And that\u2019s a wrap, thanks for reading! There are more things that I\u2019ve learned that I might share in future posts, but in my opinion, these are the most relevant ones. I hope you found this series useful: looking forward to hearing your feedback and comments!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b94d9e8532f5\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">Enterprise API design practices: Part 4</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["developer-experience","development","api","design-patterns","enterprise-software"]},{"title":"Enterprise API design practices: Part 3","pubDate":"2021-02-23 09:59:43","link":"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/c56c3091c7d","author":"Francesco Vigo","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*jaVdQE1HOhSJrOK1sTNAaQ.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jaVdQE1HOhSJrOK1sTNAaQ.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@youssef00?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Youssef Abdelwahab</a> on\u00a0<a href=\"https://unsplash.com/s/photos/skyscraper-crane?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Welcome to the third part of my series on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">Part 1</a> covered some background context and the importance of design, while the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">second post</a> was about security and backend protection; this chapter is on optimization and\u00a0scale.</p>\n<h3>3. Optimize interactions</h3>\n<p>When dealing with scenarios that weren\u2019t anticipated in the initial release of an API (for example when integrating with an external product from a new partner), developers often have to rely on data-driven APIs to extract information from the backend and process it externally. While use-case-driven APIs are generally considered more useful, sometimes there might not be one available that suits the requirements of the novel use case that you must implement.</p>\n<p>By considering the following guidelines when building your data-driven APIs, you can make them easier to consume and more efficient for the backend and the network, improving performance and reducing the operational cost (fewer data transfers, faster and cheaper queries on the DBs,\u00a0etc.).</p>\n<p>I\u2019ll use an example with sample data. Consider the following data as a representation of your backend database: an imaginary set of alertsthat your Enterprise product detected over time. Instead of just three, imagine the following JSON output with thousands of\u00a0records:</p>\n<pre>{<br>    \"alerts\" : [<br>        {<br>            \"name\": \"Impossible Travel\",<br>            \"alert_type\": \"Behavior\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Critical\",<br>            \"created\": \"2020-09-27T09:27:33Z\",<br>            \"modified\": \"2020-09-28T14:34:44Z\",<br>            \"alert_id\": \"8493638e-af28-4a83-b1a9-12085fdbf5b3\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Malware Detected\",<br>            \"alert_type\": \"Endpoint\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"High\",<br>            \"created\": \"2020-10-04T11:22:01Z\",<br>            \"modified\": \"2020-10-08T08:45:33Z\",<br>            \"alert_id\": \"b1018a33-e30f-43b9-9d07-c12d42646bbe\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Large Upload\",<br>            \"alert_type\": \"Network\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Low\",<br>            \"created\": \"2020-11-01T07:04:42Z\",<br>            \"modified\": \"2020-12-01T11:13:24Z\",<br>            \"alert_id\": \"c79ed6a8-bba0-4177-a9c5-39e7f95c86f2\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        }<br>    ]<br>}</pre>\n<p>Imagine implementing a simple /v1/alerts REST API endpoint to retrieve the data and you can\u2019t anticipate all the future needs. I recommend considering the following guidelines:</p>\n<ul>\n<li>\n<strong>Filters</strong>: allow your consumers to reduce the result set by offering filtering capabilities in your API on as many fields as possible without stressing the backend too much (if some filters are not indexed it could become expensive, so you must find the right compromise). In the example above, good filters might include: <em>name</em>, <em>severity,</em> <em>alert_type, alert_status, created,</em> and <em>modified. </em>More complicated fields like <em>details</em> and <em>stack</em> might be too expensive for the backend (as they might require full-text search) and you would probably leave them out unless really required.</li>\n<li>\n<strong>Data formats: </strong>be very consistent in how you present and accept data across your API endpoints. This holds true especially for types such as numbers and dates, or complex structures. For example, to represent integers in JSON you can use numbers (i.e. \"fieldname\": 3) or strings (i.e. \"fieldname\": \"3\" ): no matter what you choose, you need to be consistent across all your API endpoints. And you should also use the same format when returning outputs and accepting inputs.</li>\n<li>\n<strong>Dates: </strong>dates and times can be represented in many ways: timestamps (in seconds, milliseconds, microseconds), strings (ISO8601 such as in the example above, or custom formats such as 20200101), with or without time zone information. This can easily become a problem for the developers. Again, the key is consistency: try to accept and return only a single date format (i.e. timestamp in milliseconds or ISO8601) and be explicit about whether you consider time zones or not: usually choosing to do everything in UTC is a good idea because removes reduce ambiguity. <strong>Make sure to document the date formats properly.</strong>\n</li>\n<li>\n<strong>Filter types: </strong>depending on the type of field, you should provide appropriate filters, not just <em>equals</em>. A good example is supporting <em>range filters </em>for dates that, in our example above, allow consumers to retrieve only the alerts created or modified in a specific interval. If some fields are enumerators with a limited number of possible values, it might be useful to support a <em>multi-select</em> filter (i.e. IN): in the example above it should be possible to filter by severity values and include only the High and Critical values using a single API\u00a0call.</li>\n<li>\n<strong>Sorting: </strong>is your API consumer interested only in the older alerts or the newest? Supporting sorting in your data-driven API is extremely important. One field to sort by is generally enough, but sometimes (depending on the data) you might need\u00a0more.</li>\n<li>\n<strong>Result limiting and pagination</strong>: you can\u2019t expect all the entries to be returned at once (and your clients might not be interested or ready to ingest all of them anyway), so you should implement some logic where clients should retrieve a limited number of results and can get more when they need. If you are using pagination, clients should be able to specify the page size within a maximum allowed value. Defaults and maximums should be reasonable and properly documented.</li>\n<li>\n<strong>Field limiting</strong>: consider whether you really need to return all the fields of your results all the time, or if your clients usually just need a few. By letting the client decide what fields (or groups of fields) your API should return, you can reduce the network throughput and backend cost, and performance. You should provide and document some sane default. In the example above, you could decide to return by default all the fields except details and evidence, which can be requested by the client only if they explicitly ask, using an include parameter.</li>\n</ul>\n<p>Let\u2019s put it all together. In the above example, you should be able to retrieve, using a single API call, something like\u00a0this:</p>\n<blockquote>Up to 100 alerts that were created between 2020\u201304\u201301T00:00:00Z (April 1st 2020) and 2020\u201310\u201301T00:00:00Z (October 1st 2020) with severity \u201cmedium\u201d or \u201chigh\u201d, sorted by \u201cmodified\u201d date, newest first, including all the fields but \u201cevidence\u201d.</blockquote>\n<p>There are multiple ways you can implement this; through REST, <a href=\"https://graphql.org/\">GraphQL</a>, or custom query languages: in many cases, you don\u2019t need something too complex as often data sets are fairly simple. The proper way depends on many design considerations that are outside the scope of this post. But having some, or most of these capabilities in your API will make it better and more future proof. By the way, if you\u2019re into GraphQL, I recommend reading <a href=\"https://xuorig.medium.com/the-tension-between-data-use-case-driven-graphql-apis-8f982198653b\">this\u00a0post</a>.</p>\n<h3>4. Plan for\u00a0scale</h3>\n<p>A good API shouldn\u2019t be presumptuous: it shouldn\u2019t expect that clients are not doing anything else than waiting for its response, especially at scale, where performance is\u00a0key.</p>\n<p>If your API requires more than a few milliseconds to produce a response, I recommend considering supporting jobs instead. The logic can be as\u00a0follows:</p>\n<ul>\n<li>Implement an API endpoint to start an operation that is supposed to take some time. If accepted, it would return immediately with a\u00a0<em>jobId</em><strong><em>.</em></strong>\n</li>\n<li>The client stores the jobId and periodically reaches out to a second endpoint that, when provided with the jobId, returns the completion status of the job (i.e. <em>running</em>, <em>completed</em>, <em>failed)</em>.</li>\n<li>Once results are available (or <em>some</em> are), the client can invoke a third endpoint to fetch the\u00a0results.</li>\n</ul>\n<p>Other possible solutions include publisher/subscriber approaches or pushing data with <em>webhooks</em>, also depending on the size of the result set and the speed requirements<em>.</em> There isn\u2019t a one-size-fits-all solution, but I strongly recommend avoiding a polling logic where API clients are kept waiting for the server to reply while it\u2019s running long jobs in the\u00a0backend.</p>\n<p>If your need high performance and throughput when in your APIs, consider <a href=\"https://grpc.io/\">gRPC</a>, as its binary representation of data using protocol buffers has significant speed advantages over\u00a0REST.</p>\n<p>Side note: if you want to learn more about REST, GraphQL, webhooks, or gRPC use cases, I recommend starting from <a href=\"https://nordicapis.com/when-to-use-what-rest-graphql-webhooks-grpc/\">this\u00a0post</a>.</p>\n<p>Finally, other considerations for scale include supporting batch operations on multiple entries at the same time (for example <em>mass updates</em>), but I recommend considering them only when you have a real use case in\u00a0mind.</p>\n<h3>What\u2019s next</h3>\n<p>In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">next and last chapter</a>, I\u2019ll share some more suggestions about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c56c3091c7d\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">Enterprise API design practices: Part 3</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*jaVdQE1HOhSJrOK1sTNAaQ.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@youssef00?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Youssef Abdelwahab</a> on\u00a0<a href=\"https://unsplash.com/s/photos/skyscraper-crane?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Welcome to the third part of my series on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">Part 1</a> covered some background context and the importance of design, while the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">second post</a> was about security and backend protection; this chapter is on optimization and\u00a0scale.</p>\n<h3>3. Optimize interactions</h3>\n<p>When dealing with scenarios that weren\u2019t anticipated in the initial release of an API (for example when integrating with an external product from a new partner), developers often have to rely on data-driven APIs to extract information from the backend and process it externally. While use-case-driven APIs are generally considered more useful, sometimes there might not be one available that suits the requirements of the novel use case that you must implement.</p>\n<p>By considering the following guidelines when building your data-driven APIs, you can make them easier to consume and more efficient for the backend and the network, improving performance and reducing the operational cost (fewer data transfers, faster and cheaper queries on the DBs,\u00a0etc.).</p>\n<p>I\u2019ll use an example with sample data. Consider the following data as a representation of your backend database: an imaginary set of alertsthat your Enterprise product detected over time. Instead of just three, imagine the following JSON output with thousands of\u00a0records:</p>\n<pre>{<br>    \"alerts\" : [<br>        {<br>            \"name\": \"Impossible Travel\",<br>            \"alert_type\": \"Behavior\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Critical\",<br>            \"created\": \"2020-09-27T09:27:33Z\",<br>            \"modified\": \"2020-09-28T14:34:44Z\",<br>            \"alert_id\": \"8493638e-af28-4a83-b1a9-12085fdbf5b3\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Malware Detected\",<br>            \"alert_type\": \"Endpoint\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"High\",<br>            \"created\": \"2020-10-04T11:22:01Z\",<br>            \"modified\": \"2020-10-08T08:45:33Z\",<br>            \"alert_id\": \"b1018a33-e30f-43b9-9d07-c12d42646bbe\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        },<br>        {<br>            \"name\": \"Large Upload\",<br>            \"alert_type\": \"Network\",<br>            \"alert_status\": \"ACTIVE\",<br>            \"severity\": \"Low\",<br>            \"created\": \"2020-11-01T07:04:42Z\",<br>            \"modified\": \"2020-12-01T11:13:24Z\",<br>            \"alert_id\": \"c79ed6a8-bba0-4177-a9c5-39e7f95c86f2\",<br>            \"details\": \"... long blob of data ...\",<br>            \"evidence\": [<br>                \"long list of stuff\"<br>            ]<br>        }<br>    ]<br>}</pre>\n<p>Imagine implementing a simple /v1/alerts REST API endpoint to retrieve the data and you can\u2019t anticipate all the future needs. I recommend considering the following guidelines:</p>\n<ul>\n<li>\n<strong>Filters</strong>: allow your consumers to reduce the result set by offering filtering capabilities in your API on as many fields as possible without stressing the backend too much (if some filters are not indexed it could become expensive, so you must find the right compromise). In the example above, good filters might include: <em>name</em>, <em>severity,</em> <em>alert_type, alert_status, created,</em> and <em>modified. </em>More complicated fields like <em>details</em> and <em>stack</em> might be too expensive for the backend (as they might require full-text search) and you would probably leave them out unless really required.</li>\n<li>\n<strong>Data formats: </strong>be very consistent in how you present and accept data across your API endpoints. This holds true especially for types such as numbers and dates, or complex structures. For example, to represent integers in JSON you can use numbers (i.e. \"fieldname\": 3) or strings (i.e. \"fieldname\": \"3\" ): no matter what you choose, you need to be consistent across all your API endpoints. And you should also use the same format when returning outputs and accepting inputs.</li>\n<li>\n<strong>Dates: </strong>dates and times can be represented in many ways: timestamps (in seconds, milliseconds, microseconds), strings (ISO8601 such as in the example above, or custom formats such as 20200101), with or without time zone information. This can easily become a problem for the developers. Again, the key is consistency: try to accept and return only a single date format (i.e. timestamp in milliseconds or ISO8601) and be explicit about whether you consider time zones or not: usually choosing to do everything in UTC is a good idea because removes reduce ambiguity. <strong>Make sure to document the date formats properly.</strong>\n</li>\n<li>\n<strong>Filter types: </strong>depending on the type of field, you should provide appropriate filters, not just <em>equals</em>. A good example is supporting <em>range filters </em>for dates that, in our example above, allow consumers to retrieve only the alerts created or modified in a specific interval. If some fields are enumerators with a limited number of possible values, it might be useful to support a <em>multi-select</em> filter (i.e. IN): in the example above it should be possible to filter by severity values and include only the High and Critical values using a single API\u00a0call.</li>\n<li>\n<strong>Sorting: </strong>is your API consumer interested only in the older alerts or the newest? Supporting sorting in your data-driven API is extremely important. One field to sort by is generally enough, but sometimes (depending on the data) you might need\u00a0more.</li>\n<li>\n<strong>Result limiting and pagination</strong>: you can\u2019t expect all the entries to be returned at once (and your clients might not be interested or ready to ingest all of them anyway), so you should implement some logic where clients should retrieve a limited number of results and can get more when they need. If you are using pagination, clients should be able to specify the page size within a maximum allowed value. Defaults and maximums should be reasonable and properly documented.</li>\n<li>\n<strong>Field limiting</strong>: consider whether you really need to return all the fields of your results all the time, or if your clients usually just need a few. By letting the client decide what fields (or groups of fields) your API should return, you can reduce the network throughput and backend cost, and performance. You should provide and document some sane default. In the example above, you could decide to return by default all the fields except details and evidence, which can be requested by the client only if they explicitly ask, using an include parameter.</li>\n</ul>\n<p>Let\u2019s put it all together. In the above example, you should be able to retrieve, using a single API call, something like\u00a0this:</p>\n<blockquote>Up to 100 alerts that were created between 2020\u201304\u201301T00:00:00Z (April 1st 2020) and 2020\u201310\u201301T00:00:00Z (October 1st 2020) with severity \u201cmedium\u201d or \u201chigh\u201d, sorted by \u201cmodified\u201d date, newest first, including all the fields but \u201cevidence\u201d.</blockquote>\n<p>There are multiple ways you can implement this; through REST, <a href=\"https://graphql.org/\">GraphQL</a>, or custom query languages: in many cases, you don\u2019t need something too complex as often data sets are fairly simple. The proper way depends on many design considerations that are outside the scope of this post. But having some, or most of these capabilities in your API will make it better and more future proof. By the way, if you\u2019re into GraphQL, I recommend reading <a href=\"https://xuorig.medium.com/the-tension-between-data-use-case-driven-graphql-apis-8f982198653b\">this\u00a0post</a>.</p>\n<h3>4. Plan for\u00a0scale</h3>\n<p>A good API shouldn\u2019t be presumptuous: it shouldn\u2019t expect that clients are not doing anything else than waiting for its response, especially at scale, where performance is\u00a0key.</p>\n<p>If your API requires more than a few milliseconds to produce a response, I recommend considering supporting jobs instead. The logic can be as\u00a0follows:</p>\n<ul>\n<li>Implement an API endpoint to start an operation that is supposed to take some time. If accepted, it would return immediately with a\u00a0<em>jobId</em><strong><em>.</em></strong>\n</li>\n<li>The client stores the jobId and periodically reaches out to a second endpoint that, when provided with the jobId, returns the completion status of the job (i.e. <em>running</em>, <em>completed</em>, <em>failed)</em>.</li>\n<li>Once results are available (or <em>some</em> are), the client can invoke a third endpoint to fetch the\u00a0results.</li>\n</ul>\n<p>Other possible solutions include publisher/subscriber approaches or pushing data with <em>webhooks</em>, also depending on the size of the result set and the speed requirements<em>.</em> There isn\u2019t a one-size-fits-all solution, but I strongly recommend avoiding a polling logic where API clients are kept waiting for the server to reply while it\u2019s running long jobs in the\u00a0backend.</p>\n<p>If your need high performance and throughput when in your APIs, consider <a href=\"https://grpc.io/\">gRPC</a>, as its binary representation of data using protocol buffers has significant speed advantages over\u00a0REST.</p>\n<p>Side note: if you want to learn more about REST, GraphQL, webhooks, or gRPC use cases, I recommend starting from <a href=\"https://nordicapis.com/when-to-use-what-rest-graphql-webhooks-grpc/\">this\u00a0post</a>.</p>\n<p>Finally, other considerations for scale include supporting batch operations on multiple entries at the same time (for example <em>mass updates</em>), but I recommend considering them only when you have a real use case in\u00a0mind.</p>\n<h3>What\u2019s next</h3>\n<p>In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">next and last chapter</a>, I\u2019ll share some more suggestions about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c56c3091c7d\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">Enterprise API design practices: Part 3</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["api-design","enterprise-software","design-patterns","development","api"]},{"title":"Enterprise API design practices: Part 2","pubDate":"2021-02-23 09:53:10","link":"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/b316ca0c9724","author":"Francesco Vigo","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*0DRNtTfd-C3bcw4NnpmIaw.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0DRNtTfd-C3bcw4NnpmIaw.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@dominik_photography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Dominik Vanyi</a> on\u00a0<a href=\"https://unsplash.com/s/photos/mining-truck?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Welcome to the second part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">first post</a>, I covered some background context and the importance of design; this chapter is about security and protecting your\u00a0backend.</p>\n<h3>1. Secure your\u00a0APIs</h3>\n<p>Most of the projects I\u2019ve been involved with revolve around Cybersecurity, but I don\u2019t consider myself biased when I say that security should be a top priority for every Enterprise API. Misconfigurations and attacks are common and you should go long ways to protect your customers and their\u00a0data.</p>\n<p>Besides the common table stakes (i.e. use TLS and some form of authentication), when it comes to API security I recommend considering at least the following:</p>\n<ul>\n<li>\n<strong>API Credentials</strong>: <a href=\"https://oauth.net/\">OAuth 2</a> should lead the way but, especially for on-premise products, API Keys are still commonly used. In any case, you should support multiple sets of API credentials in your product to allow for more granularity and proper auditing and compliance. For example, every individual service (or user) that consumes your API should have a dedicated set of credentials, which will make it easier to track down from the audit logs who/what performed an operation and apply proper rate\u00a0limits.</li>\n<li>\n<strong>API RBAC (Role-Based Access Control)</strong>: another important reason to support multiple sets of API credentials is the ability to assign different <em>permissions</em> to each set. This way your customers won\u2019t give administrative privileges to a set of credentials used, for example, to perform only read-only routine operations. Having RBAC is not enough: you need to guide your customers to use it properly: a good solution is to enforce a flow in your product UI that discourages users from creating API credentials with unnecessary permissions (assigning permissions in an additive way rather than subtractive, for example).</li>\n<li>\n<strong>Credentials shouldn\u2019t last forever</strong>: it\u2019s a good practice to create sets of API credentials that expire at some point or require a manual reauthorization. For how long a credential should stay valid is a good question, as you want to make sure that a business process doesn\u2019t fail because someone forgot to reauthorize something that was set up 6 months ago. There are ways to mitigate this risk in your product UI. I recommend showing a list of the API credential sets (without showing the actual secrets!), when they expire, and how long ago they were used for the last time. This will help customers identify and delete stale credentials and refresh expiring\u00a0ones.</li>\n</ul>\n<h3>2. Protect your\u00a0backend</h3>\n<p>Don\u2019t trust your API users: they are likely to break things when given a chance! Not maliciously, of course, but there are so many things that can go wrong when you grant external users/applications access to the heart of your product through an API. And yes, bad guys and Denial of Service attacks exist\u00a0too.</p>\n<p>Similarly to Product UIs and CLI tools that validate user inputs and behavior, your APIs should do it too. But it goes beyond just making sure that all the required fields are present and correctly typed (if an API doesn\u2019t do it properly, I would file a bug to get it fixed): include <em>cost</em> and <em>performance</em> in your design considerations<em>.</em></p>\n<p>Consider the following examples:</p>\n<ul>\n<li>Example 1: A proprietary API serving data from a Public Cloud database backend: in many cases such backends charge by the amount of data processed in each query. If your API allows consumers to freely search and return unlimited data from the backend, you might end up with an expensive bill.</li>\n<li>Example 2: An on-premise product with an API that allows users to provide files to be scanned for malicious behavior (i.e. a sandbox). A developer makes a mistake and forgets to add a <em>sleep() </em>in a cycle, hammering the API with tens of requests in a very short amount of time. If the API doesn\u2019t protect the product from this situation, it will likely not just slow down the API service itself, but it can probably have significant performance impacts on the server and all the other services running on it, including your product and its\u00a0UI.</li>\n</ul>\n<p>Both examples are not uncommon for APIs designed exclusively for internal consumption (i.e. when if something goes wrong you can send an IM to the developer that is breaking your API and resolve the problem right\u00a0away).</p>\n<p>You should consider guidelines to mitigate these risk, such\u00a0as:</p>\n<ul>\n<li>\n<strong>Shrink the result sets</strong>: try to support granularity when reading data from backends, for example by reducing the number of selected columns and slicing the results by time or other values. Set proper limits with sane defaults that don\u2019t overload the backend. More on this in the next\u00a0post.</li>\n<li>\n<strong>Rate limits</strong>: API rate limits are a practical way to tell your consumers that they need to slow down. Different consumers, identified with their sets of API credentials, should have their own limits, according to their strategic value, service level agreements, or monetization policy. Also, not all requests are equal: for example, some products implement rate limits using tokens. Each consumer has a limited number of tokens that replenish over time (or cost money) and each API call has a token <em>cost</em> that depends on a number of factors (i.e. impact on the backend load or business value of the service being consumed): once you run out of tokens you need to wait or buy\u00a0more.</li>\n<li>\n<strong>Communicate errors properly</strong>: when hitting rate limits, API consumers should receive a proper error code from the server (i.e. 429 if using HTTP) and the error message should specify how long until new API calls can be accepted for this client: this won\u2019t just improve DX, but will also allow the clients/SDKs to properly adapt and throttle their requests. Imagine that your API is used by a mobile app and they hit the rate limit: specifying how long they should wait before you are ready to accept a new request allows the app developer to show a popup in the UI to tell the end-user how long to wait. This will not just improve your API\u2019s DX, but will empower the app developers to provide a better U<em>ser Experience</em> (UX) to their customers!</li>\n</ul>\n<h3>What\u2019s next</h3>\n<p>In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">next chapter</a>, I\u2019ll dive deep into some technical suggestions on data-driven APIs and scale. In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">last part</a> of the series, I\u2019ll share some more suggestions about monitoring your API and supporting the developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b316ca0c9724\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">Enterprise API design practices: Part 2</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0DRNtTfd-C3bcw4NnpmIaw.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@dominik_photography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Dominik Vanyi</a> on\u00a0<a href=\"https://unsplash.com/s/photos/mining-truck?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>Welcome to the second part of my series on creating <strong>valuable</strong>, <strong>usable </strong>and <strong>future-proof </strong>Enterprise APIs. In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">first post</a>, I covered some background context and the importance of design; this chapter is about security and protecting your\u00a0backend.</p>\n<h3>1. Secure your\u00a0APIs</h3>\n<p>Most of the projects I\u2019ve been involved with revolve around Cybersecurity, but I don\u2019t consider myself biased when I say that security should be a top priority for every Enterprise API. Misconfigurations and attacks are common and you should go long ways to protect your customers and their\u00a0data.</p>\n<p>Besides the common table stakes (i.e. use TLS and some form of authentication), when it comes to API security I recommend considering at least the following:</p>\n<ul>\n<li>\n<strong>API Credentials</strong>: <a href=\"https://oauth.net/\">OAuth 2</a> should lead the way but, especially for on-premise products, API Keys are still commonly used. In any case, you should support multiple sets of API credentials in your product to allow for more granularity and proper auditing and compliance. For example, every individual service (or user) that consumes your API should have a dedicated set of credentials, which will make it easier to track down from the audit logs who/what performed an operation and apply proper rate\u00a0limits.</li>\n<li>\n<strong>API RBAC (Role-Based Access Control)</strong>: another important reason to support multiple sets of API credentials is the ability to assign different <em>permissions</em> to each set. This way your customers won\u2019t give administrative privileges to a set of credentials used, for example, to perform only read-only routine operations. Having RBAC is not enough: you need to guide your customers to use it properly: a good solution is to enforce a flow in your product UI that discourages users from creating API credentials with unnecessary permissions (assigning permissions in an additive way rather than subtractive, for example).</li>\n<li>\n<strong>Credentials shouldn\u2019t last forever</strong>: it\u2019s a good practice to create sets of API credentials that expire at some point or require a manual reauthorization. For how long a credential should stay valid is a good question, as you want to make sure that a business process doesn\u2019t fail because someone forgot to reauthorize something that was set up 6 months ago. There are ways to mitigate this risk in your product UI. I recommend showing a list of the API credential sets (without showing the actual secrets!), when they expire, and how long ago they were used for the last time. This will help customers identify and delete stale credentials and refresh expiring\u00a0ones.</li>\n</ul>\n<h3>2. Protect your\u00a0backend</h3>\n<p>Don\u2019t trust your API users: they are likely to break things when given a chance! Not maliciously, of course, but there are so many things that can go wrong when you grant external users/applications access to the heart of your product through an API. And yes, bad guys and Denial of Service attacks exist\u00a0too.</p>\n<p>Similarly to Product UIs and CLI tools that validate user inputs and behavior, your APIs should do it too. But it goes beyond just making sure that all the required fields are present and correctly typed (if an API doesn\u2019t do it properly, I would file a bug to get it fixed): include <em>cost</em> and <em>performance</em> in your design considerations<em>.</em></p>\n<p>Consider the following examples:</p>\n<ul>\n<li>Example 1: A proprietary API serving data from a Public Cloud database backend: in many cases such backends charge by the amount of data processed in each query. If your API allows consumers to freely search and return unlimited data from the backend, you might end up with an expensive bill.</li>\n<li>Example 2: An on-premise product with an API that allows users to provide files to be scanned for malicious behavior (i.e. a sandbox). A developer makes a mistake and forgets to add a <em>sleep() </em>in a cycle, hammering the API with tens of requests in a very short amount of time. If the API doesn\u2019t protect the product from this situation, it will likely not just slow down the API service itself, but it can probably have significant performance impacts on the server and all the other services running on it, including your product and its\u00a0UI.</li>\n</ul>\n<p>Both examples are not uncommon for APIs designed exclusively for internal consumption (i.e. when if something goes wrong you can send an IM to the developer that is breaking your API and resolve the problem right\u00a0away).</p>\n<p>You should consider guidelines to mitigate these risk, such\u00a0as:</p>\n<ul>\n<li>\n<strong>Shrink the result sets</strong>: try to support granularity when reading data from backends, for example by reducing the number of selected columns and slicing the results by time or other values. Set proper limits with sane defaults that don\u2019t overload the backend. More on this in the next\u00a0post.</li>\n<li>\n<strong>Rate limits</strong>: API rate limits are a practical way to tell your consumers that they need to slow down. Different consumers, identified with their sets of API credentials, should have their own limits, according to their strategic value, service level agreements, or monetization policy. Also, not all requests are equal: for example, some products implement rate limits using tokens. Each consumer has a limited number of tokens that replenish over time (or cost money) and each API call has a token <em>cost</em> that depends on a number of factors (i.e. impact on the backend load or business value of the service being consumed): once you run out of tokens you need to wait or buy\u00a0more.</li>\n<li>\n<strong>Communicate errors properly</strong>: when hitting rate limits, API consumers should receive a proper error code from the server (i.e. 429 if using HTTP) and the error message should specify how long until new API calls can be accepted for this client: this won\u2019t just improve DX, but will also allow the clients/SDKs to properly adapt and throttle their requests. Imagine that your API is used by a mobile app and they hit the rate limit: specifying how long they should wait before you are ready to accept a new request allows the app developer to show a popup in the UI to tell the end-user how long to wait. This will not just improve your API\u2019s DX, but will empower the app developers to provide a better U<em>ser Experience</em> (UX) to their customers!</li>\n</ul>\n<h3>What\u2019s next</h3>\n<p>In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">next chapter</a>, I\u2019ll dive deep into some technical suggestions on data-driven APIs and scale. In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">last part</a> of the series, I\u2019ll share some more suggestions about monitoring your API and supporting the developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b316ca0c9724\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">Enterprise API design practices: Part 2</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["api-security","design-patterns","enterprise-software","api","development"]},{"title":"Enterprise API design practices: Part 1","pubDate":"2021-02-23 09:48:22","link":"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/1c5a47717ed0","author":"Francesco Vigo","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*UCzxQTkyzFZIc_ah.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*UCzxQTkyzFZIc_ah.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@amyames?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Amy Elting</a> on\u00a0<a href=\"https://unsplash.com/s/photos/wires?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>In this series, I will share some guidelines on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. In this initial post, I will cover some background context and a very important lesson, before getting more technical in parts <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">two</a>, <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">three</a>, and\u00a0<a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">four</a>.</p>\n<p>If you\u2019re wondering why this matters, I recommend reading about the <a href=\"https://www.forbes.com/sites/tomtaulli/2020/01/18/api-economy--is-it-the-next-big-thing/\">API\u00a0Economy</a>.</p>\n<p>In the last few years, thanks to my role as a Principal Solutions Architect at Palo Alto Networks, I\u2019ve <em>dealt with</em> a lot of APIs. Most of these APIs were offered by Enterprise companies, predominately in Cybersecurity. By <em>dealt with </em>I mean that I\u2019ve either written code to integrate with the APIs myself, or I\u2019ve helped other developers do it. During some of these integrations, I had the privilege of discussing issues and potential improvements with the Engineers and Product Managers responsible for product APIs. Meeting with these stakeholders and providing feedback often lead to API improvements and a stronger partnership.</p>\n<p>In this blog post, I will share some of the things I\u2019ve learned about good API design that can lead to better business outcomes and fewer headaches for API builders and consumers, including better Developer Experience (DX) and more efficient operations.</p>\n<blockquote>\n<em>There are tons of great resources out there about API best practices (such as the</em><a href=\"https://cloud.google.com/apis/design\"><em> Google Cloud API Design Guide</em></a><em>) and this post is not meant to be a comprehensive list, but rather present a set of common patterns that I ran into. Places where I usually find very interesting content are: </em><a href=\"https://apisyouwonthate.com/\"><em>APIs you won\u2019t hate</em></a><em> and </em><a href=\"https://nordicapis.com/\"><em>Nordic\u00a0APIs</em></a><em>.</em>\n</blockquote>\n<h3>Background context</h3>\n<p>Before sharing the lessons I\u2019ve learned, let me explain the context where most of my experience is based\u00a0upon:</p>\n<ul>\n<li>\n<strong>B2B</strong>: Enterprise on-premise or SaaS products integrating with other Enterprise products (by partners) or custom applications/scripts (by customers).</li>\n<li>\n<strong>Top-down</strong>: some business mandate requires integrating with a specific product (i.e. because a joint customer was requesting an integration between two vendors), without the developers being able to choose an alternative that provided a better Developer Experience (DX). Sometimes we really had to bite the bullet and power through sub-optimal DX.</li>\n</ul>\n<p>And in most cases we had the following goals and constraints:</p>\n<ul>\n<li>\n<strong>Speed</strong>: build something quickly and move on to the next project. That\u2019s how many Enterprise companies work when it comes to technology partnerships. While this doesn\u2019t sound ideal, there are several good business reasons: sales objection handling, customer acquisition, marketing, etc. You know that only a few ideas can be successful: similarly, many product integrations don\u2019t move the needle, so it\u2019s good to test many hypotheses quickly and invest properly only in the few that are worth\u00a0it.</li>\n<li>\n<strong>Build with what we have</strong>: because of the above requirement, we rarely had the opportunity to request API or Product changes before we had to ship something.</li>\n<li>\n<strong>Create value</strong>: doing things quickly doesn\u2019t mean you need to check a box that provides no value to customers. Even when the integration use cases were limited in scope, we always had an outcome in mind that could benefit some end-user.</li>\n<li>\n<strong>Performance</strong>: don\u2019t create things that create performance issues on the APIs or, even worse, on the backing products. Or that could end up in significantly higher public cloud spending because they trigger expensive, non-optimized backend queries. APIs should protect themselves and the backing products from misuse but, in my experience, it\u2019s not always the case, especially with on-premise products.</li>\n</ul>\n<p>While I believe that a different context could lead to different conclusions, I think that the <em>guidelines</em> that follow are valid for most scenarios. Let\u2019s begin with the most disruptive one.</p>\n<h3>0. Design first (if you can\u2026 but you really\u00a0should)</h3>\n<p>Although it might sound like preaching to the choir, my first recommendation is to adopt the <a href=\"https://apisyouwonthate.com/blog/api-design-first-vs-code-first\">design-first approach</a> when creating APIs. This also (especially!) holds true for Enterprise products even though it may sound hard because APIs are too often not perceived as first-class components of a product (or as products by themselves). I\u2019ve seen a lot of different scenarios, including:</p>\n<ul>\n<li>APIs created to decouple the client logic from the server backend, without thinking about any other consumer than the product UI itself (a good example of a great idea made much less effective).</li>\n<li>APIs that basically just exposed the underlying database with some authentication in front of\u00a0it.</li>\n<li>APIs created only to satisfy a very narrow/specific feature request coming from a customer and evolved in the same, very tactical, way, feature by\u00a0feature.</li>\n<li>APIs meant to be internal-only that were later exposed to external consumers because of a business request. By the way, this breaks the <a href=\"https://nordicapis.com/the-bezos-api-mandate-amazons-manifesto-for-externalization/\">so-called Bezos API Mandate</a> and is usually a primary cause of technical debt.</li>\n</ul>\n<p>The examples above often performed poorly when we tried to consume those APIs in real-world integration use cases, leading to poor DX and reliance on client-side logic to work around the problems, which many times translated into lower quality results and developer fatigue (more on this in part\u00a04).</p>\n<p>But such approaches aren\u2019t necessarily wrong: arguably you can\u2019t really know all the potential use cases of an API when you ship its first iteration and you gotta start somewhere but, <em>if you understand that you don\u2019t know what you don\u2019t know</em>, you can at least mitigate the risks. And the easiest and cheapest way to mitigate risks is to do it earlier on, hence I strongly recommend to embrace a <strong>design-first approach</strong> when creating\u00a0APIs.</p>\n<p>However, if you really want or are forced to follow a <em>code-first approach</em>, you can still apply most of the guidelines presented in this series as you develop your\u00a0APIs.</p>\n<h3>What\u2019s next</h3>\n<p>In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">next post</a>, I\u2019ll cover some security and backend considerations. In <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">part three</a> I\u2019ll dive into some more technical considerations about data-driven APIs and scale. Finally, in the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">last chapter</a>, I\u2019ll present some more guidelines about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1c5a47717ed0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">Enterprise API design practices: Part 1</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*UCzxQTkyzFZIc_ah.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@amyames?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Amy Elting</a> on\u00a0<a href=\"https://unsplash.com/s/photos/wires?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>In this series, I will share some guidelines on creating <strong>valuable</strong>, <strong>usable, </strong>and <strong>future-proof </strong>Enterprise APIs. In this initial post, I will cover some background context and a very important lesson, before getting more technical in parts <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">two</a>, <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">three</a>, and\u00a0<a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">four</a>.</p>\n<p>If you\u2019re wondering why this matters, I recommend reading about the <a href=\"https://www.forbes.com/sites/tomtaulli/2020/01/18/api-economy--is-it-the-next-big-thing/\">API\u00a0Economy</a>.</p>\n<p>In the last few years, thanks to my role as a Principal Solutions Architect at Palo Alto Networks, I\u2019ve <em>dealt with</em> a lot of APIs. Most of these APIs were offered by Enterprise companies, predominately in Cybersecurity. By <em>dealt with </em>I mean that I\u2019ve either written code to integrate with the APIs myself, or I\u2019ve helped other developers do it. During some of these integrations, I had the privilege of discussing issues and potential improvements with the Engineers and Product Managers responsible for product APIs. Meeting with these stakeholders and providing feedback often lead to API improvements and a stronger partnership.</p>\n<p>In this blog post, I will share some of the things I\u2019ve learned about good API design that can lead to better business outcomes and fewer headaches for API builders and consumers, including better Developer Experience (DX) and more efficient operations.</p>\n<blockquote>\n<em>There are tons of great resources out there about API best practices (such as the</em><a href=\"https://cloud.google.com/apis/design\"><em> Google Cloud API Design Guide</em></a><em>) and this post is not meant to be a comprehensive list, but rather present a set of common patterns that I ran into. Places where I usually find very interesting content are: </em><a href=\"https://apisyouwonthate.com/\"><em>APIs you won\u2019t hate</em></a><em> and </em><a href=\"https://nordicapis.com/\"><em>Nordic\u00a0APIs</em></a><em>.</em>\n</blockquote>\n<h3>Background context</h3>\n<p>Before sharing the lessons I\u2019ve learned, let me explain the context where most of my experience is based\u00a0upon:</p>\n<ul>\n<li>\n<strong>B2B</strong>: Enterprise on-premise or SaaS products integrating with other Enterprise products (by partners) or custom applications/scripts (by customers).</li>\n<li>\n<strong>Top-down</strong>: some business mandate requires integrating with a specific product (i.e. because a joint customer was requesting an integration between two vendors), without the developers being able to choose an alternative that provided a better Developer Experience (DX). Sometimes we really had to bite the bullet and power through sub-optimal DX.</li>\n</ul>\n<p>And in most cases we had the following goals and constraints:</p>\n<ul>\n<li>\n<strong>Speed</strong>: build something quickly and move on to the next project. That\u2019s how many Enterprise companies work when it comes to technology partnerships. While this doesn\u2019t sound ideal, there are several good business reasons: sales objection handling, customer acquisition, marketing, etc. You know that only a few ideas can be successful: similarly, many product integrations don\u2019t move the needle, so it\u2019s good to test many hypotheses quickly and invest properly only in the few that are worth\u00a0it.</li>\n<li>\n<strong>Build with what we have</strong>: because of the above requirement, we rarely had the opportunity to request API or Product changes before we had to ship something.</li>\n<li>\n<strong>Create value</strong>: doing things quickly doesn\u2019t mean you need to check a box that provides no value to customers. Even when the integration use cases were limited in scope, we always had an outcome in mind that could benefit some end-user.</li>\n<li>\n<strong>Performance</strong>: don\u2019t create things that create performance issues on the APIs or, even worse, on the backing products. Or that could end up in significantly higher public cloud spending because they trigger expensive, non-optimized backend queries. APIs should protect themselves and the backing products from misuse but, in my experience, it\u2019s not always the case, especially with on-premise products.</li>\n</ul>\n<p>While I believe that a different context could lead to different conclusions, I think that the <em>guidelines</em> that follow are valid for most scenarios. Let\u2019s begin with the most disruptive one.</p>\n<h3>0. Design first (if you can\u2026 but you really\u00a0should)</h3>\n<p>Although it might sound like preaching to the choir, my first recommendation is to adopt the <a href=\"https://apisyouwonthate.com/blog/api-design-first-vs-code-first\">design-first approach</a> when creating APIs. This also (especially!) holds true for Enterprise products even though it may sound hard because APIs are too often not perceived as first-class components of a product (or as products by themselves). I\u2019ve seen a lot of different scenarios, including:</p>\n<ul>\n<li>APIs created to decouple the client logic from the server backend, without thinking about any other consumer than the product UI itself (a good example of a great idea made much less effective).</li>\n<li>APIs that basically just exposed the underlying database with some authentication in front of\u00a0it.</li>\n<li>APIs created only to satisfy a very narrow/specific feature request coming from a customer and evolved in the same, very tactical, way, feature by\u00a0feature.</li>\n<li>APIs meant to be internal-only that were later exposed to external consumers because of a business request. By the way, this breaks the <a href=\"https://nordicapis.com/the-bezos-api-mandate-amazons-manifesto-for-externalization/\">so-called Bezos API Mandate</a> and is usually a primary cause of technical debt.</li>\n</ul>\n<p>The examples above often performed poorly when we tried to consume those APIs in real-world integration use cases, leading to poor DX and reliance on client-side logic to work around the problems, which many times translated into lower quality results and developer fatigue (more on this in part\u00a04).</p>\n<p>But such approaches aren\u2019t necessarily wrong: arguably you can\u2019t really know all the potential use cases of an API when you ship its first iteration and you gotta start somewhere but, <em>if you understand that you don\u2019t know what you don\u2019t know</em>, you can at least mitigate the risks. And the easiest and cheapest way to mitigate risks is to do it earlier on, hence I strongly recommend to embrace a <strong>design-first approach</strong> when creating\u00a0APIs.</p>\n<p>However, if you really want or are forced to follow a <em>code-first approach</em>, you can still apply most of the guidelines presented in this series as you develop your\u00a0APIs.</p>\n<h3>What\u2019s next</h3>\n<p>In the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-2-b316ca0c9724\">next post</a>, I\u2019ll cover some security and backend considerations. In <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-3-c56c3091c7d\">part three</a> I\u2019ll dive into some more technical considerations about data-driven APIs and scale. Finally, in the <a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-4-b94d9e8532f5\">last chapter</a>, I\u2019ll present some more guidelines about monitoring and supporting your API and developer ecosystem.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1c5a47717ed0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/enterprise-api-design-practices-part-1-1c5a47717ed0\">Enterprise API design practices: Part 1</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["api-economy","api","enterprise-software","development","developer-experience"]},{"title":"Fingerprinting SSL Servers using JARM and Python","pubDate":"2021-01-29 17:26:48","link":"https://medium.com/palo-alto-networks-developer-blog/fingerprinting-ssl-servers-using-jarm-and-python-6d03f6d38dec?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/6d03f6d38dec","author":"Andrew Scott","thumbnail":"https://cdn-images-1.medium.com/max/668/1*INRqNCAsDsVqMadUX1n80A.png","description":"\n<h4>Introducing <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a>, the simple way to use JARM with\u00a0Python</h4>\n<figure><img alt=\"pyJARM logo by Charlie Sestito\" src=\"https://cdn-images-1.medium.com/max/668/1*INRqNCAsDsVqMadUX1n80A.png\"><figcaption>pyJARM logo by <a href=\"https://github.com/csestito\">Charlie\u00a0Sestito</a></figcaption></figure><p>In early November 2020, <a href=\"https://engineering.salesforce.com/easily-identify-malicious-servers-on-the-internet-with-jarm-e095edac525a\">SalesForce announced a new active TLS fingerprinting tool</a>, inexplicably named JARM. The premise of JARM is that if a target server is scanned using a variety of different custom-crafted TCP \u201cClient hello\u201d packets, the resulting responses can be combined and hashed to provide a fingerprint that is indicative of a shared or similar application or server\u00a0stack.</p>\n<p>Building on this basic premise, it\u2019s asserted that different companies, platforms, or individual products will present similar fingerprints and therefore can allow researchers and security teams to identify possible C2 servers or group hosts with similar SSL configurations.</p>\n<h4>JARM Use\u00a0Cases</h4>\n<p>I believe there are many interesting use-cases for JARM, and once proven there are opportunities to add JARM to a number of existing products and research initiatives. A short list of these use-cases include:</p>\n<ol>\n<li>Identifying malicious servers or <a href=\"https://github.com/cedowens/C2-JARM\">malware C2 servers</a> on the\u00a0internet</li>\n<li>Identifying hosts owned by an organization that may not be using an approved or default configuration</li>\n<li>Grouping hosts that have similar configurations</li>\n<li>Identifying hosts that are/are not behind a firewall or\u00a0WAF</li>\n</ol>\n<h4>How JARM\u00a0Works</h4>\n<p>JARM works by crafting 10 custom TCP \u201cClient hello\u201d packets which it sends to the target server and records the responses. Each response is then parsed into a three piece record, with each piece being separated by a pipe. These raw results look like c02b|0303|h2|0017-0010. All of these raw scan results are combined into a single comma-separated string and\u00a0hashed.</p>\n<p>The hash itself is a custom fuzzy hash that produces a 62 character fingerprint. The first 30 characters are made up of the cipher and TLS version chosen by the server for each of the 10 client hello\u2019s sent (i.e. 3 characters per scan). The remaining 32 characters are a truncated SHA256 hash of the \u201cServer hello\u201d extensions returned by the server. A JARM fingerprint that is all 0\u2019s indicates that the server did not\u00a0respond.</p>\n<h4>Introducing pyJARM</h4>\n<p>When I saw the <a href=\"https://github.com/salesforce/jarm\">initial implementation of JARM</a> in Python that was delivered by the Salesforce team I was happy that they\u2019d made it available and open sourced it. However, with my developer hat on, when I looked at the code I couldn\u2019t help but think that it could be more usable. I worked with <a href=\"https://github.com/fvigo\">Francesco Vigo</a>, a colleague at Palo Alto Networks, to rewrite the original implementation into a more convenient library, known as\u00a0<a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a>.</p>\n<p>We also proceeded to add some additional features to <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> that we thought would be convenient for developers and researchers, such as proxy and async support. We also made <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> available to install via pip and open-sourced it under the ISC\u00a0license.</p>\n<h4>Using pyJARM</h4>\n<p>Installation of <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> is simple and can be accomplished with a simple pip install pyjarm using any machine with access to <a href=\"https://pypi.org/project/pyjarm/\">pypi</a> and Python 3.7+ installed.</p>\n<p><a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> can be run either as a command line tool or utilized programatically. When running as a command line tool it can accept a single target, or a file with a list of inputs. It can output to stdout or write directly to a csv\u00a0file.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LHdIwR_9SEa5tVqNCIYeAQ.png\"></figure><p>You can also use <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> programmatically by importing the Scanner class and calling Scanner.scan with an IP/Domain and port\u00a0number.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XaT22qtUxbr2SlZembGaFQ.png\"></figure><h4>What\u2019s Next?</h4>\n<p>We\u2019re eager to see wider adoption of <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> and JARM generally in the infosec community and within vendor tools and solutions. If you have any questions, issues, or enhancements for pyJARM, we\u2019d encourage you to open an Issue or PR on\u00a0<a href=\"https://github.com/PaloAltoNetworks/pyjarm\">Github</a>.</p>\n<p><a href=\"https://github.com/PaloAltoNetworks/pyjarm\">PaloAltoNetworks/pyjarm</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d03f6d38dec\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/fingerprinting-ssl-servers-using-jarm-and-python-6d03f6d38dec\">Fingerprinting SSL Servers using JARM and Python</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Introducing <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a>, the simple way to use JARM with\u00a0Python</h4>\n<figure><img alt=\"pyJARM logo by Charlie Sestito\" src=\"https://cdn-images-1.medium.com/max/668/1*INRqNCAsDsVqMadUX1n80A.png\"><figcaption>pyJARM logo by <a href=\"https://github.com/csestito\">Charlie\u00a0Sestito</a></figcaption></figure><p>In early November 2020, <a href=\"https://engineering.salesforce.com/easily-identify-malicious-servers-on-the-internet-with-jarm-e095edac525a\">SalesForce announced a new active TLS fingerprinting tool</a>, inexplicably named JARM. The premise of JARM is that if a target server is scanned using a variety of different custom-crafted TCP \u201cClient hello\u201d packets, the resulting responses can be combined and hashed to provide a fingerprint that is indicative of a shared or similar application or server\u00a0stack.</p>\n<p>Building on this basic premise, it\u2019s asserted that different companies, platforms, or individual products will present similar fingerprints and therefore can allow researchers and security teams to identify possible C2 servers or group hosts with similar SSL configurations.</p>\n<h4>JARM Use\u00a0Cases</h4>\n<p>I believe there are many interesting use-cases for JARM, and once proven there are opportunities to add JARM to a number of existing products and research initiatives. A short list of these use-cases include:</p>\n<ol>\n<li>Identifying malicious servers or <a href=\"https://github.com/cedowens/C2-JARM\">malware C2 servers</a> on the\u00a0internet</li>\n<li>Identifying hosts owned by an organization that may not be using an approved or default configuration</li>\n<li>Grouping hosts that have similar configurations</li>\n<li>Identifying hosts that are/are not behind a firewall or\u00a0WAF</li>\n</ol>\n<h4>How JARM\u00a0Works</h4>\n<p>JARM works by crafting 10 custom TCP \u201cClient hello\u201d packets which it sends to the target server and records the responses. Each response is then parsed into a three piece record, with each piece being separated by a pipe. These raw results look like c02b|0303|h2|0017-0010. All of these raw scan results are combined into a single comma-separated string and\u00a0hashed.</p>\n<p>The hash itself is a custom fuzzy hash that produces a 62 character fingerprint. The first 30 characters are made up of the cipher and TLS version chosen by the server for each of the 10 client hello\u2019s sent (i.e. 3 characters per scan). The remaining 32 characters are a truncated SHA256 hash of the \u201cServer hello\u201d extensions returned by the server. A JARM fingerprint that is all 0\u2019s indicates that the server did not\u00a0respond.</p>\n<h4>Introducing pyJARM</h4>\n<p>When I saw the <a href=\"https://github.com/salesforce/jarm\">initial implementation of JARM</a> in Python that was delivered by the Salesforce team I was happy that they\u2019d made it available and open sourced it. However, with my developer hat on, when I looked at the code I couldn\u2019t help but think that it could be more usable. I worked with <a href=\"https://github.com/fvigo\">Francesco Vigo</a>, a colleague at Palo Alto Networks, to rewrite the original implementation into a more convenient library, known as\u00a0<a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a>.</p>\n<p>We also proceeded to add some additional features to <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> that we thought would be convenient for developers and researchers, such as proxy and async support. We also made <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> available to install via pip and open-sourced it under the ISC\u00a0license.</p>\n<h4>Using pyJARM</h4>\n<p>Installation of <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> is simple and can be accomplished with a simple pip install pyjarm using any machine with access to <a href=\"https://pypi.org/project/pyjarm/\">pypi</a> and Python 3.7+ installed.</p>\n<p><a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> can be run either as a command line tool or utilized programatically. When running as a command line tool it can accept a single target, or a file with a list of inputs. It can output to stdout or write directly to a csv\u00a0file.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LHdIwR_9SEa5tVqNCIYeAQ.png\"></figure><p>You can also use <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> programmatically by importing the Scanner class and calling Scanner.scan with an IP/Domain and port\u00a0number.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XaT22qtUxbr2SlZembGaFQ.png\"></figure><h4>What\u2019s Next?</h4>\n<p>We\u2019re eager to see wider adoption of <a href=\"https://github.com/PaloAltoNetworks/pyjarm\">pyJARM</a> and JARM generally in the infosec community and within vendor tools and solutions. If you have any questions, issues, or enhancements for pyJARM, we\u2019d encourage you to open an Issue or PR on\u00a0<a href=\"https://github.com/PaloAltoNetworks/pyjarm\">Github</a>.</p>\n<p><a href=\"https://github.com/PaloAltoNetworks/pyjarm\">PaloAltoNetworks/pyjarm</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d03f6d38dec\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/fingerprinting-ssl-servers-using-jarm-and-python-6d03f6d38dec\">Fingerprinting SSL Servers using JARM and Python</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["jarm","python","programming","malware","cybersecurity"]},{"title":"Continuous Scraping of Cloud Services Endpoints","pubDate":"2021-01-25 18:21:54","link":"https://medium.com/palo-alto-networks-developer-blog/continuous-scraping-of-cloud-services-endpoints-978920fd27ca?source=rss----7f77455ad9a7---4","guid":"https://medium.com/p/978920fd27ca","author":"Luigi Mori","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*8MiuCQ4qNzvBFPcISAwLKw.png","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8MiuCQ4qNzvBFPcISAwLKw.png\"><figcaption>Continuous Scraping</figcaption></figure><p><strong><em>TL;DR</em></strong><em>: The addition of GitHub Actions to the already feature rich GitHub platform created an interesting and powerful framework for tracking URLs/IP Ranges used by Cloud Services. Live Example: </em><a href=\"https://github.com/jtschichold/mm-cloud-services-endpoints-test\"><em>jtschichold/mm-cloud-services-endpoints-test</em></a></p>\n<p>Not too long after GitHub announced their Continuous Integration framework feature, GitHub Actions, some smart people had the idea of using GitHub Actions scheduling mechanism to periodically scrape web sites and track changes using\u00a0git.</p>\n<p>If you think for a moment with GitHub Actions you\u00a0have:</p>\n<ul>\n<li>a scheduling engine to periodically run (almost) arbitrary code (GitHub\u00a0Actions)</li>\n<li>a storage mechanism (git\u00a0repo)</li>\n<li>a super powerful change tracking mechanism (git\u00a0itself)</li>\n<li>a CDN to publish the results of your activities (GitHub)</li>\n<li>a mechanism to interact with users (GitHub Pull Requests)</li>\n<li>a mechanism to notify users (GitHub repo notifications)</li>\n</ul>\n<p><strong>This looks like a perfect fit for tracking low volume, slow changing feed\u200a\u2014\u200alike list of URLs/IPs used by Cloud Services</strong>. You can already implement trackers in hundreds of ways, but in most architectures some of the requirements listed above require ad-hoc coding, are painful to implement (looking at you, \u201c<em>user interaction</em>\u201d) or expensive (<em>CDN</em>). <strong>With GitHub Actions instead you have all the requirements offered as services in a nice package, ready to\u00a0use.</strong></p>\n<p>The following lines guide you thru the process of setting up a Cloud Services tracker, using some simple code we implemented to smooth some of the tasks. Thanks to the GitHub Actions <a href=\"https://docs.github.com/en/github/setting-up-and-managing-billing-and-payments-on-github/about-billing-for-github-actions\">Pricing structure</a> you can test this idea for free (as long as you keep your repo\u00a0public).</p>\n<h3>Quick introduction to GitHub\u00a0Actions</h3>\n<p><a href=\"https://github.com/features/actions\">GitHub Actions</a> is the Continuous Integration framework offered by GitHub and tightly integrated with their GitHub platform. GitHub Actions follows the same patterns used by other Continuous Integration services\u200a\u2014\u200ato configure GitHub\u00a0Actions:</p>\n<ul>\n<li>create a workflow definition file inside the directory\u00a0.github/workflows of your GitHub repo, using\u00a0YAML</li>\n<li>specify in the workflow file one or more list of <em>steps</em> to be executed and when they should be executed (the <em>triggers</em>)</li>\n<li>commit &amp;\u00a0push</li>\n</ul>\n<p>Let\u2019s look at a real world example to give you a better idea on how GitHub Actions workflows work and how to use them for scraping. The following is a commented excerpt taken from the source of my inspiration\u200a\u2014\u200athe repo <a href=\"https://github.com/alex/nyt-2020-election-scraper/blob/master/.github/workflows/scrape.yml\">alex/nyt-2020-election-scraper</a>. This workflow has a single <em>job</em> called scheduled that downloads the data of the US2020 Presidential Election and updates local tables, pushing the results back into the\u00a0repo:</p>\n<a href=\"https://medium.com/media/3183297900eed9d277f81b1bd898ec4d/href\">https://medium.com/media/3183297900eed9d277f81b1bd898ec4d/href</a><a href=\"https://medium.com/media/338ae661555bf56b31eeed4215ff8ae6/href\">https://medium.com/media/338ae661555bf56b31eeed4215ff8ae6/href</a><p>As you can see the scraping workflow is extremely easy to understand and flexible. For tracking Cloud Services we need some additional steps for processing and double checking the scraping results, but not too\u00a0many.</p>\n<p>You can find the full documentation with a more in-depth Quickstart on the official GitHub Action webpage: <a href=\"https://docs.github.com/en/actions\">https://docs.github.com/en/actions</a></p>\n<h3>Tracking Cloud\u00a0Services</h3>\n<p>To track Cloud Services endpoints we want a GitHub Action workflow looking like\u00a0this:</p>\n<ol>\n<li>\n<strong>Checkout</strong> the\u00a0repo</li>\n<li>\n<strong>Fetch</strong> the latest version of list of endpoints used by a Cloud Service from their feed/API/webpage</li>\n<li>\n<strong>Filter</strong> the list (if we want to exclude some IP ranges or\u00a0URLs)</li>\n<li>\n<strong>Transform</strong> the list in a format we can\u00a0consume</li>\n<li>\n<strong>Check</strong> the change in the latest version of the list against the one stored in the\u00a0repo</li>\n<li>\n<strong>Update</strong>. If the change is not <em>big</em>, commit the changes &amp; push. If the change is <em>big</em>, open a PR and ask for manual approval.</li>\n</ol>\n<p><strong>Why do we want check the changes before pushing?</strong> as an additional safeguard against breaking changes. Breaking changes could happen for a variety of reasons,\u00a0example:</p>\n<ul>\n<li>formatting errors in the endpoint list provided by the Cloud\u00a0Service</li>\n<li>structural changes in the webpage where the endpoints are listed, this for Cloud Services that provides the endpoint lists on webpages instead of JSON\u00a0files</li>\n</ul>\n<p>The proposed solution here is to detect <em>big</em> changes (for some definition of <em>big</em>) and ask for manual approval before updating the list if a <em>big</em> change is detected.</p>\n<p>Let\u2019s follow the steps one by one and see how we can implement our workflow. You can find a live example in<a href=\"http://jtschichold/mm-cloud-services-endpoints-tests\"> jtschichold/mm-cloud-services-endpoints-tests</a></p>\n<h4>1. Checkout the\u00a0repo</h4>\n<p>This is simple, there is a GitHub Action for that\u200a\u2014\u200aaction/checkout:</p>\n<a href=\"https://medium.com/media/d4c6d3e4dc9f8cbfb9ce251587192125/href\">https://medium.com/media/d4c6d3e4dc9f8cbfb9ce251587192125/href</a><p>Without arguments, the action checkouts the repo in the current directory from the default branch, or the default branch for the triggering event.</p>\n<h4>2. Fetch</h4>\n<p>Many endpoint lists are published as JSON files and easy to fetch using curl and post process using jq. But not all of them. To simplify this process we have a created a GitHub Action called jtschichold\\mm-cloud-services-miners that can be used to fetch endpoint lists from a number of Cloud Services. The action has been designed to be easy to extend. If you need a Cloud Service that is not supported by the action feel free to open a PR\u00a0:-)</p>\n<p>We can use the action in the workflow in the following way:</p>\n<a href=\"https://medium.com/media/56b82025cd39fdabccd8cd15b0ce6931/href\">https://medium.com/media/56b82025cd39fdabccd8cd15b0ce6931/href</a><p>The config input of the action points to a config file in the repo where the action finds the details about what to fetch and where to save the\u00a0results:</p>\n<a href=\"https://medium.com/media/a7c9c3179729991cb35d92169238d9e7/href\">https://medium.com/media/a7c9c3179729991cb35d92169238d9e7/href</a><p>The resulting files are JSON files with the list of the select IP Ranges. As an example, public-cloud-azuread-ips.txt looks like\u00a0this:</p>\n<pre>[<br>  \"13.64.151.161/32\",<br>  \"13.66.141.64/27\",<br>  \"13.67.9.224/27\",<br>...<br>  \"2603:1006:2000::/48\",<br>  \"2603:1007:200::/48\",<br>  \"2603:1016:1400::/48\",<br>  \"2603:1017::/48\",<br>  \"2603:1026:3000::/48\",<br>  \"2603:1027:1::/48\",<br>  \"2603:1036:3000::/48\",<br>  \"2603:1037:1::/48\",<br>  \"2603:1046:2000::/48\",<br>  \"2603:1047:1::/48\",<br>  \"2603:1056:2000::/48\",<br>  \"2603:1057:2::/48\"<br>]</pre>\n<p>You can read the full documentation of the action on the homepage <a href=\"https://github.com/jtschichold/mm-cloud-services-miners\">jtschichold/mm-cloud-services-miners</a></p>\n<h4>3 &amp; 4. Filter &amp; Transform</h4>\n<p>Now we have fetched the list of the endpoints, next step is filtering the lists for extra security and transform the lists in a format that can be consumed.</p>\n<p>We have created 2 GitHub Actions to simplify this step, both of them do filter &amp; transform lists\u200a\u2014\u200aone for IPs and one for\u00a0URLs:</p>\n<p><a href=\"https://github.com/jtschichold/mm-process-ip-list\">jtschichold/mm-process-ip-list</a> is used to filter IP lists, transform the lists in plain text format and (optionally) aggregate multiple lists. It can filter IP\u00a0lists:</p>\n<ul>\n<li>to exclude IP ranges overlapping a list of CIDRS read from a\u00a0file</li>\n<li>to exclude IP ranges with a small subnet mask (like 0.0.0.0/0\u2026)</li>\n<li>to exclude IP ranges overlapping Reserved IP addresses (see <a href=\"https://en.wikipedia.org/wiki/Reserved_IP_addresses\">Wikipedia</a>)</li>\n</ul>\n<p><a href=\"https://github.com/jtschichold/mm-process-urls-list\">jtschichold/mm-process-urls-list</a> is instead used to filter URL lists, transform the lists in other formats and (optionally again) aggregate multiple lists. It can filter URL lists to exclude URLs matching a list of regular expressions read from a\u00a0file.</p>\n<p>Let\u2019s see how can we use jtschichold/mm-process-ip-list to filter the lists we have created during the fetch\u00a0step:</p>\n<a href=\"https://medium.com/media/c58e32a403d202acd6a1d748d9c08cba/href\">https://medium.com/media/c58e32a403d202acd6a1d748d9c08cba/href</a><p>After this step, public-cloud-azuread-ips.txt looks like\u00a0this:</p>\n<pre>13.64.151.161/32<br>13.66.141.64/27<br>13.67.9.224/27<br>...<br>2603:1006:2000::/48<br>2603:1007:200::/48<br>2603:1016:1400::/48<br>2603:1017::/48<br>2603:1026:3000::/48<br>2603:1027:1::/48<br>2603:1036:3000::/48<br>2603:1037:1::/48<br>2603:1046:2000::/48<br>2603:1047:1::/48<br>2603:1056:2000::/48<br>2603:1057:2::/48</pre>\n<h4>5 &amp; 6. Check &amp;\u00a0Update</h4>\n<p>Before pushing the update to the repo, we want to make sure the new lists didn\u2019t change <em>much</em>. As git is the perfect tool track versions of text files, we can just use its abilities to compute some metrics over the changes in the new version of the feeds. We have embedded the logic in a GitHub Action, <a href=\"https://github.com/jtschichold/mm-check-changes\">jtschichold/mm-check-changes</a>:</p>\n<a href=\"https://medium.com/media/0336403edf98421875ca6f8654c8f8ee/href\">https://medium.com/media/0336403edf98421875ca6f8654c8f8ee/href</a><p>Now we can use this action to decide if we want to automatically push the changes or rather open a PR to ask for manual approval:</p>\n<a href=\"https://medium.com/media/e8608b90427e18d4c9441be9164efd6a/href\">https://medium.com/media/e8608b90427e18d4c9441be9164efd6a/href</a><h4>Final workflow</h4>\n<a href=\"https://medium.com/media/0789fd3dd8d4342f9a1811b2cefa651d/href\">https://medium.com/media/0789fd3dd8d4342f9a1811b2cefa651d/href</a><h3>Consume the\u00a0results</h3>\n<p>Nothing new here, thanks to GitHub once the results are pushed into the repo you can directly access the files using the classic GitHub URL\u00a0schema:</p>\n<pre><a href=\"https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt\">https://raw.githubusercontent.com/&lt;repo name&gt;/&lt;branch&gt;/&lt;</a>path to the file&gt;</pre>\n<p>Example from <a href=\"https://github.com/jtschichold/mm-cloud-services-endpoints-test\">jtschichold/mm-cloud-services-endpoints-test</a>:</p>\n<pre><a href=\"https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt\">https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt</a></pre>\n<p>The URL can be directly used in your scripts or in configuring an EDL on PAN-OS: <a href=\"https://docs.paloaltonetworks.com/pan-os/10-0/pan-os-admin/policy/use-an-external-dynamic-list-in-policy/external-dynamic-list.html\">https://docs.paloaltonetworks.com/pan-os/10-0/pan-os-admin/policy/use-an-external-dynamic-list-in-policy/external-dynamic-list.html</a></p>\n<h3>Summary</h3>\n<p>In this post we have covered the benefits of implementing a tracker of Cloud Services endpoints using GitHub Actions and how to do\u00a0it:</p>\n<ul>\n<li>Basic GitHub Actions workflow structure</li>\n<li>How to fetch the endpoints</li>\n<li>How to filter the fetched endpoints for additional security</li>\n<li>How to transform the endpoints in the desired\u00a0format</li>\n<li>How to prevent breaking changes and ask for manual\u00a0approval</li>\n</ul>\n<h3>Shut up and give me the\u00a0code</h3>\n<p>Here you\u00a0are:</p>\n<ul>\n<li>Live Example: <a href=\"https://github.com/jtschichold/mm-cloud-services-endpoints-test\">jtschichold/mm-cloud-services-endpoints-test</a>\n</li>\n<li>Fetch action: <a href=\"https://github.com/jtschichold/mm-cloud-services-miners\">jtschichold/mm-cloud-services-miners</a>\n</li>\n<li>Filter &amp; Transform action for IPs: <a href=\"https://github.com/jtschichold/mm-process-ip-list\">jtschichold/mm-process-ip-list</a>\n</li>\n<li>Filter &amp; Transform action for URLs: <a href=\"https://github.com/jtschichold/mm-process-url-list\">jtschichold/mm-process-url-list</a>\n</li>\n<li>Check changes action: <a href=\"https://github.com/jtschichold/mm-check-changes\">jtschichold/mm-check-changes</a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=978920fd27ca\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/continuous-scraping-of-cloud-services-endpoints-978920fd27ca\">Continuous Scraping of Cloud Services Endpoints</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8MiuCQ4qNzvBFPcISAwLKw.png\"><figcaption>Continuous Scraping</figcaption></figure><p><strong><em>TL;DR</em></strong><em>: The addition of GitHub Actions to the already feature rich GitHub platform created an interesting and powerful framework for tracking URLs/IP Ranges used by Cloud Services. Live Example: </em><a href=\"https://github.com/jtschichold/mm-cloud-services-endpoints-test\"><em>jtschichold/mm-cloud-services-endpoints-test</em></a></p>\n<p>Not too long after GitHub announced their Continuous Integration framework feature, GitHub Actions, some smart people had the idea of using GitHub Actions scheduling mechanism to periodically scrape web sites and track changes using\u00a0git.</p>\n<p>If you think for a moment with GitHub Actions you\u00a0have:</p>\n<ul>\n<li>a scheduling engine to periodically run (almost) arbitrary code (GitHub\u00a0Actions)</li>\n<li>a storage mechanism (git\u00a0repo)</li>\n<li>a super powerful change tracking mechanism (git\u00a0itself)</li>\n<li>a CDN to publish the results of your activities (GitHub)</li>\n<li>a mechanism to interact with users (GitHub Pull Requests)</li>\n<li>a mechanism to notify users (GitHub repo notifications)</li>\n</ul>\n<p><strong>This looks like a perfect fit for tracking low volume, slow changing feed\u200a\u2014\u200alike list of URLs/IPs used by Cloud Services</strong>. You can already implement trackers in hundreds of ways, but in most architectures some of the requirements listed above require ad-hoc coding, are painful to implement (looking at you, \u201c<em>user interaction</em>\u201d) or expensive (<em>CDN</em>). <strong>With GitHub Actions instead you have all the requirements offered as services in a nice package, ready to\u00a0use.</strong></p>\n<p>The following lines guide you thru the process of setting up a Cloud Services tracker, using some simple code we implemented to smooth some of the tasks. Thanks to the GitHub Actions <a href=\"https://docs.github.com/en/github/setting-up-and-managing-billing-and-payments-on-github/about-billing-for-github-actions\">Pricing structure</a> you can test this idea for free (as long as you keep your repo\u00a0public).</p>\n<h3>Quick introduction to GitHub\u00a0Actions</h3>\n<p><a href=\"https://github.com/features/actions\">GitHub Actions</a> is the Continuous Integration framework offered by GitHub and tightly integrated with their GitHub platform. GitHub Actions follows the same patterns used by other Continuous Integration services\u200a\u2014\u200ato configure GitHub\u00a0Actions:</p>\n<ul>\n<li>create a workflow definition file inside the directory\u00a0.github/workflows of your GitHub repo, using\u00a0YAML</li>\n<li>specify in the workflow file one or more list of <em>steps</em> to be executed and when they should be executed (the <em>triggers</em>)</li>\n<li>commit &amp;\u00a0push</li>\n</ul>\n<p>Let\u2019s look at a real world example to give you a better idea on how GitHub Actions workflows work and how to use them for scraping. The following is a commented excerpt taken from the source of my inspiration\u200a\u2014\u200athe repo <a href=\"https://github.com/alex/nyt-2020-election-scraper/blob/master/.github/workflows/scrape.yml\">alex/nyt-2020-election-scraper</a>. This workflow has a single <em>job</em> called scheduled that downloads the data of the US2020 Presidential Election and updates local tables, pushing the results back into the\u00a0repo:</p>\n<a href=\"https://medium.com/media/3183297900eed9d277f81b1bd898ec4d/href\">https://medium.com/media/3183297900eed9d277f81b1bd898ec4d/href</a><a href=\"https://medium.com/media/338ae661555bf56b31eeed4215ff8ae6/href\">https://medium.com/media/338ae661555bf56b31eeed4215ff8ae6/href</a><p>As you can see the scraping workflow is extremely easy to understand and flexible. For tracking Cloud Services we need some additional steps for processing and double checking the scraping results, but not too\u00a0many.</p>\n<p>You can find the full documentation with a more in-depth Quickstart on the official GitHub Action webpage: <a href=\"https://docs.github.com/en/actions\">https://docs.github.com/en/actions</a></p>\n<h3>Tracking Cloud\u00a0Services</h3>\n<p>To track Cloud Services endpoints we want a GitHub Action workflow looking like\u00a0this:</p>\n<ol>\n<li>\n<strong>Checkout</strong> the\u00a0repo</li>\n<li>\n<strong>Fetch</strong> the latest version of list of endpoints used by a Cloud Service from their feed/API/webpage</li>\n<li>\n<strong>Filter</strong> the list (if we want to exclude some IP ranges or\u00a0URLs)</li>\n<li>\n<strong>Transform</strong> the list in a format we can\u00a0consume</li>\n<li>\n<strong>Check</strong> the change in the latest version of the list against the one stored in the\u00a0repo</li>\n<li>\n<strong>Update</strong>. If the change is not <em>big</em>, commit the changes &amp; push. If the change is <em>big</em>, open a PR and ask for manual approval.</li>\n</ol>\n<p><strong>Why do we want check the changes before pushing?</strong> as an additional safeguard against breaking changes. Breaking changes could happen for a variety of reasons,\u00a0example:</p>\n<ul>\n<li>formatting errors in the endpoint list provided by the Cloud\u00a0Service</li>\n<li>structural changes in the webpage where the endpoints are listed, this for Cloud Services that provides the endpoint lists on webpages instead of JSON\u00a0files</li>\n</ul>\n<p>The proposed solution here is to detect <em>big</em> changes (for some definition of <em>big</em>) and ask for manual approval before updating the list if a <em>big</em> change is detected.</p>\n<p>Let\u2019s follow the steps one by one and see how we can implement our workflow. You can find a live example in<a href=\"http://jtschichold/mm-cloud-services-endpoints-tests\"> jtschichold/mm-cloud-services-endpoints-tests</a></p>\n<h4>1. Checkout the\u00a0repo</h4>\n<p>This is simple, there is a GitHub Action for that\u200a\u2014\u200aaction/checkout:</p>\n<a href=\"https://medium.com/media/d4c6d3e4dc9f8cbfb9ce251587192125/href\">https://medium.com/media/d4c6d3e4dc9f8cbfb9ce251587192125/href</a><p>Without arguments, the action checkouts the repo in the current directory from the default branch, or the default branch for the triggering event.</p>\n<h4>2. Fetch</h4>\n<p>Many endpoint lists are published as JSON files and easy to fetch using curl and post process using jq. But not all of them. To simplify this process we have a created a GitHub Action called jtschichold\\mm-cloud-services-miners that can be used to fetch endpoint lists from a number of Cloud Services. The action has been designed to be easy to extend. If you need a Cloud Service that is not supported by the action feel free to open a PR\u00a0:-)</p>\n<p>We can use the action in the workflow in the following way:</p>\n<a href=\"https://medium.com/media/56b82025cd39fdabccd8cd15b0ce6931/href\">https://medium.com/media/56b82025cd39fdabccd8cd15b0ce6931/href</a><p>The config input of the action points to a config file in the repo where the action finds the details about what to fetch and where to save the\u00a0results:</p>\n<a href=\"https://medium.com/media/a7c9c3179729991cb35d92169238d9e7/href\">https://medium.com/media/a7c9c3179729991cb35d92169238d9e7/href</a><p>The resulting files are JSON files with the list of the select IP Ranges. As an example, public-cloud-azuread-ips.txt looks like\u00a0this:</p>\n<pre>[<br>  \"13.64.151.161/32\",<br>  \"13.66.141.64/27\",<br>  \"13.67.9.224/27\",<br>...<br>  \"2603:1006:2000::/48\",<br>  \"2603:1007:200::/48\",<br>  \"2603:1016:1400::/48\",<br>  \"2603:1017::/48\",<br>  \"2603:1026:3000::/48\",<br>  \"2603:1027:1::/48\",<br>  \"2603:1036:3000::/48\",<br>  \"2603:1037:1::/48\",<br>  \"2603:1046:2000::/48\",<br>  \"2603:1047:1::/48\",<br>  \"2603:1056:2000::/48\",<br>  \"2603:1057:2::/48\"<br>]</pre>\n<p>You can read the full documentation of the action on the homepage <a href=\"https://github.com/jtschichold/mm-cloud-services-miners\">jtschichold/mm-cloud-services-miners</a></p>\n<h4>3 &amp; 4. Filter &amp; Transform</h4>\n<p>Now we have fetched the list of the endpoints, next step is filtering the lists for extra security and transform the lists in a format that can be consumed.</p>\n<p>We have created 2 GitHub Actions to simplify this step, both of them do filter &amp; transform lists\u200a\u2014\u200aone for IPs and one for\u00a0URLs:</p>\n<p><a href=\"https://github.com/jtschichold/mm-process-ip-list\">jtschichold/mm-process-ip-list</a> is used to filter IP lists, transform the lists in plain text format and (optionally) aggregate multiple lists. It can filter IP\u00a0lists:</p>\n<ul>\n<li>to exclude IP ranges overlapping a list of CIDRS read from a\u00a0file</li>\n<li>to exclude IP ranges with a small subnet mask (like 0.0.0.0/0\u2026)</li>\n<li>to exclude IP ranges overlapping Reserved IP addresses (see <a href=\"https://en.wikipedia.org/wiki/Reserved_IP_addresses\">Wikipedia</a>)</li>\n</ul>\n<p><a href=\"https://github.com/jtschichold/mm-process-urls-list\">jtschichold/mm-process-urls-list</a> is instead used to filter URL lists, transform the lists in other formats and (optionally again) aggregate multiple lists. It can filter URL lists to exclude URLs matching a list of regular expressions read from a\u00a0file.</p>\n<p>Let\u2019s see how can we use jtschichold/mm-process-ip-list to filter the lists we have created during the fetch\u00a0step:</p>\n<a href=\"https://medium.com/media/c58e32a403d202acd6a1d748d9c08cba/href\">https://medium.com/media/c58e32a403d202acd6a1d748d9c08cba/href</a><p>After this step, public-cloud-azuread-ips.txt looks like\u00a0this:</p>\n<pre>13.64.151.161/32<br>13.66.141.64/27<br>13.67.9.224/27<br>...<br>2603:1006:2000::/48<br>2603:1007:200::/48<br>2603:1016:1400::/48<br>2603:1017::/48<br>2603:1026:3000::/48<br>2603:1027:1::/48<br>2603:1036:3000::/48<br>2603:1037:1::/48<br>2603:1046:2000::/48<br>2603:1047:1::/48<br>2603:1056:2000::/48<br>2603:1057:2::/48</pre>\n<h4>5 &amp; 6. Check &amp;\u00a0Update</h4>\n<p>Before pushing the update to the repo, we want to make sure the new lists didn\u2019t change <em>much</em>. As git is the perfect tool track versions of text files, we can just use its abilities to compute some metrics over the changes in the new version of the feeds. We have embedded the logic in a GitHub Action, <a href=\"https://github.com/jtschichold/mm-check-changes\">jtschichold/mm-check-changes</a>:</p>\n<a href=\"https://medium.com/media/0336403edf98421875ca6f8654c8f8ee/href\">https://medium.com/media/0336403edf98421875ca6f8654c8f8ee/href</a><p>Now we can use this action to decide if we want to automatically push the changes or rather open a PR to ask for manual approval:</p>\n<a href=\"https://medium.com/media/e8608b90427e18d4c9441be9164efd6a/href\">https://medium.com/media/e8608b90427e18d4c9441be9164efd6a/href</a><h4>Final workflow</h4>\n<a href=\"https://medium.com/media/0789fd3dd8d4342f9a1811b2cefa651d/href\">https://medium.com/media/0789fd3dd8d4342f9a1811b2cefa651d/href</a><h3>Consume the\u00a0results</h3>\n<p>Nothing new here, thanks to GitHub once the results are pushed into the repo you can directly access the files using the classic GitHub URL\u00a0schema:</p>\n<pre><a href=\"https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt\">https://raw.githubusercontent.com/&lt;repo name&gt;/&lt;branch&gt;/&lt;</a>path to the file&gt;</pre>\n<p>Example from <a href=\"https://github.com/jtschichold/mm-cloud-services-endpoints-test\">jtschichold/mm-cloud-services-endpoints-test</a>:</p>\n<pre><a href=\"https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt\">https://raw.githubusercontent.com/jtschichold/mm-cloud-services-endpoints-test/main/adobe/endpoints-urls.txt</a></pre>\n<p>The URL can be directly used in your scripts or in configuring an EDL on PAN-OS: <a href=\"https://docs.paloaltonetworks.com/pan-os/10-0/pan-os-admin/policy/use-an-external-dynamic-list-in-policy/external-dynamic-list.html\">https://docs.paloaltonetworks.com/pan-os/10-0/pan-os-admin/policy/use-an-external-dynamic-list-in-policy/external-dynamic-list.html</a></p>\n<h3>Summary</h3>\n<p>In this post we have covered the benefits of implementing a tracker of Cloud Services endpoints using GitHub Actions and how to do\u00a0it:</p>\n<ul>\n<li>Basic GitHub Actions workflow structure</li>\n<li>How to fetch the endpoints</li>\n<li>How to filter the fetched endpoints for additional security</li>\n<li>How to transform the endpoints in the desired\u00a0format</li>\n<li>How to prevent breaking changes and ask for manual\u00a0approval</li>\n</ul>\n<h3>Shut up and give me the\u00a0code</h3>\n<p>Here you\u00a0are:</p>\n<ul>\n<li>Live Example: <a href=\"https://github.com/jtschichold/mm-cloud-services-endpoints-test\">jtschichold/mm-cloud-services-endpoints-test</a>\n</li>\n<li>Fetch action: <a href=\"https://github.com/jtschichold/mm-cloud-services-miners\">jtschichold/mm-cloud-services-miners</a>\n</li>\n<li>Filter &amp; Transform action for IPs: <a href=\"https://github.com/jtschichold/mm-process-ip-list\">jtschichold/mm-process-ip-list</a>\n</li>\n<li>Filter &amp; Transform action for URLs: <a href=\"https://github.com/jtschichold/mm-process-url-list\">jtschichold/mm-process-url-list</a>\n</li>\n<li>Check changes action: <a href=\"https://github.com/jtschichold/mm-check-changes\">jtschichold/mm-check-changes</a>\n</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=978920fd27ca\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/palo-alto-networks-developer-blog/continuous-scraping-of-cloud-services-endpoints-978920fd27ca\">Continuous Scraping of Cloud Services Endpoints</a> was originally published in <a href=\"https://medium.com/palo-alto-networks-developer-blog\">Palo Alto Networks Developer Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["github-actions","developer-relations","palo-alto-networks","security"]}]}