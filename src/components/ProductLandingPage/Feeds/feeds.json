{
  "version": "https://jsonfeed.org/version/1",
  "title": "HashiCorp Blog - Terraform",
  "home_page_url": "https://hashicorp.com/blog/products/terraform",
  "description": "Terraform is a platform for building, changing, and managing infrastructure in a safe, repeatable way.",
  "favicon": "https://www.hashicorp.com/favicon.svg",
  "author": {
    "name": "HashiCorp, Inc."
  },
  "items": [
    {
      "guid": "https://www.hashicorp.com/blog/terraform-1-10-improves-handling-secrets-in-state-with-ephemeral-values",
      "url": "https://www.hashicorp.com/blog/terraform-1-10-improves-handling-secrets-in-state-with-ephemeral-values",
      "title": "Terraform 1.10 improves handling secrets in state with ephemeral values",
      "content_html": "<p>Today, we are announcing the general availability of HashiCorp Terraform 1.10, which is <a href=\"https://developer.hashicorp.com/terraform/install?product_intent=terraform\">ready for download</a> and immediately available for use in <a href=\"https://www.hashicorp.com/products/terraform\">HCP Terraform</a>. This version introduces ephemeral values, for secure secrets handling and other improvements. </p>\n\n<h2>Ephemeral values to enable secure handling of secrets</h2>\n\n<p>Terraform’s management of infrastructure involves handling secrets, such as private keys, certifications, API tokens, etc. As an example, a data source may be used to fetch a secret and write it to a managed resource’s attribute. Or a secret is generated by a resource type (e.g. a random password) and is written to another resource type like a dedicated secrets manager. </p>\n\n<p>Today, these secrets get persisted in the plan or state file. Since the secrets are stored in plaintext within these artifacts, any mismanaged access to the files would compromise the secrets. We’ve been working on a feature to improve the security of this workflow, and it’s now ready for Terraform 1.10.. </p>\n\n<p>To enable secure handling of secrets, we’re introducing ephemeral values. These values are not stored in any artifact. Not the plan file or the statefile. They are not expected to remain consistent from plan to apply, or from one plan/apply round to the next. Ephemeral values encompass the following language constructs:</p>\n\n<ul>\n<li><strong>Ephemeral input variables and output variables:</strong> Similar to marking a value as <code>sensitive</code>, you can now mark the input variables and output variables as <code>ephemeral</code>. Marking an input variable as <code>ephemeral</code> is useful for data that only needs to exist temporarily, such as a short-lived token or session identifier.</li>\n<li><strong>Ephemeral resources:</strong> A new third resource mode alongside managed resource types and data resources. These are declared with <code>ephemeral</code> blocks, which declare that something needs to be created or fetched separately for each Terraform phase, then used to configure some other ephemeral object, and then explicitly closed before the end of the phase.</li>\n<li><strong>Managed resources’ write-only attribute:</strong> A new attribute for managed resources, which has a property that can only be written to, not read. Write-only attributes will be available in Terraform 1.11.</li>\n</ul>\n\n<p>Ephemeral values represent an advancement in how Terraform helps you manage your infrastructure. Whether you are using it to generate credentials, fetch a token, or setting up a temporary network tunnel, ephemeral values will ensure that these values are not persisted in Terraform artifacts. </p>\n\n<h3>Ephemeral values in practice</h3>\n\n<p>In this example, an ephemeral resource is being used to fetch a secret from AWS Secrets Manager, which is then used to initialize the PostgresSQL provider. Before Terraform 1.10, a data source was used to fetch the secret, in which case the secret value would be stored in both the plan and state file. With the introduction of ephemeral values, the secret is now kept secure because it is no longer stored in any artifact.</p>\n<pre><code>provider \"aws\" {\n  region = \"eu-west-2\"\n}\n\ndata \"aws_db_instance\" \"example\" {\n  db_instance_identifier = \"testdbinstance\"\n}\n\nephemeral \"aws_secretsmanager_secret_version\" \"db_master\" {\n  secret_id = data.aws_db_instance.example.master_user_secret[0].secret_arn\n}\n\nlocals {\n  credentials = jsondecode(ephemeral.aws_secretsmanager_secret.db_master.secret_string)\n}\n\nprovider \"postgresql\" {\n  host     = data.aws_db_instance.example.address\n  port     = data.aws_db_instance.example.port\n  username = local.credentials[\"username\"]\n  password = local.credentials[\"password\"]\n}\n\nresource \"postgresql_database\" \"db\" {\n  name = \"new_db\"\n}</code></pre><h2>Available in Terraform 1.10</h2>\n\n<p>The Terraform 1.10 release includes ephemeral input and output variables, ephemeral resources, and new functions (<code>ephemeralasnull</code>, <code>terraform.applying</code>). There are currently ephemeral resources available for three different providers:</p>\n\n<ul>\n<li><strong>AWS:</strong> <code>aws_secretsmanager_secret_version</code>, <code>aws_lambda_invocation</code></li>\n<li><strong>Azure:</strong> <code>azurerm_key_vault_secret, azurerm_key_vault_certificate</code></li>\n<li><strong>Kubernetes:</strong> <code>kubernetes_token_request</code>, <code>kubernetes_certificate_signing_request</code></li>\n</ul>\n\n<p>Ephemeral resources are currently available in the AWS, Azure, Kubernetes, and <a href=\"https://registry.terraform.io/providers/hashicorp/random/latest/docs\">random</a> providers, and will be supported in the Google Cloud  provider on December 9th. Ephemeral resources in Google Cloud will include:</p>\n\n<ul>\n<li><strong>Google Cloud:</strong> <code>google_service_account_access_token</code>, <code>google_service_account_id_token</code>, <code>google_service_account_jwt</code>, <code>google_service_account_key</code></li>\n</ul>\n\n<h2>Other improvements and next steps</h2>\n\n<p>Terraform 1.10 also includes other enhancements outlined in the <a href=\"https://github.com/hashicorp/terraform/blob/v1.10/CHANGELOG.md\">changelog</a>. One key inclusion is more performance improvements:</p>\n\n<ul>\n<li><strong>Performance improvements:</strong> We refactored plan changes and reduced repeated decoding in resource state to improve plan and apply performances. This will help mitigate performance issues when a large number of resource instances are evaluated.</li>\n</ul>\n\n<p>To get started with HashiCorp Terraform:</p>\n\n<ul>\n<li><a href=\"https://developer.hashicorp.com/terraform/downloads\">Download Terraform 1.10</a></li>\n<li><a href=\"https://app.terraform.io/public/signup/account\">Sign up for a free HCP Terraform account</a></li>\n<li><a href=\"https://developer.hashicorp.com/terraform/language/v1.10.x/upgrade-guides\">Read the Terraform 1.10 upgrade guide</a></li>\n<li>Get hands-on with tutorials at <a href=\"https://developer.hashicorp.com/terraform/tutorials\">HashiCorp Developer</a></li>\n</ul>\n\n<p>As always, this release wouldn&#39;t have been possible without the great community feedback we&#39;ve received via GitHub issues, HashiCorp Discuss forums, and from our customers. Thank you!</p>",
      "summary": "Terraform 1.10 is generally available, and it includes ephemeral values along with improvements to plan and apply performances.",
      "date_published": "2024-11-27T18:00:00.000Z",
      "author": {
        "name": "Garvita Rai"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/5-ways-to-improve-devex-and-security-for-infrastructure-provisioning",
      "url": "https://www.hashicorp.com/blog/5-ways-to-improve-devex-and-security-for-infrastructure-provisioning",
      "title": "5 ways to improve DevEx and security for infrastructure provisioning",
      "content_html": "<p>In a competitive business environment, where competitors are working every day to poach your customers, speed is everything. When your product includes software, your focus needs to be squarely on giving your application developers more time to work on high-value projects, eliminating tedious manual tasks, and automating as much of their development and deployment processes as possible. </p>\n\n<p>This means sysadmins, DevOps engineers, or <a href=\"https://www.hashicorp.com/resources/what-is-a-platform-team-and-why-do-we-need-them\">platform teams</a> need to stop using most of their manual infrastructure provisioning and review workflows and start giving developers the keys to provision their own infrastructure (i.e. servers, databases, load balancers, caches, firewall settings, queues, monitoring, subnet configurations, routing rules) on a platform with great developer experience (DevEx).</p>\n<img src=https://www.datocms-assets.com/2885/1732301744-sysadmins-yearning-for-better-processes.png alt=Give me your sysadmins, your scripters, your click-ops engineers yearning for better processes.><p>Here are five infrastructure provisioning strategies you can start implementing right now to speed up your software delivery.</p>\n\n<h2>1. Automate provisioning with IaC</h2>\n\n<p>Manual infrastructure provisioning and management often require lengthy approval processes and manual tracking of resources, which are slow, tedious, and error-prone tasks. So why do some companies continue to use these traditional system administration methods?</p>\n\n<p>It typically comes down to one of two reasons: </p>\n\n<ol>\n<li>Developers and sysadmins/DevOps engineers use whatever vendor console or one-off script works best in the moment, not worrying about future process stability. This usually happens when an organization has loosely defined standards.</li>\n<li>An organization is tightly controlled, following rigid legacy processes for infrastructure management that require layers of approvals and oversight.</li>\n</ol>\n\n<p>By automating infrastructure provisioning, using an <a href=\"https://www.hashicorp.com/resources/what-is-infrastructure-as-code\">infrastructure as code (IaC) solution</a>, organizations can achieve both speed and control. </p>\n\n<p>IaC allows infrastructure to be: </p>\n\n<ul>\n<li>Codified</li>\n<li>Standardized</li>\n<li>Replicated</li>\n<li>Reused</li>\n<li>Stored and centralized</li>\n<li>Audited</li>\n<li>and versioned </li>\n</ul>\n\n<p>By defining infrastructure as code, teams can automate, understand, modify, and control their infrastructure deployment more easily. Meaning things move much faster.</p>\n\n<p>How much faster? </p>\n\n<p><a href=\"https://www.hashicorp.com/case-studies/vodafone-italy\">Vodafone, a leading Italian telecommunications company, is one company that has used infrastructure automation both to lower costs and to increase its competitiveness.</a> According to Emanuele Di Saverio, Center of Excellence Lead for automation at Vodafone, dependency on legacy provisioning processes severely limited their business. </p>\n\n<blockquote>\n<p>“It put us at a disadvantage both financially and competitively. We had to over-purchase and live with the spare capacity in order to be responsive to business requests. More importantly, this approach delayed upgrades and updates to existing services as well as releases of inventive new ones, which are central to the business’s long-term strategic vision.”<br><br>—Emanuele Di Saverio, Center of Excellence Lead for automation at Vodafone</p>\n</blockquote>\n\n<p>Massimiliano Romano, DevOps platform architect at Vodafone Italy, added, “Since it’s all cloud-based, we realized the only way to keep up with service demand and the various compliance standards we have to meet would be to unwind our centralized approach to infrastructure development in favor of a faster, autonomous model.” </p>\n\n<p>Using IaC, Vodafone automated 95% of infrastructure provisioning, accelerated their release cadence by 3X, cut infrastructure setup time from three months to one week, and did it all while ensuring every compliance requirement was met.</p>\n\n<h2>2. Build and reuse golden configurations</h2>\n\n<p>Oftentimes, one casualty of moving fast in the cloud is consistency. When teams work in silos, manually provisioning infrastructure using the most expedient method, the result is a non-standard, unwieldy cloud environment (<a href=\"https://infrastructure-as-code.com/posts/snowflakes-as-code.html\">“snowflake” server</a>) that is difficult to manage, often wastes resources, and could potentially open up security vulnerabilities.  </p>\n\n<p>Adopting IaC is the first step to resolving this — without it you can’t have a single language to define all of your infrastructure configurations. The next step is to use that single infrastructure language to create templates for various teams to share. Without these, teams often reinvent solutions to the same problems (<a href=\"https://www.cnpatterns.org/development-design/avoid-reinventing-the-wheel\">reinventing the wheel</a>), wasting time and creating infrastructure configurations that often fail to meet security standards or require manual intervention to ensure resources are protected.</p>\n\n<p>Templates help organizations stop reinventing the wheel. As you create more, you start to build a <a href=\"https://www.hashicorp.com/blog/a-golden-path-to-secure-cloud-provisioning-with-the-infrastructure-cloud\">golden path</a>, where developers and other infrastructure users know that they have standard infrastructure provisioning templates that include golden images, golden modules, and golden automated policy checks that they can pick from a library and start using without having to understand the company’s cost controls, security requirements, or operational needs. Those things are already baked into the golden configurations and blessed by all relevant stakeholders before anyone starts using them.</p>\n\n<p>The task of building out these components lies with platform teams, who will work with security, cost, and compliance stakeholders to design the configurations. Then they’ll test and validate the code and finally deploy it to a central library where developers can easily access and reuse it.</p>\n\n<p>Golden configurations take the guesswork out of provisioning infrastructure. They not only accelerate developer workflows but also give executives confidence that the business is secured and compliant with regulations.</p>\n\n<p><a href=\"https://www.hashicorp.com/case-studies/cielo\">Cielo S.A., the leading electronic payments provider in Latin America</a>, used standard configurations to reduce infrastructure provisioning time by 90%, which resulted in a 5X increase in speed to market while also meeting their security and compliance goals.</p>\n\n<blockquote>\n<p>“Competition in payment processing in Brazil is intense and only getting more challenging. We needed to accelerate delivery of new solutions, features, and functionality to our customers if we wanted to maintain our market advantage. But to do that we needed a way to provision infrastructure faster, more efficiently, and in a cost-effective way.”<br><br>—Antonio Lombardi Neto, Infrastructure and Telecom Director at Cielo</p>\n</blockquote>\n\n<p>Neto added that using an IaC solution with reusable modules helped them meet their goals and positioned the company for even more success. “<a href=\"https://www.terraform.io/\">Terraform</a> helped us reduce the average infrastructure delivery time from 1 month to under 15 minutes, and cut change request time up to 50%,” he said. “The end result is that our time-to-market with new products and features is five times faster than before, ensuring we’re able to meet the evolving needs of our partners and customers and stay ahead of a growing list of competitors.”</p>\n\n<h2>3. Leverage policy as code</h2>\n\n<p>Enforcing security, compliance, cost, or operations policies consistently across an organization is challenging. Organizations tend to fall back on manual processes such as ticket queues and manual reviews to provide oversight and to ensure no security gaps are created. This slows down infrastructure provisioning drastically.</p>\n\n<p>To address this challenge, many organizations use automated <a href=\"https://developer.hashicorp.com/sentinel/docs/concepts/policy-as-code\">policy as code</a> checks. Policy <em>as code</em> is important in the same ways that IaC is important. It enables organizations to flexibly create, review, and version control automated provisioning checks in a single language that compliance and security stakeholders can understand, audit, and contribute to. With the help of stakeholders, platform teams can build a library of standard (golden) policy templates, store them in a central library, and then pin them to run in various provisioning workflows, or in every provisioning run. This ensures no environment is provisioned without proper guardrails, and it eliminates the manual review and approval bottlenecks discussed earlier, shaving hours, days, or possibly weeks off of your provisioning wait time.</p>\n\n<p><a href=\"https://www.hashicorp.com/case-studies/trimble\">Trimble, a positioning, modeling, connectivity, and data analytics software provider for essential global industries</a><span style=\"text-decoration:underline;\">,</span> had manual compliance checks that really hampered the company’s ability to move fast. </p>\n\n<blockquote>\n<p>“Manually building and deploying all the infrastructure made lead time for new projects and new infrastructure very hard to achieve because we spend so much time on <em>toil:</em> repeatable tasks that, without documentation or automation, consumed almost all of our time. We didn’t have a centralized way of managing infrastructure as code or continuous integration and deployment (CI/CD), which created a number of issues with version conflict that resulted in a lot of additional overhead and some friction among teams.”<br><br>—John Weigand, Senior DevOps Engineer for Trimble eBuilder</p>\n</blockquote>\n\n<p>Weigand further explained that some of the friction involved questions about security. “Doing everything manually increased the risk of errors and challenged our ability to maintain compliance with stringent security and compliance standards,” he said.</p>\n\n<p>By automating infrastructure provisioning and using policy as code, Trimble reduced infrastructure development from three days to one hour and enabled the company to control deployment based on specific policies, helping developers identify compliance, security, and governance issues much earlier in the development cycle.</p>\n\n<h2>4. Enable self-service infrastructure</h2>\n\n<p>The more developers can do on their own, the faster new features and products can be delivered to the market. As mentioned before, ticketing systems and other manual processes, however, slow down developers, forcing them to wait on gatekeepers and hampering production. </p>\n\n<p>Self-service addresses this issue and enables organizations to scale cloud infrastructure for greater innovation quickly and efficiently. It gives freedom and autonomy to developers without allowing them to venture outside of the golden path explained earlier. Typically the golden configurations in this path start out as modules or images that developers have to do some manual work with, either through version control or command line interfaces, to start using.  It requires some knowledge of the tooling by the developer, and it’s not a completely “push-button” process.</p>\n\n<p>Platform teams can improve developer efficiency even further by enhancing self-service through <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/no-code-provisioning\">no-code provisioning</a>. This is exactly what it sounds like, developers can use these enhanced no-code IaC modules to provision infrastructure for their apps, without needing to type any code or understand any of the underlying systems or tools. They just select a module, and go.</p>\n\n<p>To truly give infrastructure provisioning and management a push-button self-service experience, platforms should eventually work toward setting up an <a href=\"https://www.youtube.com/watch?v=2Nrlkn-km5A\">internal developer platform (IDP)</a> layer and interface so that developers can just click on a menu of no-code modules and run a variety of workflows all from one portal without needing to trigger a lot of tools at runtime.</p>\n\n<p>No one understands this better than Nedbank Group, one of South Africa&#39;s four largest banks. <a href=\"https://www.hashicorp.com/blog/5-ways-to-improve-devex-and-security-for-infrastructure-provisioning%5BNedbank\">https://www.hashicorp.com/blog/5-ways-to-improve-devex-and-security-for-infrastructure-provisioning[Nedbank</a> decreased the time it took to deliver infrastructure to apps teams by 99%, largely through IaC and self-service capabilities](<a href=\"https://www.hashicorp.com/case-studies/nedbank\">https://www.hashicorp.com/case-studies/nedbank</a>).</p>\n\n<p>According to Freddy Ambani, Nedbank’s Head of Cloud Operations, “Our aspiration is to enable every developer to be as productive as possible with hyper-automation for consumption of all our cloud services, which will improve time to market.”</p>\n\n<p>Using an IaC solution and leveraging self-service capabilities has helped NedBank reach its goal. “Infrastructure procurement that used to take several months now happens in just a few minutes, and we’re able to complete projects at 25% lower resource costs,” Ambani said.</p>\n\n<h2>5. Delete infrastructure automatically</h2>\n\n<p>It’s easy to waste money on cloud resources. In fact, out of control cloud spending is one the biggest concerns for many business leaders. When organizations scale quickly, unless there is an automated process to do so, cloud instances that are no longer used often get forgotten, wasting money as they continue to run without performing any necessary function.</p>\n\n<p>By <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/settings/deletion#automatically-destroy\">automating infrastructure destruction</a>, organizations can set end-of-life dates or pre-defined inactivity periods and use them to remove cloud infrastructure without the need for manual tracking, audits, or intervention. Though not directly connected to accelerating provisioning, automated deletion does remove dependency on manual processes, freeing up time and resources, and is a critical step in healthy <a href=\"https://www.hashicorp.com/infrastructure-cloud/infrastructure-lifecycle-management\">Infrastructure Lifecycle Management (ILM)</a>. </p>\n\n<h2>The power of automation</h2>\n\n<p>Leveraging best practices for ILM and leaning into automation can eliminate provisioning bottlenecks, accelerate developer workflows, and help organizations go to market faster — all without compromising on compliance or security. For more information on how to implement a platform approach to achieve ILM maturity, check out <a href=\"https://www.hashicorp.com/on-demand/infrastructure-lifecycle-management-with-the-hashicorp-cloud-platform-email?utm_source=direct&utm_medium=organic&utm_campaign=&utm_content=blog&utm_offer=article\">Infrastructure Lifecycle Management with the HashiCorp Cloud Platform</a>. </p>",
      "summary": "Still using manual scripting and provisioning processes? Learn how to accelerate provisioning using five best practices for Infrastructure Lifecycle Management.",
      "date_published": "2024-11-22T18:00:00.000Z",
      "author": {
        "name": "Mitchell Ross"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/fix-the-developers-vs-security-conflict-by-shifting-further-left",
      "url": "https://www.hashicorp.com/blog/fix-the-developers-vs-security-conflict-by-shifting-further-left",
      "title": "Fix the developers vs. security conflict by shifting further left",
      "content_html": "<p>Prolonged tension between developers and security teams can undermine the efficacy of cloud security. Security teams resent it when developers ignore their guidelines, forcing them to patch vulnerabilities after apps are in production. Developers want security teams to plug gaps and stay off their backs — they have frequent deadlines to meet. Collaboration between them suffers. Not surprisingly, getting these teams to operate from the same playbook is easier said than done.  </p>\n\n<p>CISOs and developers know this all too well. Nearly two in three CISOs and developers agree that “a lack of communication and collaboration between their teams is a problem when it comes to implementing better software supply chain security,” according to a <a href=\"https://get.chainguard.dev/hubfs/Chainguard-Harris-Poll-ciso-and-developer-trends-Report.pdf\">software security study</a> by Chainguard and the Harris Poll. The December 2023 study also found that tooling is another source of tension between these teams: “73% of developers agree that the work/tools their security team requires them to use interfere with their productivity and innovation.”‍</p>\n\n<p>Yet, defusing the tension between these teams doesn’t require a federal mediator. The turning point begins when <a href=\"https://www.hashicorp.com/resources/what-is-a-platform-team-and-why-do-we-need-them\">platform teams</a> remove one of the biggest obstacles: <strong>conflicting toolchains</strong> that cause inefficiencies and introduce vulnerabilities that result in costly errors. <strong>A lack of automation</strong> also hinders organizations from managing their cloud resources efficiently.</p>\n\n<p>Getting dev and sec to work cleanly together demands tooling changes associated with cloud migration, including automation for scale and provisioning dynamic infrastructure. Unfortunately, many toolchains (especially in cybersecurity) were built for the static, on-prem era of infrastructure. Lacking automation, platform teams can’t easily switch from “static” infrastructure to “dynamic” infrastructure with these outdated toolsets.</p>\n\n<p>Platform teams play an important, if underappreciated, role in solving this impasse. This post recommends field-tested tools and products that establish a secure, golden developer path — reducing friction and satisfying both teams’ objectives.</p>\n\n<h2>Your “shift-left” needs a platform</h2>\n\n<blockquote>\n<p>“Maybe they don’t want to think about infosec. They’re worried about their software architecture. They don’t want to think about CMDB [a configuration management database]. Patching compliance, forget it. They just want to go fast.” <br><br>\n— Chad Prey, “<a href=\"https://www.hashicorp.com/resources/terraform-for-the-rest-of-us-a-petco-ops-case-study\">Terraform for the Rest of Us: A Petco Ops Case Study</a>”</p>\n</blockquote>\n\n<p>The “shift-left” movement, an idea that was coined decades ago around testing, was an attempt to fix friction between teams when security and quality assurance were being tested at the end of an application’s development lifecycle. The general idea was to push testing and other aspects of security and QA review to the early and middle stages of development, not just at the end, when reviews would find multiple issues and send an app back into redesign stages. </p>\n\n<p>During the emergence of DevOps philosophies, the shift-left movement went hand-in-hand with cross-team skilling, where managers wanted to turn developers and IT pros into experts in everything from development to operations (and sometimes cybersecurity). For companies that couldn’t afford the top 1% of engineering talent, this movement fell flat on its face.  </p>\n\n<p>Trying to get all developers to become well-versed in cybersecurity so that those controls and strategies can be implemented in the design phase is unrealistic — <strong>it’s the wrong way to shift-left</strong>.</p>\n\n<p>Instead of focusing fully on shifting security left from a culture or skilling perspective, it needs to be done with tools, because <a href=\"https://www.youtube.com/watch?v=LCuAcc_ne6k\">software eats culture for breakfast</a>. Think of it as shifting <em>further</em> left.</p>\n\n<p>It’s “further” left because it’s there before the developer even starts coding. Secure designs are <em>baked-into</em> the various templates developers use to start a project in CI/CD and in their infrastructure platform. When organizations deploy a platform-based shift-left approach that leverages APIs, automated checks, self-service tooling, and guardrails like secure modules and <a href=\"https://developer.hashicorp.com/sentinel/docs/concepts/policy-as-code\">policy as code</a>, they avoid the bottlenecks created by development teams submitting changes for manual review by security, Ops, or compliance teams. </p>\n\n<p>At the same time, security teams can be confident knowing they have embedded required policies and best practices before code and applications make it to production.** There are fewer tickets for them!**</p>\n\n<p>Developers will like the fact that they can just code and not have to know a lot about cybersecurity best practices, or keep up with changing company compliance and security policies. As long as the guardrails don’t become a “golden cage” rather than facilitating a golden path, developers should work much faster.  Eventually platform teams want the developer experience for security to feel <em>invisible</em>, removing as many manual touchpoints as possible. For example, with the right <a href=\"https://www.vaultproject.io/use-cases/secrets-management\">secrets management</a> solution, developers should barely realize they&#39;re managing secrets in their workflow.</p>\n\n<h2>How to build a platform that helps dev and sec work together effectively</h2>\n\n<p>The National Institute of Standards and Technology (NIST) strongly advocates <a href=\"https://csrc.nist.gov/Projects/devsecops\">DevSecOps</a> partly because it “[r]educes friction between the development, operation, and security teams in order to maintain the speed and agility needed to support the organization’s mission while taking advantage of modern and innovative technology.” </p>\n\n<p>While NIST continues to identify software supply chain and DevOps security practices, many organizations now embrace DevSecOps but haven’t deployed “modern and innovative technology” to improve their cloud security and reduce the stress between their infrastructure and development teams.</p>\n\n<p>Today, a modern platform must prioritize cloud security and the developer experience, establishing a secure and consistent workflow that supports all teams in the delivery pipeline. Platform teams should pick tools and products that excel at these functions: </p>\n\n<ul>\n<li>Version control tooling</li>\n<li>Static and dynamic scanning tools</li>\n<li>Secrets management platforms</li>\n<li>Secret scanning tools</li>\n<li><a href=\"https://www.hashicorp.com/resources/what-is-infrastructure-as-code\">Infrastructure as code</a> provisioning platforms with built-in policies as code engines</li>\n<li>Secure remote access tools<br></li>\n<li>Image and module lifecycle management</li>\n</ul>\n\n<p>In particular, platform teams want to focus on the lifecycles that matter most to developers and security teams: </p>\n\n<ul>\n<li>Infrastructure Lifecycle Management (ILM): A systematic and repeatable approach to creating, securing, and maintaining infrastructure.<br></li>\n<li>Security Lifecycle Management (SLM): A systematic way for organizations to manage their most sensitive data, especially secrets/credentials, from creation to expiration or revocation. This also includes having a platform for the management of remote access sessions.</li>\n</ul>\n\n<p>While executives and managers can take years shopping for dozens of products to build holistic ILM and SLM solutions, or even longer trying to manage initiatives to roll their own toolchains, the savvy leaders are focused on consolidating down to fewer tools with a small number of trusted partner vendors (we know that <a href=\"https://lsvp.com/cyber60-2024-2025/\">half of CISOs</a> are asking for tool consolidation right now).</p>\n\n<p>HashiCorp is one of those trusted ILM and SLM partners to <a href=\"https://www.hashicorp.com/resources?contentType=Case%20Study\">thousands of customers</a> with a consolidated solution: <a href=\"https://www.hashicorp.com/infrastructure-cloud\">The Infrastructure Cloud</a>. It includes the world’s <a href=\"https://survey.stackoverflow.co/2024/technology/#1-other-tools\">most popular</a> infrastructure as code provisioner, <a href=\"https://www.hashicorp.com/products/terraform\">HashiCorp Terraform</a>, and the <a href=\"https://radar.cncf.io/2021-02-secrets-management\">gold standard</a> of secrets management platforms, <a href=\"https://www.hashicorp.com/products/vault\">HashiCorp Vault</a>. Along with other products, organizations can deploy Terraform, Vault, and other components of the Infrastructure Cloud as on-prem, self-managed software, or as managed services on the HashiCorp Cloud Platform (HCP).</p>\n\n<h2>What the right SLM and ILM tools can do</h2>\n\n<p>Modern ILM is all about empowering developers to provision cloud and on-prem resources quickly without a burdensome, ticket-heavy or review-heavy workflow. Platform teams avoid these problems by providing a standardized shared service with curated self-service workflows, tools, and templates for developers that propagate best practices for every deployment while automating secure practices and guardrails. </p>\n\n<p>HCP Terraform and Terraform Enterprise support secure provisioning, enabling best practices including:</p>\n\n<ol>\n<li><strong>Standardized workflows</strong> — Baking security fundamentals into the workflow with templates that empower even junior developers to become highly productive. </li>\n<li><p><strong>Secure modules</strong> — Managing version control and provisioning becomes easier when you codify, store, version, and deprecate modules in one place.</p></li>\n<li><p><strong>Policy as Code guardrails and gates</strong> — Automating the enforcement of identity and access management (IAM) controls, CIS benchmarks, proper infrastructure tagging, and the storage location of data (for GDPR compliance).</p></li>\n<li><p><strong>Custom condition checks</strong> — Platform engineers can add ongoing security checks at all phases of the infrastructure lifecycle to detect insecure modules that might slip through other guardrails.</p></li>\n<li><p><strong>Drift detection and continuous validation</strong> — Teams need a system to detect problems leading to outages, higher costs, and vulnerabilities. </p></li>\n<li><p><strong>Observability</strong> — Teams need visibility into workspaces, a reporting component, and a clear audit trail for all changes.</p></li>\n</ol>\n\n<blockquote>\n<p>“We have many developers who want to deploy apps on the cloud, but they aren’t familiar with all the different cloud service providers, or they might want to deploy their application on multiple clouds. By using modules, we can deploy standardized solutions across multiple clouds using a common syntax: HashiCorp Terraform [which uses HashiCorp configuration language (HCL)]. We’re able to bake in our security controls so our developers don’t have to go look at a long list of controls before they’re able to do anything in the cloud.”<br><br>\n— Itay Cohai, “<a href=\"https://www.hashicorp.com/resources/compliance-at-scale-hardened-terraform-modules-at-morgan-stanley\">Compliance at Scale: Hardened Terraform Modules at Morgan Stanley</a>”</p>\n</blockquote>\n\n<p>For modern SLM practices, secrets management is the core focus since compromised credentials are still the <a href=\"https://www.verizon.com/business/resources/reports/dbir/\">#1 cause of most breaches</a>. It’s also the antidote to <a href=\"https://www.hashicorp.com/resources/what-is-secret-sprawl-why-is-it-harmful\">secret sprawl</a>, where secrets such as passwords are kept in obvious, often unguarded places for attackers to find and exploit. </p>\n\n<p>HashiCorp Vault makes implementing a scalable, secrets management program with solid governance, auditing, and security easy. The key is centralizing your management through one control plane.</p>\n\n<p>Here are five best practices for a well-managed secrets management platform: </p>\n\n<ol>\n<li><strong>Central secrets control plane</strong> — Reduces errors, speeds up debugging and auditing, and simplifies security management </li>\n<li><strong>Access control lists</strong> — Limit lateral movement through your systems </li>\n<li><strong>Dynamic or auto-rotated secrets</strong> — Temporary credentials that reduce the time of breach </li>\n<li><strong>Encryption as a service</strong> — Prevents breaches and enables encrypted data during transit as a service </li>\n<li><strong>Auditing</strong> — Better understanding of your security posture and breach detection</li>\n</ol>\n\n<p><a href=\"https://www.canva.com/\">Canva</a>, an online platform for visual communication and graphic design, sought to simplify secrets management for its developers using Vault: </p>\n\n<blockquote>\n<p>&quot;They&#39;ll just get some sort of key with a click or two and then plug that key into their target client. The secrets management system should take care of issuing the secret to the correct client and integrate with a wide array of products and all the major cloud providers.&quot; <br><br>\n—Moe Abbas, “<a href=\"https://www.hashicorp.com/resources/streamlining-secrets-management-at-canva-with-hashicorp-vault\">Streamlining secrets management at Canva with HashiCorp Vault</a>”</p>\n</blockquote>\n\n<h3>Vault&#39;s impact on Canva’s security was impressive:</h3>\n\n<ul>\n<li>Closed a whole risk category in the business by removing direct engineering access to secrets kept in Vault.</li>\n<li>87.5% reduction in processes around secret provisioning. </li>\n<li>1.2 million secrets were issued by Vault in May 2024, and it’s still growing.</li>\n<li>100% of secrets can be attributed back to an owner with access to a complete audit trail in seconds.</li>\n</ul>\n\n<h2>Leveling up dev and sec collaboration</h2>\n\n<p>Misaligned priorities, mismatched tools, and inconsistent workflows are the precursors of friction between security and development teams. Prolonged problems between these teams elevate security and compliance risks and hinder development speed and time to market. Platform teams understand that the right tools can propel a cultural shift that reduces risk and cost while accelerating production.</p>\n\n<p>Platform teams understand that an effective cloud security program eliminates friction, enables reproducibility, and establishes infrastructure automation. The Infrastructure Cloud helps organizations shift left, lifting the burden of implementing security requirements from development teams and removing many common friction points between security and dev teams.</p>\n\n<p>Move fast and secure things by bridging the gap between developers and security teams. <a href=\"https://www.hashicorp.com/on-demand/move-fast-and-secure-things\">Read our white paper</a> or <a href=\"https://www.hashicorp.com/lp/cloud-wars-balancing-speed-and-security-for-seamless-development\">attend our upcoming webinar</a> to learn practical strategies that help improve cross-team collaboration in the cloud.</p>",
      "summary": "Resolve the friction between dev and security teams with platform-led workflows that make cloud security seamless and scalable.",
      "date_published": "2024-11-12T17:00:00.000Z",
      "author": {
        "name": "Thomas O'Connell"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/hashicorp-at-aws-re-invent-your-blueprint-to-cloud-success",
      "url": "https://www.hashicorp.com/blog/hashicorp-at-aws-re-invent-your-blueprint-to-cloud-success",
      "title": "HashiCorp at AWS re:Invent: Your blueprint to cloud success",
      "content_html": "<p>Amazon Web Services’ flagship cloud conference — AWS re:Invent — is back, and this year  HashiCorp’s presence is bigger than ever. (You can still <a href=\"https://reinvent.awsevents.com/register/?trk=www.google.com\">register to attend in-person or remotely</a>.) For both in-person and remote attendees, we’re pleased to share the latest news on our long-standing relationship with Amazon and how we help organizations provision, secure, run, and connect applications running in AWS.</p>\n\n<p>In 2024, HashiCorp is an Emerald sponsor of re:Invent and we have a <a href=\"https://events.hashicorp.com/awsreinvent2024\">full program</a> of events where attendees can learn and network at the show. Here are some highlights: </p>\n\n<h2>Visit us at booth 1112</h2>\n\n<p>Stop by for demos and talks on Infrastructure and Security Lifecycle Management practices that help you provision, secure, connect, and run applications on AWS.</p>\n\n<ul>\n<li><strong>Product demos</strong>: Meet with technical experts and stop by our booth theater for hourly 15-minute demos on Terraform, Vault, Consul, and more.</li>\n<li><strong>Technical deep dives</strong>: Get your questions answered at whiteboard sessions about specifics across our infrastructure and security portfolio.</li>\n<li><strong>Limited-edition swag:</strong> Collect exclusive branded merch as you learn more about HashiCorp products and solutions.</li>\n<li><strong>Integration highlights:</strong> Learn more about how we integrate and innovate with AWS to boost developer productivity, mitigate risk, and maximize cloud investments. </li>\n<li><strong>Community zone:</strong> Attend whiteboard sessions on Infrastructure and Security Lifecycle Management.</li>\n</ul>\n<img src=https://www.datocms-assets.com/2885/1731008439-screenshot-2024-11-06-at-5-15-34-pm.png alt=re:invent map><h2>Find us on stage</h2>\n\n<p>HashiCorp will be presenting four breakout sessions at re:Invent. Sign up in the<a href=\"https://reinvent.awsevents.com/experience/session-content/?trk=8e7b011c-05fc-43d5-8fe0-4a96063d4b30\"> AWS Session Catalog</a> (AWS login required) for the sessions below: </p>\n\n<p><strong>Build for massive scale and security with the HashiCorp Cloud Platform</strong></p>\n\n<p>Tuesday | December 3, 11:30 a.m. - 12:30 p.m. | Room: Venetian | Summit Showroom</p>\n\n<ul>\n<li>In this session, explore how to build scalable, secure, and manageable cloud infrastructure while enabling efficient engineering workflows using the HashiCorp Cloud Platform (HCP). Discover how to use HCP for infrastructure and security in public cloud projects through deep conceptual and technical insights. The session includes a reference codebase and live demo. Learn foundational principles and patterns for Infrastructure and Security Lifecycle Management that address process and people challenges and help to up-skill your engineering team to support rapid business and platform evolution.</li>\n<li>Session ID: DOP101-S</li>\n</ul>\n\n<p><strong>Building multi-account multi-region applications with Terraform stacks</strong></p>\n\n<p>Wednesday | December 4, 10 - 11 a.m. | Room: Venetian | Murano 3305</p>\n\n<ul>\n<li>Efficiently managing complex application platforms remains a significant challenge for many organizations. In this session, discover how to streamline the provisioning and management of infrastructure across diverse environments and simplify complex multilayered deployments with new capabilities in HCP Terraform. Learn how to use Terraform to enhance your infrastructure management practices and increase overall operational efficiency.</li>\n<li>Session ID: DOP206-S</li>\n</ul>\n\n<p><strong>How to get dynamic with secrets management</strong></p>\n\n<p>Tuesday | December 3, 12:30 - 1:30 p.m. | Room: Wynn | Bollinger</p>\n\n<ul>\n<li>As organizations scale their applications and teams, adopting a service-oriented architecture (SOA) can improve scalability, flexibility, and agility. This session will walk through a journey from zero to scale, discussing when and how an organization should implement service-oriented architecture (SOA). Attendees will learn how to set up a multi-account, multi-runtime microservices architecture at scale using Amazon ECS, EKS, and EC2 along with HashiCorp Terraform, Consul, Boundary, and Vault.</li>\n<li>Session ID: SEC204-S</li>\n</ul>\n\n<p><strong>Infrastructure at the speed of AI</strong></p>\n\n<p>Monday | December 2, 5:30 - 6:30 p.m. | Room: MGM | Chairman&#39;s Ballroom 370</p>\n\n<ul>\n<li>This session covers HashiCorp’s role in building out infrastructure for AI and ML workloads on AWS. Explore how to secure that infrastructure and the data used as part of those models. Learn to implement a self-service, vending-machine-style approach for AI, including integrating tools like Amazon SageMaker and AWS Lambda. Discover how to simplify and scale AI operations, manage costs and risks, and use HCP Terraform for security and governance. Learn best practices for deploying and scaling AI infrastructure while ensuring robust security and efficient management on Day 2.</li>\n<li>Session ID: AIM102-S</li>\n</ul>\n\n<h2>Attend a 15-minute tech talk — every hour, on the hour, at the booth theater</h2>\n\n<p>The building blocks of Infrastructure Lifecycle Management</p>\n\n<ul>\n<li>11 a.m. | Tuesday - Thursday</li>\n</ul>\n\n<p>The building blocks of Security Lifecycle Management  </p>\n\n<ul>\n<li>12 p.m. | Tuesday - Thursday</li>\n</ul>\n\n<p>Keys to successful Security Lifecycle Management on AWS with HashiCorp</p>\n\n<ul>\n<li>1 p.m. | Tuesday</li>\n</ul>\n\n<p>Unified security lifecycle management with HCP                                           </p>\n\n<ul>\n<li>1 p.m. | Wednesday</li>\n</ul>\n\n<p>Simplify remote access management with Boundary</p>\n\n<ul>\n<li>1 p.m. | Thursday</li>\n</ul>\n\n<p>Build and manage complex application platforms with HashiCorp Terraform</p>\n\n<ul>\n<li>2 p.m. | Tuesday </li>\n</ul>\n\n<p>Pillars of scalable Infrastructure Lifecycle Management on AWS with HashiCorp</p>\n\n<ul>\n<li>2 p.m. | Wednesday</li>\n</ul>\n\n<p>Module lifecycle management (MLM) in HCP Terraform and HCP Waypoint templates   </p>\n\n<ul>\n<li>2 p.m. | Thursday</li>\n</ul>\n\n<p>Mitigate material risks through vulnerability management                                           </p>\n\n<ul>\n<li>3 p.m. | Tuesday </li>\n</ul>\n\n<p>Infrastructure at the speed of AI  </p>\n\n<ul>\n<li>3 p.m. | Wednesday</li>\n</ul>\n\n<h2>View joint sessions with AWS at the booth theater</h2>\n\n<p>Terraform policies co-developed by AWS and HashiCorp</p>\n\n<ul>\n<li>4 p.m. - 4:15 p.m. | Tuesday</li>\n</ul>\n\n<p>Fight secret sprawl with HCP Vault and AWS Secrets Manager</p>\n\n<ul>\n<li>4 p.m.- 4:15 p.m. | Wednesday</li>\n</ul>\n\n<p>Analyze Terraform using Agentic GenAI workflows</p>\n\n<ul>\n<li>3 p.m.- 3:15 p.m. | Thursday </li>\n</ul>\n\n<h2>Sign up for AWS and Terraform notable sessions</h2>\n\n<p>Check the <a href=\"https://reinvent.awsevents.com/session-catalog/?trk=direct\">AWS session catalog</a> for AWS-led talks and workshops featuring Terraform. Just search for “Terraform” in the <a href=\"https://registration.awsevents.com/flow/awsevents/reinvent24/public/page/catalog?search=terraform\">session catalog</a> search bar. Here are a few suggestions:  </p>\n\n<p>Breakout session | SVS320 | Accelerate serverless deployments using Terraform with proven patterns</p>\n\n<ul>\n<li>Monday | 12:00 PM - 1:00 PM | Venetian | Level 3 | Lido 3002 </li>\n</ul>\n\n<p>Chalk talk | DOP316 | Unleashing AWS agility with Terraform </p>\n\n<ul>\n<li>Monday | 3:00 PM - 4:00 PM PST | MGM Grand | Level 1 | Boulevard 169</li>\n<li>Repeat: Tuesday | 1:30 PM - 2:30 PM | MGM Grand | Level 1 | Boulevard 156</li>\n</ul>\n\n<p>Workshop | DOP326 | Terraform expertise: Accelerate AWS deployments with modular IaC &amp; AI </p>\n\n<ul>\n<li>Wednesday | 3:30 PM - 5:30 PM | MGM Grand | Level 3 | Premier 312</li>\n</ul>\n\n<p>Workshop | SVS337 | Building serverless applications using Terraform  </p>\n\n<ul>\n<li>Monday | 3:00 PM - 5:00 PM | Mandalay Bay | Level 2 South | Lagoon F</li>\n</ul>\n\n<h2>More ways to connect and learn</h2>\n\n<p>In celebration of 3 billion downloads of the Terraform AWS provider and more community milestones, we are hosting events for our customers and extended community. </p>\n<img src=https://www.datocms-assets.com/2885/1731007241-download-1.jpeg alt=CHICA Las Vegas><p>HashiCorp will host a breakfast for HashiCorp Certified product users and Ambassadors on Thursday, Dec. 5 at 7 - 8:30 a.m. at <a href=\"https://www.venetianlasvegas.com/restaurants/chica.html\">CHICA Las Vegas</a> to say thank you and network with our Terraform contributors and collaborators. Please share your <a href=\"https://ambassadorcertifiedbreakfast.splashthat.com/\">interest in attending</a> and note that you will need a conference pass to attend.</p>\n\n<p>To learn more, book a meeting with us, meet other customers, or chat with experts at AWS re:Invent, please visit <a href=\"https://events.hashicorp.com/awsreinvent2024\">https://events.hashicorp.com/awsreinvent2024</a>.</p>",
      "summary": "If you’re attending AWS re:Invent in Las Vegas, Dec. 2 - Dec. 6th, visit us for breakout sessions, expert talks, and product demos to learn how to take a unified approach to Infrastructure and Security Lifecycle Management.",
      "date_published": "2024-11-07T19:00:00.000Z",
      "author": {
        "name": "Mike Doheny"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/speed-up-app-delivery-with-automated-cancellation-of-plan-only-terraform-runs",
      "url": "https://www.hashicorp.com/blog/speed-up-app-delivery-with-automated-cancellation-of-plan-only-terraform-runs",
      "title": "Speed up app delivery with automated cancellation of plan-only Terraform runs",
      "content_html": "<p>Today, we’re excited to announce the general availability of a new feature that automatically cancels <a href=\"https://developer.hashicorp.com/terraform/enterprise/run/modes-and-options#plan-only-speculative-plan\">plan-only</a> Terraform runs triggered by pull requests in version control systems (VCS) for HCP Terraform and Terraform Enterprise. This enhancement helps customers avoid the backlog of multiple runs caused by new commits pushed to a branch, ultimately speeding up the application delivery process. </p>\n\n<p>When integrated with a VCS, HCP Terraform can automatically initiate a Terraform run to perform a plan operation whenever a pull request (PR) is created in the repository. However, when team members push new commits to the same branch, it can lead to a queue of Terraform runs, causing delays and inefficiencies. With the automatic cancellation of plan-only runs, you can now easily cancel any unfinished runs for outdated commits by selecting the option in your organization’s settings.</p>\n\n<p>This blog will show you how to set up this feature and see it in action.</p>\n\n<h2>Managing organization speculative plan settings</h2>\n\n<p>To view and manage an organization’s speculative plan settings, click Settings, followed by General under Version Control, then view the Manage speculative plans part of the page.</p>\n\n<p>Once the organization setting is enabled, HCP Terraform will cancel ongoing or pending speculative plans if new commits are received on the same branch.</p>\n<img src=https://www.datocms-assets.com/2885/1730933102-managing-organization-speculative-plan-settings.png alt=Toggle automatic speculative plan cancellation for outdated commits.><h2>Run details page</h2>\n\n<p>When a newer commit gets pushed to the same branch, users will see a message mentioning that the run was automatically canceled.</p>\n<img src=https://www.datocms-assets.com/2885/1730933136-run-details-page.png alt=Plan auto-cancellation visibility><h2>Updated VCS status checks</h2>\n\n<p>For non-aggregated status checks, Terraform now shows a different message for auto-canceled plan that differs from the normally canceled one (which would show “Terraform plan canceled”).</p>\n<img src=https://www.datocms-assets.com/2885/1730933297-updated-vcs-status-checks.png alt=VCS status feed><p>For aggregated status checks, Terraform now includes a count of automatically canceled runs in the aggregated result, separating it from the count of manually canceled runs.</p>\n<img src=https://www.datocms-assets.com/2885/1730933262-updated-vcs-status-checks-one-item.png alt=Individual cancelled run><p>Also, in the aggregated status page, Terraform now includes a message saying that the run has been auto-canceled using the same message that it would send for non-aggregated status checks.</p>\n<img src=https://www.datocms-assets.com/2885/1730933239-updated-vcs-status-checks-workspace-message.png alt=Run detailed view><h2>Aggregated status page</h2>\n\n<p>When more than one run is automatically canceled for a commit, Terraform will display an alert in the “Resources to be changed” section as a reminder that the section may not reflect a complete result if all the runs of the commit were to reach completion. You can toggle “Hide automatically canceled plans” in the check box shown below:</p>\n<img src=https://www.datocms-assets.com/2885/1730933206-aggregated-status-page-png.png alt=Aggregated status page><h2>Getting started</h2>\n\n<p>These enhancements to HCP Terraform and Terraform Enterprise reflect our ongoing commitment to helping customers maximize their infrastructure investments and speed up application delivery by optimizing plan-only Terraform runs managed through the VCS workflow. </p>\n\n<p>To learn more about these features, visit our page on <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/organizations/vcs-speculative-plan-management\">automatically canceling plan-only runs</a>. If you are new to Terraform, <a href=\"https://app.terraform.io/public/signup/account\">sign up for HCP Terraform</a> and get started for free today.</p>",
      "summary": "Automatic cancellation of plan-only runs allows customers to easily cancel any unfinished runs for outdated commits to speed up application delivery.",
      "date_published": "2024-11-06T17:00:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/enhancing-azure-deployments-with-azurerm-and-azapi-terraform-providers",
      "url": "https://www.hashicorp.com/blog/enhancing-azure-deployments-with-azurerm-and-azapi-terraform-providers",
      "title": "Enhancing Azure deployments with AzureRM and AzAPI Terraform providers",
      "content_html": "<p>In addition to the AzureRM Terraform provider, Microsoft maintains another Terraform provider for Azure services: AzAPI, <a href=\"https://techcommunity.microsoft.com/t5/azure-tools-blog/announcing-azapi-2-0/ba-p/4275733\">which recently reached version 2.0</a>.  With this new release, we collaborated with Microsoft to offer a comparison guide to help Terraform users decide which provider to use. This blog will look at the ideal scenarios for each provider with clear guidance, particularly those familiar with the AzureRM provider.</p>\n\n<h2>Comparing AzureRM and AzAPI Terraform providers</h2>\n\n<p>At a high level, AzureRM provides a stable, well-tested layer on top of Azure APIs. It handles the entire resource lifecycle — creation, updates, and deletion — while managing breaking changes, ensuring smooth operations. AzureRM is ideal for users looking for stability and simplified configuration management.</p>\n\n<p>On the other hand, AzAPI is a lightweight wrapper around Azure APIs, enabling direct and early access to the latest Azure features. It allows for quicker adoption of new services or workarounds for AzureRM limitations, making it ideal for users who need the latest Azure services and functionality as fast as possible. The sections below look deeper into what these differences mean for you.</p>\n\n<h2>AzureRM: A proven, simplified approach</h2>\n\n<p>AzureRM abstracts complexity by managing Azure API versions on your behalf. The provider ensures that resources are fully compatible with one another and that configuration changes don&#39;t introduce breaking issues, thanks to its rigorous testing. If you&#39;re using resources that don&#39;t require constant updates or access to the latest API versions, AzureRM provides a more stable and simplified experience.</p>\n\n<p>Key benefits of AzureRM:</p>\n\n<ul>\n<li><strong>Automatic API versioning:</strong> AzureRM handles API version compatibility, making upgrades seamless.</li>\n<li><strong>Simplicity:</strong> Resource property names are intuitive (e.g., <code>disk_size_in_gb</code> vs. <code>disk_size</code>), reducing the need to consult Azure API documentation frequently.</li>\n<li><strong>Comprehensive documentation:</strong> AzureRM offers extensive resources and examples for each service, making it easier to onboard and use in your projects.</li>\n</ul>\n\n<p>AzureRM is ideal for scenarios where you prioritize stability, want to minimize complexity, and don&#39;t need the very latest features.</p>\n\n<h2>AzAPI: Cutting-edge access to Azure APIs</h2>\n\n<p>AzAPI, by contrast, provides a thinner layer, allowing for direct access to the latest Azure API versions as soon as they&#39;re available. It&#39;s perfect for scenarios where you need quick access to preview features before they are fully supported in AzureRM.</p>\n\n<p>Key benefits of AzAPI:</p>\n\n<ul>\n<li><strong>Immediate API access:</strong> AzAPI gives users access to the latest API versions for Azure resources, allowing teams to use new Azure services and features sooner.</li>\n<li><strong>Targeted resource updates:</strong> With the <code>azapi_update_resource</code> function, you can modify specific resource properties without upgrading the entire resource or provider.</li>\n<li><strong>Fine-grained control:</strong> AzAPI&#39;s approach to resource versioning allows for more control over the infrastructure configuration, giving users the ability to choose API versions that best fit their needs.</li>\n</ul>\n\n<p>AzAPI is recommended for scenarios where early access to new Azure features is crucial, or when you need granular control over resource versions.</p>\n\n<h2>Documentation and community support</h2>\n\n<p>AzureRM has a more extensive collection of blog posts, community contributions, and official documentation. This makes it easier for new users to find examples and ramp up quickly.</p>\n\n<p>AzAPI, while newer, follows Azure&#39;s API structures more closely, making it easier for users familiar with Bicep or ARM templates to understand.</p>\n\n<h2>When to use each provider</h2>\n\n<p><strong>Choose AzureRM</strong> if you prioritize stability, simplicity, and automatic versioning. It&#39;s best for teams that want to minimize the complexity of managing infrastructure and don&#39;t need immediate access to new Azure features.</p>\n\n<p><strong>Choose AzAPI</strong> if you need cutting-edge access to the latest Azure APIs or need to customize resource configurations without waiting for AzureRM to be updated. It&#39;s ideal for teams that require rapid innovation and fine-grained control over API versions.</p>\n\n<p>Both providers provide a first-class experience, backed by Microsoft and HashiCorp, and can be adapted based on your needs. You can also transition between them seamlessly with tools like the upcoming Azure Terraform Migration tool release (<a href=\"https://github.com/Azure/aztfmigrate\">aztfmigrate</a>), making it easy to adjust your approach as your infrastructure evolves.</p>\n\n<p>We hope this guide along with Microsoft&#39;s guide to <a href=\"https://aka.ms/tf/providermessaging\">Unlocking the Best of Azure with AzureRM and AzAPI Providers</a> helps you determine when to use AzureRM versus AzAPI, ensuring you get the most out of your Terraform and Azure infrastructure.</p>",
      "summary": "This blog compares the AzureRM and AzAPI Terraform providers, offering insights on when to use each for optimal Azure infrastructure management.",
      "date_published": "2024-10-30T20:00:00.000Z",
      "author": {
        "name": "Mike Doheny"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-stacks-explained",
      "url": "https://www.hashicorp.com/blog/terraform-stacks-explained",
      "title": "Terraform Stacks, explained",
      "content_html": "<p><em>This post was updated in October 2024.</em></p>\n\n<p>Terraform Stacks are a feature intended to simplify infrastructure provisioning and management at scale, providing a built-in way to scale without complexity. In this blog, we would like to provide more details about our vision, where we are right now, and where we are going next. </p>\n\n<h2>What challenges do Terraform Stacks solve?</h2>\n\n<p>There are a number of benefits to using small modules and workspaces to build a composable infrastructure. Splitting up your Terraform code into manageable pieces helps: </p>\n\n<ul>\n<li>Limit the blast radius of resource changes</li>\n<li>Reduce run time</li>\n<li>Separate management responsibilities across team boundaries</li>\n<li>Work around multi-step use cases such as provisioning a Kubernetes cluster</li>\n</ul>\n\n<p>Terraform’s ability to take code, build a graph of dependencies, and turn it into infrastructure is extremely powerful. However, once you split your infrastructure across multiple Terraform configurations, the isolation between states means you must stitch together and manage dependencies yourself.</p>\n\n<p>Additionally, when deploying and managing infrastructure at scale, teams usually need to provision the same infrastructure multiple times with different input values, across multiple: </p>\n\n<ul>\n<li>Cloud provider accounts</li>\n<li>Environments (dev, staging, production)</li>\n<li>Regions</li>\n<li>Landing zones</li>\n</ul>\n\n<p>Before Terraform Stacks, there was no built-in way to provision and manage the lifecycle of these instances as a single unit in Terraform, making it difficult to manage each infrastructure root module individually.</p>\n\n<p>We knew these challenges could be solved in a better and more valuable way than just wrapping Terraform with bespoke scripting and external tooling, which requires heavy lifting and is error-prone and risky to set up and manage.</p>\n\n<h2>What are Terraform Stacks and what are their benefits?</h2>\n\n<p>Stacks help users automate and optimize the coordination, deployment, and lifecycle management of interdependent Terraform configurations, reducing the time and overhead of managing infrastructure. Key benefits include:</p>\n\n<ul>\n<li><strong>Simplified management</strong>: Stacks eliminate the need to manually track and manage cross-configuration dependencies. Multiple Terraform modules sharing the same lifecycle can be organized and deployed together using components in a Stack.</li>\n<li><strong>Improved productivity</strong>: Stacks empower users to rapidly create and modify consistent infrastructure setups with differing inputs, all with one simple action. Users can leverage deployments in a Stack to effortlessly repeat their infrastructure and can set up orchestration rules to automate the rollout of changes across these repeated infrastructure instances.</li>\n</ul>\n\n<p>Stacks aim to be a natural next step in extending infrastructure as code to a higher layer using the same Terraform shared modules users enjoy today.</p>\n\n<h2>Common use cases for Terraform Stacks</h2>\n\n<p>Here are the common use cases for Stacks, out of the box:</p>\n\n<ul>\n<li><strong>Deploy an entire application with components like networking, storage, and compute as a single unit without worrying about dependencies</strong>. A Stack configuration describes a full unit of infrastructure as code and can be handed to users who don’t have advanced Terraform experience, allowing them to easily stand up a complex infrastructure deployment with a single action.</li>\n<li><strong>Deploy across multiple regions, availability zones, and cloud provider accounts without duplicating effort/code.</strong> <em>Deployments</em> in a Stack let you define multiple instances of the same configuration without needing to copy and paste configurations, or manage configurations separately. When a change is made to the Stack configuration, it can be rolled out across all, some, or none of the deployments in a Stack.</li>\n<li><strong>Provision and manage Kubernetes workloads.</strong> Stacks streamline the provisioning and management of Kubernetes workloads by allowing customers to deploy Kubernetes in one single configuration instead of managing multiple, independent Terraform configurations. We see Kubernetes deployments that often have this challenge where there are too many unknown variables to properly complete a plan. With Stacks, customers can drive a faster time-to-market with Kubernetes deployments at scale without going through a layered approach that is hard to complete within Terraform.</li>\n</ul>\n\n<h2>How do I use a Terraform Stack?</h2>\n\n<p>Stacks introduce a new configuration layer that sits on top of Terraform modules and is written as code.</p>\n\n<h3>Components</h3>\n\n<p>The first part of this configuration layer, declared with a <code>.tfstack.hcl</code> file extension, tells Terraform what infrastructure, or components, should be part of the Stack. You can compose and deploy multiple modules that share a lifecycle together using what are called <em>components</em> in a Stack. Add a component block to the <code>components.tfstack.hcl</code> configuration for every module you&#39;d like to include in the Stack. Specify the source module, inputs, and providers for each component.</p>\n<pre><code>component \"cluster\" {\n  source = \"./eks\"\n  inputs = {\n    aws_region          = var.aws_region\n    cluster_name_prefix = var.prefix\n    instance_type       = \"t2.medium\"\n  }\n  providers = {\n    aws       = provider.aws.this\n    random    = provider.random.this\n    tls       = provider.tls.this\n    cloudinit = provider.cloudinit.this\n  }\n}</code></pre><p>You don’t need to rewrite any modules since components can simply leverage your existing ones.</p>\n\n<h3>Deployments</h3>\n\n<p>The second part of this configuration layer, which uses a <code>.tfdeploy.hcl</code> file extension, tells Terraform where and how many times to deploy the infrastructure in the Stack. For each instance of the infrastructure, you add a <em>deployment</em> block with the appropriate input values and Terraform will take care of repeating that infrastructure for you. </p>\n<pre><code>deployment \"west-coast\" {\n  inputs = {\n    aws_region     = \"us-west-1\"\n    instance_count = 2\n  }\n}\n\ndeployment \"east-coast\" {\n  inputs = {\n    aws_region     = \"us-east-1\"\n    instance_count = 1\n  }\n}</code></pre><p>When a new version of the Stack configuration is available, plans are initiated for each deployment in the Stack. Once the plan is complete, you can approve the change in all, some, or none of the deployments in the Stack. </p>\n<img src=https://www.datocms-assets.com/2885/1700604740-tf-stacks-k8s-example.png alt=Example: The Kubernetes and namespace components are repeated across three regions using three deployments.><h3>Orchestration rules</h3>\n\n<p>Defined in HCL, Orchestration rules allow customers to automate repetitive actions in Stacks. At the launch of the public beta, users can auto-approve a plan when certain orchestration checks and criteria are met. For example, the following orchestrate block automatically approves deployments if there are no resources being removed in the plan.</p>\n<pre><code>orchestrate \"auto_approve\" “safe_plans” {\n  check {\n    #check that there are no resources being removed\n    condition = context.plan.changes.remove == 0\n    reason = \"Plan has ${context,plan.changes. remove} resources to be removed.\"\n  }\n}\n</code></pre><p>HCP Terraform evaluates the check blocks within your orchestrate block to determine if it should approve a plan. If all of the checks pass, then HCP Terraform approves the plan for you. If one or more conditions do not pass, then HCP Terraform shows the reason why, and you must manually approve that plan. This simplifies the management of large numbers of deployments by codifying orchestration checks that are aware of plan context in the Terraform workflow.</p>\n\n<h3>Deferred changes</h3>\n\n<p>This is a feature of Stacks that allows Terraform to produce a partial plan when it encounters too many unknown values — without halting the operations. This helps users work through these situations more easily, accelerating the deployment of specific workloads with Terraform. Deferred changes allow users to enable the <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/stacks-eks-deferred\">Kubernetes use case</a> mentioned earlier.</p>\n<img src=https://www.datocms-assets.com/2885/1728680402-deferredchanges.png alt=Deferred changes example><p>Consider an example of deploying three Kubernetes clusters, each with one or more namespaces, into three different geographies. In a Stack, you would use one component to reference a module for deploying the Kubernetes cluster and another component for a module that creates a namespace in it. In order to repeat this Kubernetes cluster across three geographies, you would simply define a deployment for each geography and pass in the appropriate inputs for each, such as region identifiers.</p>\n\n<p>If you decided to add a new namespace to each of your Kubernetes clusters, it would result in plans queued across all three geographies. To test this change before propagating it to multiple geographies, you could add the namespace to the US geo first. After validating everything worked as expected, you could approve the change in the Europe geo next. You have the option to save the plan in the Asia geo for later. Having changes that are not applied in one or more deployments does not prevent new changes that are made to the Stack from being planned.</p>\n\n<p>See how Kubernetes clusters are deployed in Terraform Stacks by watching this video:</p>\n<iframe width=\"560\" height=\"315\" src=https://www.youtube.com/watch?v=UPQowCwlnWc frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><h2>What’s next for Terraform Stacks?</h2>\n\n<p>At HashiConf 2024, we announced the HCP Terraform <a href=\"https://www.hashicorp.com/blog/terraform-packer-nomad-and-waypoint-updates-help-scale-ilm-at-hashiconf-2024\">public beta of Stacks</a>. During the public beta, users can experiment with Stacks to provision and manage up to 500 resources for free, including the new Kubernetes use case and the two features mentioned earlier: deferred changes and orchestration rules. Once users reach the limit, they will enter a degraded mode that allows 0 Stack applies. Stack plans can still proceed, but only Stack plans to destroy resources can be applied until the RUM count is reduced to a number under 500. Go to HashiCorp Developer to learn how to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/stacks/create#create-a-stack\">create a Stack in HCP Terraform</a>.</p>\n\n<p>While our public beta is limited to HCP Terraform plans based on resources under management (RUM), certain Stacks functionality will be incorporated in upcoming releases of the community edition of Terraform. Workspaces will continue to have their use cases and Terraform will continue to work with both workspaces and Stacks.</p>\n\n<p>We hope you’re as excited about Stacks as we are, and appreciate your support as we transform how organizations use Terraform to further simplify infrastructure provisioning and management at scale. </p>",
      "summary": "Terraform Stacks simplify provisioning and managing resources at scale, reducing the time and overhead of managing infrastructure.",
      "date_published": "2024-10-15T12:30:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/dont-leave-cloud-security-to-chance-seven-mistakes",
      "url": "https://www.hashicorp.com/blog/dont-leave-cloud-security-to-chance-seven-mistakes",
      "title": "Don’t leave cloud security to chance: 7 mistakes and how to avoid them",
      "content_html": "<p>According to Evanta, a Gartner company,<a href=\"https://www.evanta.com/resources/cio/infographic/2024-cio-leadership-perspectives#:%7E:text=For%2520the%2520third%2520year%2520in,five%2520CIO%2520priorities%2520for%25202024.\"> cybersecurity is CIOs’ top priority</a>. Yet the cloud resources that enterprise teams are briskly provisioning to deliver on their CIO’s other immediate priorities — data and analytics, AI/machine learning, digital business priorities and applications — could, in fact, be introducing systemic vulnerabilities that increase cybersecurity risk. Every resource provisioned across public, private, and hybrid cloud networks is a roll of the metaphorical dice, and can leave enterprises susceptible to ransomware and malware, data breaches, insider threats, DDoS attacks, API vulnerabilities, and more. </p>\n\n<p>This blog presents 7 common mistakes development teams make when provisioning and managing cloud resources over time, and how to avoid them through effective Infrastructure Lifecycle Management (ILM). By adopting tools and techniques to standardize the approach to infrastructure enterprise-wide, CIOs and IT organizations can do more than ease current cyber risks; they can move on to <a href=\"https://www.hashicorp.com/lp/cloud-maturity\">higher levels of cloud maturity</a> that focus on proactive cloud data security, faster innovation, and finding more efficiencies, and reducing the often-surprising costs of cloud computing.</p>\n<img src=https://www.datocms-assets.com/2885/1728657858-gartner_stats.png alt=Top Priorities within the CIO Function><h2>1. Manual processes</h2>\n\n<p><em>**Solution:</em>* Use an infrastructure as code (IaC) solution for infrastructure automation*</p>\n\n<p>Organizations relatively new to the cloud, and/or rapidly provisioning resources across the enterprise, typically use an <em>ad hoc</em> approach. Teams will each pick individual solutions and do what works in the moment. Sometimes that results in teams still using legacy methods of infrastructure management, either manually provisioning infrastructure through vendor consoles (often called “ClickOps”) or writing one-off scripts that have limited reusability and visibility, creating a brittle provisioning process.</p>\n\n<p>Successful cloud provisioning starts with a systematic and repeatable approach that can be used by all teams. Many organizations use<a href=\"https://www.hashicorp.com/resources/what-is-infrastructure-as-code\"> infrastructure as code</a> (IaC) to codify cloud infrastructure and the underlying system images. Once codified, infrastructure can be easily versioned, tracked, reused, and automatically provisioned across multiple cloud environments, eliminating the need for unscalable manual approaches. </p>\n\n<h2>2. Siloed teams</h2>\n\n<p><em>**Solution:</em>* Enable and promote cross-functional collaboration with a platform-oriented approach*</p>\n\n<p>Organizations typically start with a fragmented approach to cloud migration, letting teams build their own unique, non-standard solutions. Teams are reinventing the solutions for the same problems while also failing to share best practices. The adequate security management of so many fragmented solutions is also impossible and cost prohibitive. The resulting costs and cyber risks in this ‘wild west’ scenario quickly spiral out of control. </p>\n\n<p>The first step in implementing provisioning best practices is to break down organizational silos with access to a <a href=\"https://developer.hashicorp.com/terraform/registry/private\">central library</a> of shared infrastructure configurations, composed of proven code written by experts. In doing so, teams can avoid reinventing well-established configurations — and avoid the many risks that inconsistency invites.</p>\n\n<p>IT leaders also need to put into place a common platform to access, review, and version infrastructure. IaC workflows<a href=\"https://developer.hashicorp.com/terraform/cloud-docs/vcs\"> can be integrated</a> with a version control system (VCS) such as GitHub or GitLab, giving teams a common code base. This promotes the reuse of best practices and patterns, boosts productivity, increases visibility, and sets the foundation for infrastructure as a shared service across the entire organization.</p>\n\n<p>Finally, as teams begin collaborating, role-based access control (RBAC) can help effectively manage team <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/permissions\">permissions</a> and facilitate security best practices including <a href=\"https://www.paloaltonetworks.com/cyberpedia/what-is-the-principle-of-least-privilege\">least-privilege access</a>. </p>\n\n<h2>3. Inconsistent deployments</h2>\n\n<p><em>**Solution:</em>* Standardize cloud provisioning with “golden” best practice templates*</p>\n\n<p>Without any sorts of templates or consistent provisioning and CI/CD pipelines, teams are still doing manual operations and creating too many unique infrastructure setups. Inconsistency is at the root of many security vulnerabilities, and it’s common across large enterprises. </p>\n\n<p>The solution to inconsistent cloud infrastructure provisioning is a “golden path”. In platform engineering, platform teams create golden images, modules, pipelines, orchestration jobs, and any other software delivery component templates that serve as “golden” standards that can be reused by development teams to automatically follow best practices with every deployment. </p>\n\n<p>To build this set of golden templates,  platform teams should create, test, and validate reusable infrastructure modules and images, and then make them easily discoverable throughout the organization. With the proper provisioning platform, platforms should have the ability to:</p>\n\n<ul>\n<li><strong>Create infrastructure templates</strong>: Build infrastructure as code modules and images</li>\n<li><strong>Test and validate the code</strong>: Build tests that can<a href=\"https://developer.hashicorp.com/terraform/tutorials/configuration-language/test\"> validate the functionality</a> of IaC configurations in a safe environment by running tests against specific, short-lived resources.</li>\n<li><strong>Make templates discoverable and manageable</strong>: Once modules are ready for use, they can be published in an<a href=\"https://developer.hashicorp.com/terraform/registry/private\"> internal private registry</a>.<a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/explorer\"> From there, the platform team needs visibility tools</a> and full module lifecycle management capabilities so that they can see template usage patterns, get versioning information, organize and tag templates, and revoke or deprecate templates when a new replacement is created.</li>\n</ul>\n\n<h2>4. No embedded guardrails</h2>\n\n<p><em>**Solution:</em>* Use policy as code to meet risk and cost requirements*</p>\n\n<p>Rapid provisioning opens up tremendous possibilities for innovation, but without effective guardrails in place, it’s a nightmare for security and finance teams. Security, compliance, and cost policies have typically required manual validation and enforcement through a ticket-ops system.  This often meant days or weeks-long bottlenecks as developers and IT waited for their infrastructure changes to be approved.</p>\n\n<p>Like infrastructure as code, <a href=\"https://developer.hashicorp.com/sentinel/docs/concepts/policy-as-code\">policy as code</a> can be used to reduce manual errors, enable greater scale through automation, and accelerate productivity. Cloud provisioning solutions that include policy as code can help users define custom policies that are<a href=\"https://www.terraform.io/use-cases/enforce-policy-as-code\"> automatically enforced</a> in the provisioning workflow. </p>\n\n<p>For example, policies can: </p>\n\n<ul>\n<li>Check if end users are consuming approved modules rather than creating custom code</li>\n<li>Ensure the infrastructure is tagged for visibility</li>\n<li>Confirm that storage buckets are encrypted and not publicly accessible</li>\n<li>And much more — the possibilities are numerous</li>\n</ul>\n\n<p>Platform teams can create and organize policy sets in the same way they would manage infrastructure modules, with a<a href=\"https://registry.terraform.io/browse/policies\"> library</a> of trusted pre-written policy sets that can enforce best practices. The best policy engines can even integrate third-party tools from various vendors to enact additional checks.</p>\n\n<h2>5. Insufficient monitoring capabilities</h2>\n\n<p><em>**Solution:</em>* Monitor infrastructure drift and health over time*</p>\n\n<p>Once cloud resources are up and running, teams need to make sure their infrastructure remains performant and healthy. Failure to do so can result in costly outages or security problems due to misconfigurations. Even with a standardized provisioning process and policy guardrails in place, <a href=\"https://www.hashicorp.com/resources/how-can-i-prevent-configuration-drift\">configuration drift</a> can occur, creating vulnerabilities or bugs. </p>\n\n<p>Effective ILM incorporates a<a href=\"https://www.hashicorp.com/solutions/cloud-system-of-record\"> system of record</a> to provide visibility and monitoring capabilities, and remediate issues as they arise. A cloud infrastructure management solution with<a href=\"https://developer.hashicorp.com/terraform/tutorials/state/resource-drift\"> drift detection</a> capabilities surfaces problems to admins as they develop. In addition, continuous monitoring provides<a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/health\"> health checks</a> to verify that cloud workspaces perform as planned, over time.</p>\n\n<h2>6. No lifecycle management</h2>\n\n<p><em>**Solution:</em>* Automate infrastructure deletion*</p>\n\n<p>Organizations scaling their cloud program often accumulate abandoned cloud instances and unnecessary infrastructure that continues to run, wasting money. These forgotten resources can also open security holes if they haven’t received security updates in a while. </p>\n\n<p>Setting end-of-life dates and<a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/settings/deletion#automatically-destroy\"> automating infrastructure destruction</a> helps to eliminate unnecessary cloud waste and close security gaps. Ideally, resource deletion can be triggered when a predefined date or inactivity period is reached, with<a href=\"https://developer.hashicorp.com/packer/tutorials/hcp/hcp-schedule-image-iterations-revocation\"> artifact revocation</a> scheduled at the image level. Users should be<a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/settings/notifications\"> automatically notified</a> of impending deprecation actions and receive follow-up actions to confirm deletion.</p>\n\n<h2>7. Lack of self-service</h2>\n\n<p><em>**Solution:</em>* Scale efficiently by giving developers freedom and autonomy within the golden path*</p>\n\n<p>As organizations ramp up their consumption of cloud resources, an automated self-service provisioning workflow empowers developers to quickly deploy the resources they need without slow, demotivating ticket-based workflows that require many manual approvals.</p>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/no-code-provisioning\">No-code provisioning</a> can make self-service even faster while preserving all guardrails, especially when it’s integrated with popular self-service platforms such as<a href=\"https://developer.hashicorp.com/terraform/tutorials/it-saas/servicenow-no-code\"> ServiceNow</a> and AWS Service Catalog, or GitHub Actions. Platform teams can also set up an<a href=\"https://developer.hashicorp.com/waypoint\"> internal development platform</a> (IDP) to expand self-service, further abstracting the application deployment workflow to the point where developers don’t need to manually trigger a ton of tools at runtime. </p>\n\n<p>Ultimately, the goal of self-service is to give developers a set of golden workflows so they can focus on the application lifecycle while platform teams own the underlying infrastructure and security teams have ownership over designing the guardrails. In this way, innovation can occur freely while automatically mitigating cybersecurity risk and keeping costs down. </p>\n\n<h2>Achieving ILM maturity</h2>\n\n<p>Avoiding the most common mistakes in cloud infrastructure management can immediately improve an organization’s cybersecurity posture by preventing vulnerabilities from ever being created. It can also speed up innovation by providing developers with the cloud resources they need faster. By following the 7 solution steps described in this blog, organizations can advance their ILM maturity, setting up a virtuous cycle of continuous improvement by consistently upgrading and sharing new golden templates and workflows.</p>\n<img src=https://www.datocms-assets.com/2885/1728658203-infrastructure_stages.png alt=Stages of Adoption><p>To learn more about how ILM maturity can help CIOs achieve all of their <em>Top 5 priorities for 2024</em> and beyond, get your copy of the HashiCorp white paper, “<a href=\"https://www.hashicorp.com/on-demand/infrastructure-lifecycle-management-with-the-hashicorp-cloud-platform-email\">ILM with the HashiCorp Platform</a>,” and follow HashiCorp on <a href=\"https://www.linkedin.com/company/hashicorp/posts/?feedView=all\">LinkedIn</a>. </p>",
      "summary": "Learn how to avoid 7 common cloud security mistakes and reduce risk through Infrastructure Lifecycle Management best practices.",
      "date_published": "2024-10-15T12:30:00.000Z",
      "author": {
        "name": "Mitchell Ross"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-packer-nomad-and-waypoint-updates-help-scale-ilm-at-hashiconf-2024",
      "url": "https://www.hashicorp.com/blog/terraform-packer-nomad-and-waypoint-updates-help-scale-ilm-at-hashiconf-2024",
      "title": "Terraform, Packer, Nomad, and Waypoint updates help scale ILM at HashiConf 2024",
      "content_html": "<p>Today at HashiConf in Boston, we are pleased to announce our latest capabilities across our <a href=\"https://www.hashicorp.com/infrastructure-cloud/infrastructure-lifecycle-management\">Infrastructure Lifecycle Management (ILM)</a> portfolio, including HashiCorp Terraform, Packer, Nomad, and Waypoint, to help customers build, deploy, and manage infrastructure at scale. </p>\n\n<p>Our latest ILM capabilities help organizations manage infrastructure across Day 0, 1, and 2+.:</p>\n\n<ul>\n<li><strong>Day 0</strong>\n\n<ul>\n<li><strong>HCP Packer CI/CD pipeline metadata (GA)</strong> to track critical CI/CD information in build pipelines through integrations with GitHub and GitLab</li>\n<li><strong>HCP Packer bucket-level RBAC (GA)</strong> to gain further control over image permissions management</li>\n</ul></li>\n<li><strong>Day 1</strong>\n\n<ul>\n<li><strong>HCP Terraform Stacks (public beta)</strong> to simplify infrastructure management at scale</li>\n</ul></li>\n<li><strong>Day 2+</strong>\n\n<ul>\n<li><strong>HCP Terraform module lifecycle management (public beta)</strong> to reduce the overhead of module management</li>\n<li><strong>Terraform migrate (public beta)</strong> to accelerate migration from the community edition to HCP Terraform and Terraform Enterprise</li>\n<li><strong>HCP Waypoint</strong> (GA) with templates (GA) and add-ons (GA), now with API support and an upgrade workflow for templates</li>\n<li><strong>Nomad enhanced GPU support (GA)</strong></li>\n</ul></li>\n</ul>\n\n<p>This blog looks at how each of these new features contribute to speeding, securing, and simplifying the full lifecycle management of infrastructure.</p>\n\n<h2>Day 0: Build securely with infrastructure as code</h2>\n\n<p>As organizations plan and define the requirements of their services, Day 0 is the time to lay a strong foundation for ILM. Organizations need a programmatic approach to defining and provisioning application environments quickly and securely. They must prevent vulnerabilities in the software supply chain, including their base images and build artifacts, and ensure users have appropriate access based on their roles to mitigate security risks.</p>\n\n<h3>HCP Packer improves metadata visibility and access control</h3>\n\n<p>HCP Packer recently added <a href=\"https://developer.hashicorp.com/hcp/docs/packer/reference/build-pipeline-metadata?ajs_aid=b819fe94-ab52-42b0-8dc3-183a8dacd6e9&product_intent=packer&utm_source=google&utm_medium=sem&utm_offer=signup&utm_campaign=CLOUD_AMER_USA_ENG_BOFU_PRACTITIONER_SEM_A_ALL_TERRAFORM_CLD_GG_BRAND_-_Obility&utm_content=hashicorp+terraform-147137975932-625396954924&utm_channel_bucket=paid\">CI/CD pipeline metadata</a> views that give users even more visibility into artifact creation by letting them track critical CI/CD information such as pipeline IDs, job names, details on the operating system, VCS commits, and more. This addition grants HCP Packer level 1 SLSA compliance by providing a basic level of source code identification that can help organizations make risk-based security decisions. With this visibility, organizations can address risks earlier in the infrastructure deployment process.</p>\n\n<p>Another key addition to HCP Packer is <a href=\"https://developer.hashicorp.com/hcp/docs/packer/reference/permissions?ajs_aid=b819fe94-ab52-42b0-8dc3-183a8dacd6e9&product_intent=packer&utm_source=google&utm_medium=sem&utm_offer=signup&utm_campaign=CLOUD_AMER_USA_ENG_BOFU_PRACTITIONER_SEM_A_ALL_TERRAFORM_CLD_GG_BRAND_-_Obility&utm_content=hashicorp+terraform-147137975932-625396954924&utm_channel_bucket=paid\">bucket-level RBAC</a>, which helps admins define user access at the bucket level. This increased access granularity lets developers create buckets within the same project that they can access, while still being walled off from full-project access when they don’t need it. Specific permission can be assigned at the bucket level for actions such as creating, updating, and deleting artifact versions and more. With this improvement, organizations can now ensure sensitive <a href=\"https://developer.hashicorp.com/packer/tutorials/cloud-production/golden-image-with-hcp-packer?ajs_aid=be7881e6-b440-478c-b915-210e5a9895b9\">golden images</a> remain protected from unauthorized modifications while giving developers the self-service capabilities they need to be agile and efficient.</p>\n\n<h2>Day 1: Deploy infrastructure at scale without complexity</h2>\n\n<p>On Day 1, when developers are ready to provision the infrastructure needed to deploy an application, they want to scale quickly to meet business needs without complexity. They don’t want to waste valuable time repeating complex workarounds or repetitive manual processes.</p>\n\n<h3>HCP Terraform Stacks provide a built-in way to scale</h3>\n\n<p>Last October, we announced the private preview of <a href=\"https://www.hashicorp.com/blog/terraform-stacks-explained\">HCP Terraform Stacks</a>, a new way to simplify infrastructure provisioning and management at scale, reducing the time and overhead of managing infrastructure. Stacks empower users to rapidly create and modify consistent infrastructure setups with differing inputs, all with one simple action. Stacks also eliminate the need to manually track and manage cross-configuration dependencies as multiple Terraform modules can be organized and deployed together in a Stack.</p>\n\n<p>Today, we’re excited to announce the public beta of Terraform Stacks for all <a href=\"https://www.hashicorp.com/blog/terraform-cloud-updates-plans-with-an-enhanced-free-tier-and-more-flexibility\">new HCP Terraform plans</a> based on <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/overview/estimate-hcp-terraform-cost#what-is-a-managed-resource\">resources under management (RUM)</a>. During the public beta, HCP Terraform users can experiment with Stacks to provision and manage up to 500 resources for free, including <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/stacks-eks-deferred\">a new Kubernetes use case</a> and two new features: deferred changes and orchestration rules. Go to HashiCorp Developer to learn how to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/stacks/create#create-a-stack\">create a Stack in HCP Terraform</a>.</p>\n\n<p>The <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/stacks-eks-deferred\">new Kubernetes use case</a> streamlines the provisioning and management of Kubernetes workloads by allowing customers to deploy Kubernetes in one single configuration instead of managing multiple, independent Terraform configurations. We see Kubernetes deployments that often have this challenge where there are too many unknown variables to properly complete a plan. With Stacks, customers can drive a faster time-to-market with Kubernetes deployments at scale without going through a layered approach that is hard to complete within Terraform. </p>\n\n<p>The reason we can enable the Kubernetes use case hinges on a new feature: <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/stacks/deploy/plans#deferred-changes\">deferred changes</a>. This feature allows Terraform to produce a partial plan when it encounters too many unknown values — without halting operations. This helps users work through unknown-value situations more easily, accelerating the deployment of certain workloads with Terraform, most notably Kubernetes.</p>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/language/stacks/deploy/conditions\">Orchestration rules</a>, defined in HCL, allow customers to automate repetitive actions. For example, at the launch of the public beta, users can auto-approve a plan when certain orchestration checks and criteria are met. This simplifies the management of large numbers of deployments by codifying orchestration checks that are aware of plan context in the Terraform workflow. </p>\n\n<p>To learn more, read our updated blog <a href=\"https://www.hashicorp.com/blog/terraform-stacks-explained\">Terraform Stacks, explained</a>, refer to our <a href=\"https://developer.hashicorp.com/terraform/language/stacks/\">Stacks documentation</a>, and get hands-on experience in our <a href=\"https://developer.hashicorp.com//terraform/tutorials/cloud/stacks-deploy\">Terraform Stacks tutorials</a>.</p>\n<iframe width=\"560\" height=\"315\" src=https://www.youtube.com/watch?v=UPQowCwlnWc frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><p><em>*Stacks orchestration rules will be available to all HCP Terraform RUM plans during public beta</em></p>\n\n<h2><strong>Day 2+: Manage and optimize infrastructure operations continuously</strong></h2>\n\n<p>After deployment, on Day 2 and beyond, organizations need to manage their environments and optimize their operations continuously. End-of-life clean-up is a key part of that story, whether it&#39;s for Terraform workspaces or modules. Ephemeral workspaces have continued to advance their utility for resource clean-up, with the <a href=\"https://www.hashicorp.com/blog/hcp-terraform-adds-run-queue-visibility-and-new-ephemeral-workspace-features\">recent project-scoped auto-destroy setting enhancement</a>. Terraform also provides excellent tools for managing the creation and organization of golden modules, but visibility and end-of-life operations are also important to consider here as well. And when teams are ready to scale golden patterns and workflows, they’ll need tools that help them build an internal developer platform (IDP) to make infrastructure easily accessible for developers at any skill level.</p>\n\n<h3>HCP Terraform module lifecycle management provides comprehensive module visibility and controls</h3>\n\n<p>The HCP Terraform private registry makes it easy to publish and discover modules internally, but doesn’t fully address the end-of-life states of the module lifecycle. As a consequence, deprecating outdated versions and controlling their distribution could become difficult, especially with large quantities.</p>\n\n<p>From a management perspective, without a native workflow to provide visibility, like usage reports, and communication mechanisms with the right module consumers, organizations struggle to know how much any particular module version is being used, and who to ask for upgrades. From a security and compliance perspective, without a proper way to signal the deprecation of outdated modules, organizations are at risk of using obsolete and out-of-compliance configurations. </p>\n\n<p>Today, we are introducing module lifecycle management improvements in public beta to provide a systematic way to provide visibility, improve communication, and gain control throughout the module lifecycle. These improvements will simplify the complexity of module version management and reduce its overhead while also reducing security and compliance risks. </p>\n\n<p>To take advantage of new module lifecycle management features, the platform team can use <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/change-requests\">change requests</a> in the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/explorer\">HCP Terraform explorer</a>, to communicate infrastructure lifecycle events such as:</p>\n\n<ul>\n<li>Module deprecation</li>\n<li>Drift remediation</li>\n<li>Provider upgrades</li>\n<li>Infrastructure changes</li>\n</ul>\n\n<p>For example, platform teams can use <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/registry/manage-module-versions\">module deprecation</a> in the private registry to provide customized warnings about outdated module versions without interruption.</p>\n<img src=https://www.datocms-assets.com/2885/1728680835-deprecated-modules.png alt=Deprecated modules><p>Combined with <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/teams/notifications\">team notifications</a> in Terraform’s teams settings, which help configure a destination per-team communication channel, the requests always get to the right owners proactively in addition to showing up in the HCP Terraform workspace UI. Then users can use <em>saved views</em> in HCP Terraform explorer to track the progress of change requests for follow-up.</p>\n\n<p>To learn more about each feature, refer to our documentation on <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/registry/manage-module-versions\">module deprecation</a>, <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/change-requests\">change requests</a>, and <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/teams/notifications\">team notifications</a>. Change requests, team notifications, and module deprecation are only available in the HCP Terraform Plus tier. Saved views are available for all Terraform plans based on RUM.</p>\n<iframe width=\"560\" height=\"315\" src=https://www.youtube.com/watch?v=-dmzfQndOtk frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><h3>Terraform migrate accelerates migration from the community edition</h3>\n\n<p>Some customers are interested in trying out HCP Terraform or Terraform Enterprise, but find the migration process from Terraform Community Edition manual, time-consuming, and daunting. This slows the time-to-value for teams that want to migrate and causes friction for organizations adopting a commercial edition of Terraform.</p>\n\n<p>To help simplify and accelerate migrations from Terraform Community Edition to HCP Terraform or Terraform Enterprise, we’ve released <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/migrate/tf-migrate\">Terraform migrate</a> in public beta. Terraform migrate automates the tedious process of migrating workflows at scale in a way that is aligned with our best practices: HashiCorp Validated Designs. The Terraform migrate utility also reduces the risks of mistakes with a consistent migration process. All actions are previewed before changes are made, and Terraform migrate ultimately reduces the total cost of ownership by reducing the time spent performing manual migrations. To learn how Terraform migrate works, please refer to our <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/migrate/tf-migrate\">documentation for Terraform migrate</a>.</p>\n\n<h3>HCP Waypoint is now generally available to help provide self-service infrastructure to developers</h3>\n\n<p>As organizations quickly grow their infrastructure footprint across cloud environments, it can result in an overwhelming increase in scope and complexity in a short period of time. Enterprises can have thousands of downstream developers who need infrastructure to build applications, many of which are not well-versed in the specifics of infrastructure configuration. To scale effectively, organizations can set up an internal developer platform (IDP) that gives platform teams a central control point through which they can provide golden infrastructure workflows. This gives developers an easy way to consume these patterns in a self-service fashion.</p>\n\n<p>HCP Waypoint, a product for creating an IDP to make infrastructure easily accessible, is now generally available. With this release comes <em><a href=\"https://developer.hashicorp.com/hcp/docs/waypoint/concepts/templates\">templates</a></em> for provisioning underlying infrastructure and <em><a href=\"https://developer.hashicorp.com/hcp/docs/waypoint/concepts/add-ons\">add-ons</a></em> to manage application dependencies. These components are also GA as of today.</p>\n\n<p>The GA release of HCP Waypoint also includes a new <em>upgrade workflow</em> that pushes updates to Waypoint applications when the Waypoint template is updated, including updates to the underlying Terraform module version. We also support using the <a href=\"https://developer.hashicorp.com/hcp/api-docs/waypoint\">HCP API</a> to access Waypoint resources such as templates, add-ons, and applications. </p>\n\n<p>We’re also releasing variable support for <em><a href=\"https://www.hashicorp.com/blog/hcp-waypoint-actions-is-now-in-public-beta\">actions</a></em> (currently in public beta) to allow platform teams to specify input variables and their values when creating actions.</p>\n\n<p>HCP Waypoint is now available to all HCP Terraform Plus users. Variable support for actions will be available to HCP Terraform RUM Plus plans during public beta. To learn more, refer to our <a href=\"https://www.hashicorp.com/products/waypoint\">Waypoint product page</a> and see our blog post: <a href=\"https://www.hashicorp.com/blog/hcp-waypoint-now-ga-with-enhancements-to-golden-workflow-capabilities\">HCP Waypoint now GA with enhancements to golden workflow capabilities</a>.</p>\n\n<h3>Nomad adds support for Multi-Instance GPU (MIG), quotas for device resources, golden job versions, and more</h3>\n\n<p>Today we’re announcing Nomad 1.9, which now has the ability to schedule workloads onto an NVIDIA Multi-Instance GPU (A100 and H100 GPUs). As GPUs become more critical in high-performance computing tasks such as machine learning and generative AI, Nomad’s ability to schedule GPU workloads has continued to evolve and improve alongside the technology. Nomad’s MIG support now allows operators to partition GPU resources across multiple users for optimal GPU utilization. In addition, we now provide the ability to assign quotas to GPUs and GPU instances to help restrict aggregate usage of resources by namespace or region.</p>\n\n<p>Nomad 1.9 also brings NUMA awareness and quotas for device resources, improving Nomad’s device orchestration capabilities. In addition, Nomad also introduces golden job versions, which allow operators to tag and track their Nomad jobs for reuse. By reusing the “golden” jobs that follow organization best practices, orchestration with Nomad becomes more efficient and secure. </p>\n\n<p>To learn more, see our blog post: <a href=\"https://www.hashicorp.com/blog/nomad-1-9-adds-nvidia-mig-support-golden-job-versions-and-more\">Nomad 1.9 adds NVIDIA MIG support, golden job versions, and more</a>.</p>\n\n<h2>Get started today</h2>\n\n<p>With thousands of customers, our Infrastructure Lifecycle Management portfolio, including HashiCorp Terraform, Packer, Nomad, and Waypoint, offers a blueprint to cloud success as organizations are rethinking their cloud programs. For some organizations who have struggled with the transition to cloud, it’s a second chance to do cloud right.</p>\n\n<p>You can try many of these new features now and customers on the HashiCloud Cloud Platform can get them applied automatically with no disruption to existing workflows. HCP customers can also begin using the integrated product workflows that combine our ILM products with solutions from our Security Lifecycle Management (SLM) portfolio to simplify common use cases like image management within infrastructure provisioning and privileged access management.</p>\n\n<p>If you are new to our ILM products, you can get started in minutes using the <a href=\"https://portal.cloud.hashicorp.com/sign-up\">HashiCorp Cloud Platform</a> or sign up for <a href=\"https://app.terraform.io/public/signup/account\">HCP Terraform</a>, <a href=\"https://www.hashicorp.com/products/packer\">HCP Packer</a> and <a href=\"https://portal.cloud.hashicorp.com/sign-up\">HCP Waypoint</a> to get started for free today. To learn more about Nomad, check out our <a href=\"https://developer.hashicorp.com/nomad/tutorials/cluster-setup\">tutorials</a>.</p>\n\n<p>If you&#39;d like to see a deep dive webinar recap of these announcements, sign up for our <a href=\"https://www.hashicorp.com/lp/infrastructure-lifecycle-management\">ILM HashiConf recap</a>.</p>",
      "summary": "New Infrastructure Lifecycle Management (ILM) offerings from HashiCorp Terraform, Packer, Nomad, and Waypoint help organizations manage their infrastructure at scale with reduced complexity.",
      "date_published": "2024-10-15T12:30:00.000Z",
      "author": {
        "name": "Yushuo Huang"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/hcp-terraform-adds-run-queue-visibility-and-new-ephemeral-workspace-features",
      "url": "https://www.hashicorp.com/blog/hcp-terraform-adds-run-queue-visibility-and-new-ephemeral-workspace-features",
      "title": "HCP Terraform adds run queue visibility and new ephemeral workspace features",
      "content_html": "<p>In the past few months, the HashiCorp Terraform team launched a slew of improvements to help platform teams simplify and streamline their IT operations so they can increase developer velocity and cut costs for organizations. The new HCP Terraform improvements include:</p>\n\n<ul>\n<li>Queue visibility for HCP Terraform (GA)</li>\n<li>Project-scoped ephemeral workspaces for HCP Terraform Plus (GA)</li>\n<li>Ephemeral workspace management with the Terraform provider for HCP Terraform Plus and Terraform Enterprise (GA)</li>\n</ul>\n\n<h3>Run queue visibility</h3>\n\n<p>In the past, when Terraform runs queued up, it could be challenging for the platform engineers to identify which runs across various workspaces were causing bottlenecks. Run queue visibility for HCP Terraform (available for Terraform Enterprise soon) provides platform teams the tools and visibility to see the activity of all runs in the organization so platform engineers can easily and quickly figure out which runs are running or queued at any point in time and take remedial actions when necessary.</p>\n\n<p>This feature shows an org-level view of runs across workspaces, agent pools, and run operation types, with the ability to filter by different dimensions. So when making high-priority infrastructure changes, platform engineers can easily determine where their changes are in the run queue and quickly find out if the platform is being unresponsive and causing stuck runs.</p>\n<img src=https://www.datocms-assets.com/2885/1726598465-run-queue-blog.png alt=The updated organization runs view displays all pending runs with advanced filtering.><h3>Project-scoped ephemeral workspaces</h3>\n\n<p>Platform teams need to support self-service provisioning for developers, but temporary sandboxes and development environments drive up cloud costs when left running past their intended life. Ephemeral workspaces enable automatic destruction of resources, however, they have to be configured on every workspace. Applying auto-destroy settings to all current and new workspaces in a project required manual effort or custom API scripting.</p>\n<iframe width=\"560\" height=\"315\" src=https://youtu.be/9FQPpG4A46g frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><p>With the new project-scoped auto-destroy settings for HCP Terraform, project admins can set a default inactivity timeframe for the workspaces in a project. All new and existing workspace created via self-service workflows in the project will inherit this setting as their default auto-destroy configuration while allowing individual workspace owners to override for fine-grained control. This ensures that temporary resources are cleaned up to reduce cloud costs and manual configuration burden.</p>\n\n<p>Learn more on the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/projects/managing#automatically-destroy-inactive-workspaces\">managing projects documentation page</a>.</p>\n\n<h3>Manage ephemeral workspaces with the Terraform TFE provider</h3>\n\n<p>We recommend platform teams use the <a href=\"https://registry.terraform.io/providers/hashicorp/tfe/latest\">Terraform TFE provider</a> to manage their HCP Terraform and Terraform Enterprise resources. With the latest TFE provider, you can configure the auto-destroy time-to-live settings on workspaces.</p>\n\n<p>The following Terraform example configures the <code>website-main-dev</code> workspace to automatically destroy its resources after seven days of inactivity. This saves your team money by ensuring that resources in development environments are cleaned up when unused.</p>\n<pre><code>resource \"tfe_workspace\" \"app-dev\" {\n  name        = \"website-main-dev\"\n  tag_names   = [\"dev\", \"app\"]\n  description = \"Temporary web resources for dev team.\"\n\n  auto_destroy_activity_duration = \"7d\"\n}</code></pre><p>With version 0.57 of the <a href=\"https://registry.terraform.io/providers/hashicorp/tfe/latest\">provider for HCP Terraform and Terraform Enterprise</a>, ephemeral workspace settings can now be managed via the organization’s existing, provider-driven workspace management practices. Admins can define a workspace&#39;s auto-destroy settings using the <code>auto_destroy_activity_duration</code> or <code>auto_destroy_at</code> attributes of the <a href=\"https://registry.terraform.io/providers/hashicorp/tfe/latest/docs/resources/workspace\"><code>tfe_workspace</code> resource</a>. Now, teams can more easily self-manage and save costs at scale by cleaning up their workspace resources.</p>\n\n<h2>Get started with HCP Terraform</h2>\n\n<p>These HCP Terraform and Terraform Enterprise enhancements represent a continued effort to help customers maximize their infrastructure investments and accelerate application delivery by optimizing their infrastructure lifecycle management.</p>\n\n<p>To learn more about these features, visit our <a href=\"https://developer.hashicorp.com/terraform\">Terraform guides and documentation on HashiCorp Developer</a>. If you are new to Terraform, <a href=\"https://app.terraform.io/public/signup/account\">sign up for HCP Terraform</a> and get started for free today.</p>",
      "summary": "HCP Terraform and Terraform Enterprise gain new features related to ephemeral workspaces along with run queue visibility for HCP Terraform specifically.",
      "date_published": "2024-09-18T16:00:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/automate-aws-deployments-with-hcp-terraform-and-github-actions",
      "url": "https://www.hashicorp.com/blog/automate-aws-deployments-with-hcp-terraform-and-github-actions",
      "title": "Automate AWS deployments with HCP Terraform and GitHub Actions",
      "content_html": "<p><em>Saravanan Gnanaguru is a <a href=\"https://www.hashicorp.com/ambassador\">HashiCorp Ambassador</a></em></p>\n\n<p>Using GitHub Actions with HashiCorp Terraform to automate infrastructure as code workflows directly from version control is a popular early path for many developer teams. However, this setup can make it difficult to stop configuration drift as your infrastructure codebase grows. </p>\n\n<p>Rather than running Terraform on the GitHub Actions instance runner, it’s much easier and safer to run configurations remotely via HCP Terraform. This ensures that the creation, modification, and deletion of Terraform resources is handled on a managed cloud platform rather than on the GitHub Actions runner. HCP Terraform has many more systems and safeguards for team Terraform management and <a href=\"https://www.hashicorp.com/resources/how-can-i-prevent-configuration-drift\">drift prevention</a>.</p>\n\n<p>This post shows how to use HCP Terraform to define AWS infrastructure and GitHub Actions to automate infrastructure changes. You’ll learn how to set up a GitHub Actions workflow that interacts with HCP Terraform to automate the deployment of AWS infrastructure, such as Amazon EC2 instances.</p>\n\n<h2>Workflow overview</h2>\n\n<p>For this tutorial you can use your own Terraform configuration in a GitHub repository, or use this <a href=\"https://github.com/chefgs/terraform_repo/tree/main/tfcloud_samples\">example repository</a>. The example repository’s GitHub Actions include a workflow that creates the AWS resources defined in the repository. Whenever the repository trigger event happens on the main branch, it runs the workflow defined in the <code>.github/workflows</code> directory. It then performs the infrastructure creation or management in AWS. The figure below outlines the interaction between the GitHub repository, Actions, HCP Terraform, and AWS.</p>\n<img src=https://www.datocms-assets.com/2885/1725634943-hcptf-gh-actions.png alt=HCP Terraform and GitHub Actions workflow><p>Here’s how to implement this workflow.</p>\n\n<h2>Prerequisites</h2>\n\n<p>Ensure you have the following:</p>\n\n<ul>\n<li>An AWS account with necessary permissions to create resources.</li>\n<li>A HCP Terraform account, with a workspace set up for this tutorial.</li>\n<li>A GitHub account, with a repository for Terraform configuration files.</li>\n<li>The Terraform CLI installed on your local machine for testing purposes.</li>\n</ul>\n\n<p>Follow the following steps below to add AWS credentials in the HCP Terraform workspace and the <code>TF_API_TOKEN</code> in GitHub Actions secrets.</p>\n\n<h2>Securely access AWS from HCP Terraform</h2>\n\n<p>HCP Terraform’s <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials\">dynamic provider credentials</a> allow Terraform runs to <a href=\"https://docs.aws.amazon.com/sdkref/latest/guide/access-assume-role.html#credOrSourceAssumeRole\">assume</a> an IAM role through native OpenID Connect (OIDC) integration and obtain temporary security credentials for each run. These AWS credentials allow you to call AWS APIs that the IAM role has access to at runtime. These credentials are usable for only one hour by default, so their usefulness to an attacker is limited.</p>\n\n<p>For more on how to securely access AWS from HCP Terraform with OIDC federation, check out the <a href=\"https://www.hashicorp.com/blog/access-aws-from-hcp-terraform-with-oidc-federation\">Access AWS from HCP Terraform with OIDC federation</a> blog.</p>\n\n<h2>Add HCP Terraform token to GitHub Actions</h2>\n\n<p>Fetch the <code>TF_API_TOKEN</code> by following <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/api-tokens\">instructions available in the HCP Terraform documentation</a>. This example creates a user API token for the GitHub Action workflow.</p>\n\n<p>Open the GitHub repository with the Terraform configuration.</p>\n\n<p>Click on &quot;Settings&quot; in the repository menu. From the left sidebar, select &quot;Secrets&quot; and then choose &quot;Actions&quot;.</p>\n\n<p>To add a new repository secret, click on &quot;New repository secret&quot;. Name the secret <code>TF_API_TOKEN</code> and add the HCP Terraform API token to the “Value” field. Click &quot;Add secret&quot; to save the new secret.</p>\n<img src=https://www.datocms-assets.com/2885/1725634973-tf_api_token-secret.png alt=Naming the secret><p>By following these steps, you will securely provide your AWS credentials to HCP Terraform and also provide the HCP Terraform API token to GitHub Actions, enabling automated infrastructure deployment through a GitHub Actions workflow.</p>\n\n<h2>Create the GitHub Actions workflow</h2>\n\n<p>After setting up the credentials, add a GitHub Actions workflow to your repository. The example repository uses a <a href=\"https://github.com/chefgs/terraform_repo/blob/main/.github/workflows/tf_cloud_aws.yml\">workflow</a> YAML file defining one job with four steps to initialize, plan, apply, and destroy Terraform. This workflow uses the <a href=\"https://github.com/hashicorp/setup-terraform\">HashiCorp official marketplace actions</a> for performing the Terraform command operations.</p>\n<pre><code># This workflow will create AWS resource using HCP Terraform\n# It is reusable workflow that can be called in other workflows\n\nname: AWS Infra Creation Using in HCP Terraform\n\non:\n workflow_call:\n   secrets:\n       TF_API_TOKEN:\n           required: true\n push:\n   branches: [ \"main\" ]\n pull_request:\n   branches: [ \"main\" ]\n workflow_dispatch:\n\nenv:\n tfcode_path: tfcloud_samples/amazon_ec2\n tfc_organisation: demo-tf-org # Replace it with your TFC Org\n tfc_hostname: app.terraform.io\n tfc_workspace: demo-tf-workspace # Replace it with your TFC Workspace\n\njobs:\n aws_tfc_job:\n   name: Create AWS Infra Using TFC\n\n   runs-on: ubuntu-latest\n\n   steps:\n   - name: Checkout tf code in runner environment\n     uses: actions/checkout@v3.5.2\n\n   # Configure HCP Terraform API token, since we are using remote backend option of HCP Terraform in AWS code\n   - name: Setup Terraform CLI\n     uses: hashicorp/setup-terraform@v2.0.2\n     with:\n       cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n   # Add the AWS Creds as ENV variable in HCP Terraform workspace, since the tf run happens in HCP Terraform environment\n\n   # Invoke the Terraform commands\n   - name: Terraform init and validate\n     run: |\n       echo `pwd`\n       echo \"** Running Terraform Init**\"\n       terraform init\n        \n       echo \"** Running Terraform Validate**\"\n       terraform validate\n     working-directory: ${{ env.tfcode_path }}\n\n   - name: Terraform Plan\n     uses: hashicorp/tfc-workflows-github/actions/create-run@v1.3.0\n     id: run\n     with:\n       workspace: ${{ env.tfc_workspace }}\n       plan_only: true\n       message: \"Plan Run from GitHub Actions\"\n       ## Can specify hostname,token,organization as direct inputs\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n\n   - name: Terraform Plan Output\n     uses: hashicorp/tfc-workflows-github/actions/plan-output@v1.3.0\n     id: plan-output\n     with:\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n       plan: ${{ steps.run.outputs.plan_id }}\n  \n   - name: Reference Plan Output\n     run: |\n       echo \"Plan status: ${{ steps.plan-output.outputs.plan_status }}\"\n       echo \"Resources to Add: ${{ steps.plan-output.outputs.add }}\"\n       echo \"Resources to Change: ${{ steps.plan-output.outputs.change }}\"\n       echo \"Resources to Destroy: ${{ steps.plan-output.outputs.destroy }}\"\n\n # Once the user verifies the Terraform Plan, the user can run the Terraform Apply and Destroy commands\n apply_terraform_plan:\n     needs: aws_tfc_job\n     if: github.event_name == 'workflow_dispatch'\n     runs-on: ubuntu-latest\n     steps:\n     - name: Checkout\n       uses: actions/checkout@v3.5.2\n     - name: Setup Terraform CLI\n       uses: hashicorp/setup-terraform@v2.0.2\n       with:\n         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n     # Invoke the Terraform commands\n     - name: Terraform init and validate\n       run: |\n         echo `pwd`\n         echo \"** Running Terraform Init**\"\n         terraform init\n      \n         echo \"** Running Terraform Validate**\"\n         terraform validate\n       working-directory: ${{ env.tfcode_path }}\n    \n     - name: Terraform Apply\n       run: echo \"** Running Terraform Apply**\"; terraform apply -auto-approve\n       working-directory: ${{ env.tfcode_path }}\n      - name: Terraform Destroy\n       run: echo \"** Running Terraform Destroy**\"; terraform destroy -auto-approve\n       working-directory: ${{ env.tfcode_path }}</code></pre><p>Let’s review each section of the workflow.</p>\n\n<h3>Define the triggers</h3>\n\n<p>When you push commits or open a pull request on the <code>main</code> branch, the workflow initializes, plans, and applies Terraform. This workflow can be triggered by a:</p>\n\n<ul>\n<li><code>workflow_call</code>: This allows the workflow to be reused in other workflows. It requires the <code>TF_API_TOKEN</code> secret.</li>\n<li><code>push</code>: Triggers the workflow when there is a push to the <code>main</code> branch.</li>\n<li><code>pull_request</code>: Triggers the workflow when a pull request is made to the <code>main</code> branch.</li>\n<li><code>workflow_dispatch</code>: Allows the GitHub Actions interface to manually trigger the workflow.</li>\n</ul>\n<pre><code># This workflow will create AWS resource using HCP Terraform\n# It is reusable workflow that can be called in other workflows\n\nname: AWS Infra Creation Using in HCP Terraform\n\non:\n workflow_call:\n   secrets:\n       TF_API_TOKEN:\n           required: true\n push:\n   branches: [ \"main\" ]\n pull_request:\n   branches: [ \"main\" ]\n workflow_dispatch:</code></pre><h3>Configure environment variables</h3>\n\n<p>Set the <code>tfcode_path</code> environment variable to specify the location of your Terraform configuration files within the repository. Include the HCP Terraform organization, workspace, and hostname for future jobs.</p>\n<pre><code>env:\n tfcode_path: tfcloud_samples/amazon_ec2 # Directory in which the tf files are stored\n tfc_organisation: demo-tf-org # Replace it with your HCP Terraform Org\n tfc_hostname: app.terraform.io\n tfc_workspace: demo-tf-workspace # Replace it with your HCP Terraform Workspace</code></pre><h3>Define jobs</h3>\n\n<p>In the GitHub Actions workflow, define each automation step inside the <code>jobs</code> block. The job consists of job name and workflow runner instance definition in the <code>runs-on</code> block. This workflow uses the <code>ubuntu-latest</code> instance runner.</p>\n\n<p>The first step in the <code>apply_terraform_plan</code> is the <code>actions/checkout</code> entry, which clones the repository into the GitHub Actions runner. This makes the Terraform configuration files available for subsequent steps.</p>\n<pre><code>jobs:\n aws_tfc_job:\n   name: Create AWS Infra Using HCPTF\n\n   runs-on: ubuntu-latest\n\n   steps:\n   - name: Checkout tf code in runner environment\n     uses: actions/checkout@v3.5.2</code></pre><p>In the second step, use the <code>hashicorp/setup-terraform</code> pre-built action to configure the Terraform CLI in the runner environment. It sets up the HCP Terraform API token (<code>TF_API_TOKEN</code>) for authentication. This token allows the Terraform CLI to communicate with HCP Terraform, enabling it to manage state, perform operations, and apply configurations in the context of the HCP Terraform workspace.</p>\n<pre><code>    - name: Setup Terraform CLI\n      uses: hashicorp/setup-terraform@v2.0.2\n      with:\n        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}</code></pre><p>Next, initialize and validate Terraform. <code>terraform init</code> initializes the Terraform working directory by downloading necessary providers and initializing backends. If you’re using HCP Terraform as the backend, this command configures the workspace and prepares it for operations. </p>\n\n<p><code>terraform validate</code> checks the syntax and validity of the Terraform files, ensuring they are correctly formatted and logically sound. These commands run inside the working directory defined by <code>env.tfcode_path</code>, which contains the Terraform configuration.</p>\n<pre><code>    - name: Terraform init and validate\n      run: |\n        echo `pwd`\n        echo \"** Running Terraform Init**\"\n        terraform init\n          \n        echo \"** Running Terraform Validate**\"\n        terraform validate\n      working-directory: ${{ env.tfcode_path }}</code></pre><p>In general, you will want to review the<code>terraform plan</code> before applying to verify the changes Terraform will make. Use the <code>hashicorp/tfc-workflows-github/actions/create-run</code> action to run a plan in HCP Terraform and export the plan using the <code>plan_only</code> attribute. Then, use the <code>hashicorp/tfc-workflows-github/actions/plan-output</code> action to get the output of the Terraform plan using the <code>plan_id</code> from the previous step&#39;s output. Finally, print the plan status and resource changes in the workflow’s output.</p>\n<pre><code>   - name: Terraform Plan\n     uses: hashicorp/tfc-workflows-github/actions/create-run@v1.3.0\n     id: run\n     with:\n       workspace: ${{ env.tfc_workspace }}\n       plan_only: true\n       message: \"Plan Run from GitHub Actions\"\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n\n   - name: Terraform Plan Output\n     uses: hashicorp/tfc-workflows-github/actions/plan-output@v1.3.0\n     id: plan-output\n     with:\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n       plan: ${{ steps.run.outputs.plan_id }}\n  \n   - name: Reference Plan Output\n     run: |\n       echo \"Plan status: ${{ steps.plan-output.outputs.plan_status }}\"\n       echo \"Resources to Add: ${{ steps.plan-output.outputs.add }}\"\n       echo \"Resources to Change: ${{ steps.plan-output.outputs.change }}\"\n       echo \"Resources to Destroy: ${{ steps.plan-output.outputs.destroy }}\"</code></pre><p>The workflow outputs the plan status and any resources to add, change, or destroy in its log.</p>\n<img src=https://www.datocms-assets.com/2885/1725635607-tf-plan-reference-review.png alt=Workflow log output for plan><p>If the plan does not reflect the correct changes, fix the Terraform configuration to achieve the expected output. After verifying the plan in the previous job, manually trigger the workflow to run <code>terraform apply</code>. This command starts a run in the HCP Terraform workspace, where the actual infrastructure changes are made.</p>\n<pre><code> apply_terraform_plan:\n     needs: aws_tfc_job\n     if: github.event_name == 'workflow_dispatch'\n     runs-on: ubuntu-latest\n     steps:\n     - name: Checkout\n       uses: actions/checkout@v3.5.2\n     - name: Setup Terraform CLI\n       uses: hashicorp/setup-terraform@v2.0.2\n       with:\n         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n     # Invoke the Terraform commands\n     - name: Terraform init and validate\n       run: |\n         echo `pwd`\n         echo \"** Running Terraform Init**\"\n         terraform init\n      \n         echo \"** Running Terraform Validate**\"\n         terraform validate\n       working-directory: ${{ env.tfcode_path }}\n    \n     - name: Terraform Apply\n       run: echo \"** Running Terraform Apply**\"; terraform apply -auto-approve\n       working-directory: ${{ env.tfcode_path }}</code></pre><img src=https://www.datocms-assets.com/2885/1725635689-apply-terraform-after-review.png alt=Apply after reviewing plan><p>Optionally, you can add a <code>terraform destroy</code> step to clean up resources and avoid unnecessary costs. You can add this step in non-production or testing environments.</p>\n<pre><code>    - name: Terraform Destroy\n      run: |\n        echo \"** Running Terraform Destroy**\"\n        terraform destroy -auto-approve\n      working-directory: ${{ env.tfcode_path }}</code></pre><h2>Setup review and further learning</h2>\n\n<p>This setup leverages the strengths of both platforms: GitHub Actions for CI/CD automation and HCP Terraform for secure, collaborative infrastructure management. Integrating HCP Terraform with GitHub Actions provides a powerful, automated pipeline for deploying and managing AWS infrastructure. By leveraging these tools, teams can achieve more reliable and efficient infrastructure management, reduce manual errors, and ensure consistency across environments. </p>\n\n<p>HCP Terraform facilitates collaboration among team members ensuring that infrastructure changes are managed safely and efficiently. Platform teams can audit the runs in HCP Terraform while development teams can review runs in GitHub Actions.</p>\n\n<p>From a security perspective, HCP Terraform workspaces can be configured with environment variables, such as AWS credentials, or <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/dynamic-credentials\">dynamic credentials</a>. The GitHub Actions workflow does not directly handle the credentials, which minimizes the blast radius of compromised credentials through the workflow. HCP Terraform provides additional features like access controls, private module registry, and policy enforcement to ensure that infrastructure changes are secure and compliant with organizational policies. </p>\n\n<p>This guide has walked you through setting up a basic workflow, but the flexibility of both platforms allows for customization to fit your specific needs.</p>\n\n<p>For further questions on best practices, please refer to the GitHub Actions and HCP Terraform FAQs <a href=\"https://github.com/chefgs/terraform_repo/blob/main/tfcloud_samples/TFC_Workflow_BestPracticesFAQs.md\">available in this repository</a>. As mentioned before, this repository includes the full code example used in this post. For more information on GitHub Actions, review GitHub’s <a href=\"https://docs.github.com/en/actions\">documentation</a>. To learn more about automating Terraform with GitHub Actions, review the <a href=\"https://developer.hashicorp.com/terraform/tutorials/automation/github-actions\">official tutorial</a> on the HashiCorp Developer portal and the <a href=\"https://github.com/hashicorp/tfc-workflows-github/tree/main/actions\">starter workflow templates</a> to use HCP Terraform with GitHub Actions.</p>\n\n<p>For alternatives to this workflow, you can also use HCP Vault Secrets to sync the <code>TF_API_TOKEN</code> <a href=\"https://developer.hashicorp.com/hcp/docs/vault-secrets/integrations/github-actions\">to the GitHub Actions secrets</a>. With this method, you only need to update the token in one place, rather than every GitHub repo. HashiCorp also has <a href=\"https://www.hashicorp.com/blog/hashicorp-releases-new-ci-cd-pipeline-integration-tool-templates-for-terraform\">integration templates for HCP Terraform and GitHub Actions</a> that you should check out.</p>",
      "summary": "Learn how to use GitHub Actions to automate HCP Terraform operations.",
      "date_published": "2024-09-09T11:00:00.000Z",
      "author": {
        "name": "Saravanan Gnanaguru"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/access-aws-from-hcp-terraform-with-oidc-federation",
      "url": "https://www.hashicorp.com/blog/access-aws-from-hcp-terraform-with-oidc-federation",
      "title": "Access AWS from HCP Terraform with OIDC federation",
      "content_html": "<p>Storing access keys in HCP Terraform poses a security risk. While HCP Terraform secures sensitive credentials as write-only variables, you must audit the usage of long-lived access keys to detect if they are compromised. Not only is leaking the access key a risk, but many organizations have a policy to block the creation of such access keys. </p>\n\n<p>Fortunately, in many cases, you can authenticate with more secure alternatives to access keys. One such alternative is <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">AWS IAM OIDC federation</a>, which uses identity and access management (IAM) to grant external identities (such as HCP Terraform) the ability to <a href=\"https://docs.aws.amazon.com/sdkref/latest/guide/access-assume-role.html#credOrSourceAssumeRole\">assume</a> an IAM role.</p>\n\n<p>HCP Terraform’s <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials\">dynamic provider credentials</a> allow Terraform runs to assume an IAM role through native OpenID Connect (OIDC) integration and obtain temporary security credentials for each run. These AWS credentials allow you to call AWS APIs that the IAM role has access to at runtime. These credentials are usable for only one hour by default, so their usefulness to an attacker is limited.</p>\n\n<p>This brief tutorial will show you how to set up an OIDC provider and access AWS from HCP Terraform using dynamic provider credentials and OIDC federation.</p>\n\n<h2>Tutorial</h2>\n\n<p>For this tutorial, you will use HCP Terraform to provision an OIDC provider that establishes a trust relationship between HCP Terraform and your AWS account. This setup allows HCP Terraform to assume an IAM role at runtime and pass the obtained temporary security credentials to the AWS Terraform provider to run <code>terraform plan</code> or <code>apply</code>.</p>\n<p>To set up an OIDC provider, the below steps assume that you already have a <a href=\"https://docs.aws.amazon.com/cli/v1/userguide/cli-chap-authentication.html\">method</a> available to authenticate to your AWS account.</p>\n\n<h2>Set up the OIDC provider</h2>\n\n<p>To set up the HCP Terraform OIDC provider for OIDC federation in AWS, use the following example configuration:</p>\n<pre><code>data \"tls_certificate\" \"provider\" {\n  url = \"https://app.terraform.io\"\n}\n\nresource \"aws_iam_openid_connect_provider\" \"hcp_terraform\" {\n  url = \"https://app.terraform.io\"\n\n  client_id_list = [\n    \"aws.workload.identity\", # Default audience in HCP Terraform for AWS.\n  ]\n\n  thumbprint_list = [\n    data.tls_certificate.provider.certificates[0].sha1_fingerprint,\n  ]\n}</code></pre><p>Once the HCP Terraform OIDC provider is created, create an ‘example’ IAM role that HCP Terraform will assume at runtime:</p>\n<pre><code>data \"aws_iam_policy_document\" \"example_oidc_assume_role_policy\" {\n  statement {\n    effect = \"Allow\"\n\n    actions = [\"sts:AssumeRoleWithWebIdentity\"]\n\n    principals {\n      type        = \"Federated\"\n      identifiers = [aws_iam_openid_connect_provider.hcp_terraform.arn]\n    }\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"app.terraform.io:aud\"\n      values   = [\"aws.workload.identity\"]\n    }\n\n    condition {\n      test     = \"StringLike\"\n      variable = \"app.terraform.io:sub\"\n      values   = [\"organization:ORG_NAME:project:PROJECT_NAME:workspace:WORKSPACE_NAME:run_phase:*\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"example\" {\n  name               = \"example\"\n  assume_role_policy = data.aws_iam_policy_document.example_oidc_assume_role_policy.json\n}</code></pre><p>The IAM role defined above currently includes only an <code>assume_role_policy</code> and lacks additional permissions. Depending on your requirements, you may need to add more permissions to the role to allow it to create and manage resources, such as <a href=\"https://aws.amazon.com/s3/\">S3</a> buckets, or <a href=\"https://aws.amazon.com/ec2/\">EC2</a> instances.</p>\n\n<p>In the <code>aws_iam_policy_document</code>, define a condition that evaluates the OIDC subject claim for HCP Terraform organization, project, workspace, and run phase. The subject claim in the example searches for specific organization, project, and workspace. However, you can make the claim more flexible by using wildcards (*), such as <code>organization:ORG_NAME:project:PROJECT_NAME:workspace:*:run_phase:*</code>. </p>\n\n<p>This claim allows for matching of all workspaces and run phases within a specific HCP Terraform project and organization, which can be helpful in scenarios like using <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/no-code-provisioning\">HCP Terraform’s no-code modules</a> to provide self-service infrastructure, where workspace names may not be known in advance.</p>\n\n<p>Note that wildcards in OIDC subject claims can simplify access policies but introduce potential security risks. To balance flexibility and security, use wildcards carefully. While you can scope claims down to a specific HCP Terraform workspace or run phase for maximum security, wildcards can be used selectively to replace certain values, offering a compromise between granularity and convenience.</p>\n\n<p>You can add additional permissions to an IAM role by using the <code>aws_iam_policy_document</code> data source and the <code>aws_iam_policy</code> resource. See the example below:</p>\n<pre><code>data \"aws_iam_policy\" \"s3_full_access\" {\n  arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n}\n\nresource \"aws_iam_role_policy_attachment\" \"example_s3_full_access\" {\n  policy_arn = data.aws_iam_policy.s3_full_access.arn\n  role       = aws_iam_role.example.name\n}</code></pre><h2>Using OIDC federation</h2>\n\n<p>When using OIDC federation, apart from the region argument, you don’t need to include any authentication configuration within the provider block. As long as you set up the correct environment variables in your workspace—specifically, set <code>TFC_AWS_PROVIDER_AUTH</code> to <code>true</code> and <code>TFC_AWS_RUN_ROLE_ARN</code> to the IAM role <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html\">ARN</a> that HCP Terraform should assume at runtime.</p>\n<pre><code>resource \"tfe_variable\" \"tfc_aws_provider_auth\" {\n  key          = \"TFC_AWS_PROVIDER_AUTH\"\n  value        = \"true\"\n  category     = \"env\"\n  workspace_id = tfe_workspace.example.id\n}\n\nresource \"tfe_variable\" \"tfc_example_role_arn\" {\n  sensitive    = true\n  key          = \"TFC_AWS_RUN_ROLE_ARN\"\n  value        = aws_iam_role.example.arn\n  category     = \"env\"\n  workspace_id = tfe_workspace.example.id\n}</code></pre><p>HCP Terraform will automatically assume the IAM role and inject the temporary credentials for you, using the workspace environment variables, allowing you to focus on creating infrastructure.</p>\n\n<h2>Implementing access management for your AWS organization</h2>\n\n<p>For improved security and scalability, we recommend implementing a pattern where one or more HCP Terraform workspaces inject the IAM role and OIDC provider ARNs into other workspaces using an <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/cloud-multiple-variable-sets#create-variable-sets\">HCP Terraform variable set</a>. This enables the platform/cloud team to create HCP Terraform workspaces with pre-configured AWS authentication, scoped to a specific IAM role and permissions.</p>\n\n<p>Whether you create an OIDC provider per AWS account, per environment, or use a single OIDC provider, providing pre-configured AWS authentication for teams’ HCP Terraform workspace is a win-win for both the platform/cloud team and the teams they enable to work autonomously.</p>\n\n<p>Below is an example configuration that creates a variable set for a specific IAM role and sets two environment variables. HCP Terraform uses these environment variables to assume the IAM role and obtain temporary security credentials at runtime, injecting them into the provider to enable access to any AWS API allowed by the IAM role’s policies.</p>\n\n<p>First, create the variable set:</p>\n<pre><code>resource \"tfe_variable_set\" \"example\" {\n  name         = aws_iam_role.example.name\n  description  = \"OIDC federation configuration for ${aws_iam_role.example.arn}\"\n  organization = \"XXXXXXXXXXXXXXX\"\n}</code></pre><p>Next, set up the required environment variables and link them to the variable set:</p>\n<pre><code>resource \"tfe_variable\" \"tfc_aws_provider_auth\" {\n  key             = \"TFC_AWS_PROVIDER_AUTH\"\n  value           = \"true\"\n  category        = \"env\"\n  variable_set_id = tfe_variable_set.example.id\n}\n\nresource \"tfe_variable\" \"tfc_example_role_arn\" {\n  sensitive       = true\n  key             = \"TFC_AWS_RUN_ROLE_ARN\"\n  value           = aws_iam_role.example.arn\n  category        = \"env\"\n  variable_set_id = tfe_variable_set.example.id\n}</code></pre><p>Finally, share the variable set with another HCP Terraform workspace. This ensures that the targeted workspace receives and uses the environment variables, allowing HCP Terraform to automatically assume the IAM role and inject the temporary security credentials:</p>\n<pre><code>resource \"tfe_workspace_variable_set\" \"example\" {\n  variable_set_id = tfe_variable_set.example.id\n  workspace_id    = \"ws-XXXXXXXXXXXXXXX\"\n}</code></pre><h2>Creating infrastructure from another workspace</h2>\n\n<p>Using the IAM role created earlier in this tutorial, which has been assigned S3 permissions, you can create a bucket  right away within the workspace you’ve delegated access to without needing any additional configuration:</p>\n<pre><code>provider \"aws\" {\n  region = \"us-west-2\"\n}\n\nresource \"aws_s3_bucket\" \"example\" {\n  bucket = \"example\"\n}</code></pre><h2>Learn more about OIDC federation</h2>\n\n<p>For more on how to securely access AWS from HCP Terraform with OIDC federation, check out the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/aws-configuration\">Dynamic Credentials with the AWS Provider</a> and <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">OIDC federation</a> documentation. Find a more complete example of configuring the AWS IAM OIDC identity provider on <a href=\"https://github.com/bschaatsbergen/hcp-terraform-aws-oidc-federation/blob/main/main.tf\">GitHub</a>.</p>",
      "summary": "Securely access AWS from HCP Terraform using OIDC federation, eliminating the need to use access keys.",
      "date_published": "2024-09-05T07:00:00.000Z",
      "author": {
        "name": "Bruno Schaatsbergen"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/new-infra-integrations-github-illumio-palo-alto-networks-tessell-more",
      "url": "https://www.hashicorp.com/blog/new-infra-integrations-github-illumio-palo-alto-networks-tessell-more",
      "title": "New infrastructure integrations with GitHub, Illumio, Palo Alto Networks, Tessell, and more",
      "content_html": "<p>The HashiCorp Infrastructure Lifecycle Management ecosystem of Terraform and Packer continues to expand with new integrations that provide additional capabilities to HCP Terraform, Terraform Enterprise and Community Edition, and HCP Packer users as they provision and manage their cloud and on-premises infrastructure. </p>\n\n<p>Terraform is the world’s <a href=\"https://survey.stackoverflow.co/2024/technology#1-other-tools\">most widely used</a> multi-cloud provisioning product. Whether you&#39;re deploying to Amazon Web Services (AWS), Microsoft Azure, Google Cloud, other cloud and SaaS offerings, or an on-premises datacenter, Terraform can be your single control plane to provision and manage your entire infrastructure.</p>\n\n<p>Packer provides organizations with a single workflow to build cloud and private datacenter images and continuously manage them throughout their lifecycle.</p>\n<img src=https://www.datocms-assets.com/2885/1725376353-tf-integrations-9-24.png alt=Terraform integration vendor logos><h2><strong>Terraform providers</strong></h2>\n\n<p>As part of our focus on helping drive innovation through new Terraform integrations and AI, we recently launched our AI Spotlight Collection on the <a href=\"https://registry.terraform.io/\">HashiCorp Terraform Registry</a>.  Our goal is to accelerate Terraform users’ IT operations and support AIOps implementations by integrating with our AI and ML partners.  Make sure to keep an eye on the spotlight section as we continue to expand our listed offerings in the coming months, and reach out to <a href=\"mailto:technologypartners@hashicorp.com\">technologypartners@hashicorp.com</a> if you have a provider you would like to see listed.</p>\n\n<p>Additionally, we had 16 new verified Terraform providers from 14 different partners over the previous quarter:</p>\n\n<h3><strong>Astronomer.io</strong></h3>\n\n<p>Astronomer.io Astro gives users a single control point to access and manage connections to their data. They released a new <a href=\"https://registry.terraform.io/providers/astronomer/astro/latest\">Astro Terraform provider</a> that allows users to leverage Terraform to programmatically manage Astro infrastructure as an alternative to the Astro CLI, Astro UI, or Astro API.</p>\n\n<h3><strong>Authsignal</strong></h3>\n\n<p>Authsignal is a drop-in digital identity and authentication platform. They released a new <a href=\"https://registry.terraform.io/providers/authsignal/authsignal/latest\">Authsignal provider</a> that allows users to leverage Terraform to manage the Authsignal platform.</p>\n\n<h3><strong>Catchpoint</strong></h3>\n\n<p>Catchpoint offers cloud monitoring and visibility services. They’ve released the <a href=\"https://registry.terraform.io/providers/catchpoint/catchpoint/latest\">Catchpoint provider</a>, to allow users to manage web, API, transaction, DNS, SSL, BGP, Traceroute, and Ping tests through Terraform with minimum configurations.</p>\n\n<h3><strong>Files.com</strong></h3>\n\n<p>Files.com provides unified control and reporting for all the file transfers in their customers business. They have released the new <a href=\"https://registry.terraform.io/providers/Files-com/files/latest\">Files.com Terraform provider</a>, which provides convenient access to the Files.com API via Terraform for managing a users’ Files.com account.</p>\n\n<h3><strong>Illumio</strong></h3>\n\n<p>Illumio develops solutions to help stop attacks and ransomware from spreading with intelligent visibility and microsegmentation. They have released the <a href=\"https://registry.terraform.io/providers/illumio/illumio-cloudsecure/latest\">Illumio-CloudSecure provider</a>, which enables DevOps teams to utilize Terraform to manage Illumio CloudSecure resources.</p>\n\n<h3><strong>incident.io</strong></h3>\n\n<p>incident.io, is a Slack-powered incident management platform that has released the <a href=\"https://registry.terraform.io/providers/incident-io/incident/latest\">incident Terraform provider</a>. The provider allows Terraform to manage configuration such as incident severities, roles, custom fields and more inside of users’ incident.io accounts.</p>\n\n<h3><strong>JetBrains</strong></h3>\n\n<p>JetBrains creates intelligent software development tools that cover all stages of the software development cycle, including IDEs and tools for CI/CD and collaboration. Their new <a href=\"https://registry.terraform.io/providers/JetBrains/teamcity/latest\">TeamCity Terraform provider</a> allows DevOps engineers to initialize the JetBrains TeamCity server and automate its administration via Terraform.</p>\n\n<h3><strong>Juniper Networks</strong></h3>\n\n<p>Juniper Networks engages in the design, development, and sale of products and services for high-performance networks. They’ve released the <a href=\"https://registry.terraform.io/providers/Juniper/mist/latest\">Mist provider</a> to allow Terraform to manage Juniper Mist Organizations. The provider currently focuses on Day 0 and Day 1 operations around provisioning and deployment of the Mist service.  </p>\n\n<h3><strong>Palo Alto Networks</strong></h3>\n\n<p>Palo Alto Networks offers security solutions for on-prem, hybrid, and multi-cloud environments, across the development lifecycle to secure networks, protect cloud applications, and enable the SOC. They have released a new <a href=\"https://registry.terraform.io/providers/PaloAltoNetworks/prismasdwan/0.1.0\">prismasdwan provider</a> that provides resources and data sources to manage and query Prisma SD-WAN related config from Strata Cloud Manager.</p>\n\n<h3><strong>Render</strong></h3>\n\n<p>Render offers a unified cloud to build and run apps and websites with free TLS certificates, global CDN, private networks and auto-deploys from Git. They have released a <a href=\"https://registry.terraform.io/providers/render-oss/render/latest\">Render provider</a> that allows users to leverage Terraform to interact with and manage resources on the Render platform.</p>\n\n<h3><strong>SkySQL</strong></h3>\n\n<p>SkySQL, who brings production-grade capabilities to MariaDB, has released two providers: the <a href=\"https://registry.terraform.io/providers/skysqlinc/skysql-beta/latest\">skysql-beta provider</a> and <a href=\"https://registry.terraform.io/providers/skysqlinc/skysql/latest\">skysql provider</a>. The skysql provider allows customers to manage resources in SkySQL with Terraform and the beta provider allows them to utilize features that are in tech preview.</p>\n\n<h3><strong>Solace</strong></h3>\n\n<p>Solace builds event broker technology, an architectural layer that seamlessly gets events from where they occur to where they need to be across clouds, networks, and applications. They have released two new providers: the <a href=\"https://registry.terraform.io/providers/SolaceProducts/solacebroker/latest\">Solace Broker provider</a> that enables users to configure PubSub+ Software Event Brokers through Terraform; and the <a href=\"https://registry.terraform.io/providers/SolaceProducts/solacebrokerappliance/latest\">Solace Broker Appliance provider</a>, which enables users to configure a PubSub+ Event Broker Appliance using Terraform.</p>\n\n<h3><strong>Styra</strong></h3>\n\n<p>Styra creates and maintains solutions that enable users to define, enforce, and monitor policy across their cloud-native environments with a combination of open source and commercial products. They have released the <a href=\"https://registry.terraform.io/providers/StyraInc/styra/latest\">Styra provider</a> which enables the provisioning and managing of the Enterprise OPA Platform and Open Policy Agent authorization platform itself as Terraform resources, reducing friction and increasing control.</p>\n\n<h3><strong>Tessell</strong></h3>\n\n<p>Tessell is a database-as-a-service (DBaaS) platform that simplifies the management, security, and scalability of relational databases in the cloud. They have released the <a href=\"https://registry.terraform.io/providers/tessell-cloud/tessell/latest\">Tessell provider</a>, that allows users to integrate with the Tessell API and manage resources including: Database services across public clouds (AWS, Azure), database engines (Oracle, PostgreSQL, MySQL, SQL Server), Availability Machines for data protection (snapshots), secondary environments (sanitized snapshots), as well as the creation of database service clones.</p>\n\n<h2><strong>HCP Packer integrations</strong></h2>\n\n<p>HCP Packer, which standardizes and automates the process of governing, tracking, and auditing image artifacts, has two new integrations.  </p>\n\n<h3><strong>GitHub</strong></h3>\n\n<p>GitHub is a developer platform that allows developers to create, store, manage and share their code. The new <a href=\"https://github.com/marketplace/actions/setup-hashicorp-packer\">HCP Packer integration</a> ties Packer pipeline metadata with GitHub Actions.  This integration will enable users to build images in a GitHub Actions pipeline and then find the details of the pipeline including pipeline ID/name, workflow identifiers, job names, and runner environment details. This automation and tracking is crucial for establishing software supply chain security, auditing machine images, and creating containers. </p>\n\n<h3><strong>GitLab</strong></h3>\n\n<p>GitLab is a single application for the whole software development and operations lifecycle. The new HCP Packer integration ties HCP Packer pipeline metadata with GitLab pipelines, allowing users to track information including pipeline specifics and associated pull requests. </p>\n\n<h2><strong>Learn more about Terraform integrations</strong></h2>\n\n<p>All these integrations are available for use in the <a href=\"https://registry.terraform.io/\">HashiCorp Terraform Registry</a>. If you are a technology provider and want to verify an existing integration, please refer to our <a href=\"https://developer.hashicorp.com/terraform/docs/partnerships\">Terraform Integration Program</a>.</p>\n\n<p>If you haven’t already, try the <a href=\"https://app.terraform.io/public/signup/account\">free tier of HCP Terraform</a> to help simplify your Terraform workflows and management.</p>",
      "summary": "18 new Terraform and Packer integrations from 16 partners provide more options to automate and secure cloud infrastructure management.",
      "date_published": "2024-09-03T16:00:00.000Z",
      "author": {
        "name": "Tom O’Connell"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-enterprise-improves-deployment-flexibility-with-nomad-and-openshift",
      "url": "https://www.hashicorp.com/blog/terraform-enterprise-improves-deployment-flexibility-with-nomad-and-openshift",
      "title": "Terraform Enterprise improves deployment flexibility with Nomad and OpenShift",
      "content_html": "<p><a href=\"https://developer.hashicorp.com/terraform/enterprise\">HashiCorp Terraform Enterprise</a> is the self-hosted distribution of HCP Terraform for customers with strict regulatory, data residency, or air-gapped networking requirements.  The latest Terraform Enterprise releases provide more flexible deployment options for customers with support for deployment on two new application platforms: HashiCorp Nomad and Red Hat OpenShift.</p>\n\n<h2>New deployment runtime options</h2>\n\n<p>In September 2023, we introduced new <a href=\"https://www.hashicorp.com/blog/terraform-enterprise-adds-new-flexible-deployment-options\">flexible deployment options for Terraform Enterprise</a>, with initial support for Docker Engine and cloud-managed Kubernetes services (Amazon EKS, Microsoft AKS, and Google GKE). These options were extended in the April 2024 release with the <a href=\"https://www.hashicorp.com/blog/terraform-enterprise-adds-podman-support-and-workflow-enhancements\">addition of Podman support</a>.</p>\n\n<h3>HashiCorp Nomad</h3>\n\n<p><a href=\"https://www.hashicorp.com/products/nomad\">HashiCorp Nomad</a> is a modern and efficient application scheduler for containers, binaries, and virtual machines. With the August 2024 (<a href=\"https://developer.hashicorp.com/terraform/enterprise/releases/2024/v202408-1\">v202408-1</a>) release of Terraform Enterprise, we are excited to add Nomad as a fully supported runtime environment. Nomad Enterprise customers also benefit from direct HashiCorp support for their Terraform Enterprise deployment and its runtime.</p>\n\n<p>The quickest way to deploy Terraform Enterprise on a Nomad cluster is to use <a href=\"https://developer.hashicorp.com/nomad/tutorials/job-specifications/nomad-pack-intro\">Nomad Pack</a>, the templating and packaging tool for Nomad, with the <a href=\"https://github.com/hashicorp/nomad-pack-community-registry/tree/main/packs/tfe_fdo_nomad\">Nomad Pack for Terraform Enterprise</a>. For more information, visit the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/nomad\">Nomad installation page in the Terraform Enterprise documentation</a>.</p>\n\n<h3>Red Hat OpenShift</h3>\n\n<p><a href=\"https://www.redhat.com/en/technologies/cloud-computing/openshift\">Red Hat OpenShift</a> is a Kubernetes application platform popular among enterprises due to its pre-packaged operational extensions, strong default security posture, and commercial support options. As of the July 2024 (<a href=\"https://developer.hashicorp.com/terraform/enterprise/releases/2024/v202407-1\">v202407-1</a>) release, Terraform Enterprise is supported for deployment on OpenShift clusters. This includes self-managed OpenShift Container Platform environments and hosted OpenShift services on Amazon Web Services (AWS ROSA), Microsoft Azure, Google Cloud, and IBM Cloud.</p>\n\n<p>Deploying Terraform Enterprise on OpenShift is similar to the existing Kubernetes options, with the addition of the <code>openshift.enabled</code> parameter in the Helm chart to support OpenShift security context requirements. To learn more, refer to the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/openshift\">Operate on Red Hat OpenShift documentation</a>.</p>\n\n<h2>Migration from Replicated-based installs</h2>\n\n<p>Customers still running a Replicated deployment of Terraform Enterprise are strongly encouraged to <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/replicated-migration\">migrate to one of the new flexible deployment options</a>. The final Replicated release of Terraform Enterprise is scheduled for November 2024. While HashiCorp Support will accept cases for this release until April 1, 2026, migrating by November ensures organizations will continue to receive the latest features and fixes.</p>\n\n<p>As of the August 2024 release, the flexible deployment options for Terraform Enterprise include:</p>\n\n<ul>\n<li>Docker Engine on any supported Linux distribution</li>\n<li>Podman on Red Hat Enterprise Linux 8 or 9</li>\n<li>Cloud-managed Kubernetes: Amazon EKS, Microsoft AKS, and Google GKE</li>\n<li>Red Hat OpenShift</li>\n<li>HashiCorp Nomad</li>\n</ul>\n\n<p>If you’re unsure which deployment option to adopt before November, HashiCorp recommends the Docker Engine option. Customers running Replicated already have Docker Engine installed on their host, so you’re already halfway there. To migrate, generate a Docker Compose file from your current configuration, stop Replicated, and start the new <code>terraform-enterprise</code> container. No data migrations are necessary. Check out the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/replicated-migration#mounted-disk-to-docker\">Replicated migration guides</a> for step-by-step instructions.</p>\n\n<p>Customers can contact their HashiCorp representative for more information and to validate their migration and upgrade path.</p>\n\n<h2>Migration service offering</h2>\n\n<p>Looking for hands-on support for your migration off of Replicated? The HashiCorp Professional Services team can help you through this process from design to execution. Ask your account representative for more information on migration services to a new deployment option or to HCP Terraform.</p>\n\n<h2>Other recent Terraform Enterprise highlights</h2>\n\n<p>The last few Terraform Enterprise monthly releases brought new enhancements and features to improve efficiency and flexibility, including:</p>\n\n<ul>\n<li>A new dedicated <a href=\"https://www.hashicorp.com/blog/terraform-cloud-improves-visibility-and-control-for-projects#project-overview-page\">project overview page</a> in the UI (v202405-1) to make viewing and managing your organization’s projects more efficient</li>\n<li>New granular <a href=\"https://www.hashicorp.com/blog/terraform-improves-permissions-management-for-teams\">permissions to delegate team management</a> to non-owners (v202405-1)</li>\n<li>An <a href=\"https://developer.hashicorp.com/terraform/enterprise/users-teams-organizations/organizations#runs\">organization-level runs page</a> has been added in v202406-1 to view and cancel the current active run for all workspaces in an organization</li>\n<li><a href=\"https://www.hashicorp.com/blog/terraform-adds-a-new-setting-to-manage-team-tokens\">Improved control over team API token management</a> (v202408-1) using a new setting to control whether the members of a team can view or manage their team’s API token</li>\n</ul>\n\n<h2>Upgrade now</h2>\n\n<p>To learn more about the deployment options for Terraform Enterprise, review the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy\">installation overview documentation</a>. To catch up on everything new and changed in recent Terraform Enterprise versions, check out the <a href=\"https://developer.hashicorp.com/terraform/enterprise/releases\">release notes</a>.</p>\n\n<p>To learn more about standardizing the infrastructure lifecycle with Terraform, explore our hosted and self-managed delivery options by visiting the <a href=\"https://www.hashicorp.com/products/terraform\">Terraform product page</a> or <a href=\"https://www.hashicorp.com/contact-sales\">contacting HashiCorp sales</a>.</p>",
      "summary": "Customers can now deploy Terraform Enterprise using Red Hat OpenShift or HashiCorp Nomad runtime platforms.",
      "date_published": "2024-08-29T16:00:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-provider-for-google-cloud-6-0-is-now-ga",
      "url": "https://www.hashicorp.com/blog/terraform-provider-for-google-cloud-6-0-is-now-ga",
      "title": "Terraform provider for Google Cloud 6.0 is now GA",
      "content_html": "<p>We are excited to announce the release of version 6.0 of the <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest\">HashiCorp Terraform Google provider,</a> with updates to <em>labels</em> designed to improve usability. Users now have the ability to view resources managed by Terraform when viewing/editing such resources through other tools. This post covers the details and benefits of the updated provider, and recaps key new features released this year.</p>\n\n<h2>2024 Terraform Google provider highlights</h2>\n\n<p>As the Terraform Google provider tops 300 million downloads this year, Google and HashiCorp continue to develop new integrations to help customers work faster, get benefits from more services and features, and find developer-friendly ways to deploy cloud infrastructure. This year, we focused on listening to the community by adding oft-requested new features to the Google provider, including:</p>\n\n<ul>\n<li>Provider-defined functions</li>\n<li>Default attribution label</li>\n<li>Expanding labels model support across more resources</li>\n</ul>\n\n<h2>Provider-defined functions</h2>\n\n<p>With the release of <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">Terraform 1.8</a>, providers can implement custom functions that you can call from the Terraform configuration. Earlier this year we announced the <a href=\"https://developer.hashicorp.com/terraform/plugin/framework/functions/concepts\">general availability of provider-defined functions in the Google Cloud provider</a>, adding a simplified way to get regions, zones, names, and projects from the IDs of resources that aren’t managed by your Terraform configuration. Provider-defined functions can now help parse Google IDs when adding an IAM binding to a resource that’s managed outside of Terraform:</p>\n<pre><code>resource \"google_cloud_run_service_iam_member\" \"example_run_invoker_jane\" {\n\n member   = \"user:jane@example.com\"\n\n role     = \"run.invoker\"\n\n service  = provider::google::name_from_id(var.example_cloud_run_service_id)\n\n location = provider::google::location_from_id(var.example_cloud_run_service_id)\n\n project  = provider::google::project_from_id(var.example_cloud_run_service_id)\n\n}</code></pre><p>This release represents another step forward in our unique approach to ecosystem extensibility.</p>\n\n<h2>Default Terraform attribution label</h2>\n\n<p>In version 5.16 of the Google provider, a new optional <code>goog-terraform-provisioned</code> provider-level default label helps users track resources created by Terraform. Previously, users had to explicitly opt into this feature by setting the <code>add_terraform_attribution_label</code> option in the provider configuration block. In version 6.0, this attribution label is now enabled by default, and will be added to all newly created resources that support labels. This helps users easily identify and report on resources managed by Terraform when viewing/editing such resources through tools like Google Cloud Console, Cloud Billing, etc.</p>\n\n<p>Users who wish to opt out of this new default label can do so by disabling the <code>add_terraform_attribution_label</code> option in the provider block:</p>\n<pre><code>provider \"google\" {\n  # Opt out of the “goog-terraform-provisioned” default label\n  add_terraform_attribution_label = false\n}</code></pre><p>By default, the label is added to resources only upon creation. To proactively apply the label to existing resources, set the <code>terraform_attribution_label_addition_strategy</code> option to <code>PROACTIVE</code> in the provider block, which adds the label to all supported resources on the next <code>terraform apply</code>:</p>\n<pre><code>provider \"google\" {\n  # Apply the “goog-terraform-provisioned” label to existing resources\n  add_terraform_attribution_label               = true\n  terraform_attribution_label_addition_strategy = “PROACTIVE”\n}</code></pre><h2>Removing deprecated attributes and other behavior changes</h2>\n\n<p>Since the last major release, the Terraform Google provider has accumulated resources and properties that have been deprecated, renamed, or are no longer supported by Google. As version 6.0 is a major release, we have removed a number of resources and attributes that have been deprecated over the course of the provider’s lifetime. A complete list of behavior changes and removed properties can be found in the <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/version_6_upgrade\">Google 6.0 upgrade guide</a>.</p>\n\n<h2>Learn more about Google Cloud and HashiCorp</h2>\n\n<p>To learn the basics of Terraform using the Google provider, check out the <a href=\"https://developer.hashicorp.com/terraform/tutorials/gcp-get-started\">Get Started tutorials for Google Cloud</a>.</p>\n\n<p>When upgrading to version 6.0 of the Terraform Google provider, please consult the <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/version_6_upgrade\">upgrade guide on the Terraform Registry</a>, which contains a full list of the changes and upgrade considerations. Because this release introduces breaking changes, we recommend <a href=\"https://developer.hashicorp.com/terraform/language/providers/requirements#best-practices-for-provider-versions\">pinning your provider version</a> to protect against unexpected results. For a complete list of the changes in 6.0, please refer to the <a href=\"https://github.com/hashicorp/terraform-provider-google/releases/tag/v6.0.0\">Google provider changelog</a>.</p>\n\n<p>HashiCorp and Google partner on cloud infrastructure to make it easy for users to provision and manage Google Cloud resources. You can find out more about our partnership on our <a href=\"https://www.hashicorp.com/partners/cloud/google-cloud\">Google Cloud partner page</a>.</p>\n\n<p>If you are currently using Terraform Community Edition or are completely new to Terraform, <a href=\"http://hashi.co/tf-cloud-bc\">sign up for HCP Terraform</a> and get started using the free offering today.</p>",
      "summary": "Version 6.0 of the HashiCorp Terraform Google provider brings updates to default labels, letting practitioners view and edit resources with ease.",
      "date_published": "2024-08-27T13:00:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-azurerm-provider-4-0-adds-provider-defined-functions",
      "url": "https://www.hashicorp.com/blog/terraform-azurerm-provider-4-0-adds-provider-defined-functions",
      "title": "Terraform AzureRM provider 4.0 adds provider-defined functions",
      "content_html": "<p>Today, we are announcing the general availability of the HashiCorp <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest\">Terraform AzureRM provider 4.0</a>. This version includes new capabilities to improve the extensibility and flexibility of the provider: provider-defined functions and improved resource provider registration. Initially launched in <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">April 2024</a>, provider-defined functions allow anyone in the Terraform community to build custom functions within providers to extend the capabilities of Terraform. </p>\n\n<p>This post reviews the details and benefits of this new major version of the provider and also covers a handful of new features released this year.</p>\n\n<h2>2024 AzureRM provider highlights</h2>\n\n<p>Since the provider’s last major release in March 2022, we’ve added support for some 340 resources and 120 data sources, bringing the totals to more than 1,101 resources and almost 360 data sources as of mid-August, 2024. As the Terraform AzureRM provide<a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs\">r</a> download count tops 660 million, Microsoft and HashiCorp continue to develop new, innovative integrations that further ease the cloud adoption journey for enterprise organizations. This year we focused on improving the user experience for practitioners by adding new services to the AzureRM provider including:</p>\n\n<ul>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#provider-functions\">Provider-defined functions</a></li>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#improved-resource-provider-registration\">Improved resource provider registration</a></li>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#additional-properties-for-subnets-in-the-virtual-network-resource\">Additional properties for subnets in the virtual network resource</a></li>\n</ul>\n\n<h2>Provider-defined functions</h2>\n\n<p>With the release of <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">Terraform 1.8</a> in April, providers can implement custom functions that you can call from the Terraform configuration. The latest release of the Terraform AzureRM provider adds two Azure-specific provider functions to let users correct the casing of their resource IDs, or to access the individual components of it.</p>\n\n<p>The <code>normalise_resource_id</code> function attempts to normalize the case-sensitive system segments of a resource ID as required by the Azure APIs:.</p>\n<pre><code>output \"test\" {\n value = provider::azurerm::normalise_resource_id(\"/Subscriptions/12345678-1234-9876-4563-123456789012/ResourceGroups/resGroup1/PROVIDERS/microsoft.apimanagement/service/service1/gateWays/gateway1/hostnameconfigurations/config1\")\n}\n\n# Result: /subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/resGroup1/providers/Microsoft.ApiManagement/service/service1/gateways/gateway1/hostnameConfigurations/config1</code></pre><p>The <code>parse_resource_id</code> function takes an Azure resource ID and splits it into its component parts:</p>\n<pre><code>locals {\n parsed_id = provider::azurerm::parse_resource_id(\"/subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/resGroup1/providers/Microsoft.ApiManagement/service/service1/gateways/gateway1/hostnameConfigurations/config1\")\n}\n\noutput \"resource_group_name\" {\n value = local.parsed_id[\"resource_group_name\"]\n}\n\noutput \"resource_name\" {\n value = local.parsed_id[\"resource_name\"]\n}\n\n# Result:\n# Outputs:\n# \n# resource_group_name = \"resGroup1\"\n# resource_name = \"config1\"</code></pre><h2>Improved resource provider registration</h2>\n\n<p>Previously, the AzureRM provider took an all-or-nothing approach to Azure resource provider registration, where the Terraform provider would either attempt to register a fixed set of 68 providers upon initialization or registration could be skipped entirely by setting <code>skip_provider_registration = true</code> in the provider block. This limitation didn’t match Microsoft’s recommendations, which is to register resource providers only as needed to enable the services you’re actively using. With the addition of two new feature flags, <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#resource_provider_registrations\">resource<em>provider</em>registrations</a> and <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#resource_providers_to_register\">resource<em>providers</em>to_register</a>, users now have more control over which providers to automatically register or whether to continue managing a subscription’s resource provider registrations outside of Terraform.</p>\n\n<h2>Changes and deprecations</h2>\n\n<p>Since the last major release, the AzureRM provider has accumulated resources and properties that have been deprecated, renamed, or are no longer supported by Azure. As version 4.0 is a major release, we have removed a number of <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#behaviour-changes-and-removed-properties-in-resources\">resources</a> and <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#behaviour-changes-and-removed-properties-in-data-sources\">data sources</a> that have been deprecated over the course of the provider’s lifetime. A complete list of behavior changes and removed properties can be found in the <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#behaviour-changes-and-removed-properties-in-resources\">AzureRM provider 4.0 upgrade guide</a>.</p>\n\n<h2>Learn more about Microsoft and HashiCorp</h2>\n\n<p>The latest version of the AzureRM provider is available today. These features and enhancements will help simplify configurations and improve the overall experience of using the provider. Because this release introduces breaking changes, we recommend pinning your provider version to protect against unexpected results. For a complete list of the changes in 4.0, please review the <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide\">AzureRM provider upgrade guide</a>. </p>\n\n<p>Please share any bugs or enhancement requests with us via <a href=\"https://github.com/hashicorp/terraform-provider-azurerm/issues\">GitHub issues</a>. We are thankful to our partners and community members for their valuable contributions to the HashiCorp Terraform ecosystem.</p>",
      "summary": "Version 4.0 of the HashiCorp Terraform AzureRM provider brings support for provider-defined functions and improved resource provider registration.",
      "date_published": "2024-08-22T16:00:00.000Z",
      "author": {
        "name": "Aurora Chun"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-extension-for-vs-code-speeds-up-loading-of-large-workspaces",
      "url": "https://www.hashicorp.com/blog/terraform-extension-for-vs-code-speeds-up-loading-of-large-workspaces",
      "title": "Terraform extension for VS Code speeds up loading of large workspaces",
      "content_html": "<p>We are excited to announce that version 0.34 of the HashiCorp <a href=\"https://github.com/hashicorp/terraform-ls\">Terraform language server</a>, bundled with version 2.32 of the Terraform extension for Visual Studio Code, is now available. This latest iteration brings significant reductions in initial work and memory usage when opening workspaces. Additionally, version 0.34 of the language server introduces parallel loading of Terraform language constructs, which enables instantaneous autocompletion. This blog post highlights the new enhancements and the results of the improvements.</p>\n\n<h2>Performance with large Terraform workspaces</h2>\n\n<p>The Terraform language server provides IDE features in <a href=\"https://microsoft.github.io/language-server-protocol/\">LSP</a>-compatible editors like <a href=\"https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform\">Visual Studio Code</a>, <a href=\"https://www.sublimetext.com/\">Sublime Text</a>, <a href=\"https://neovim.io/\">Neovim</a>, and others. With previous versions of the Terraform language server, the initial loading experience of large and/or complex Terraform configurations could be time-consuming and resource-intensive. That’s because when opening the editor, the Terraform language server did a lot of work in the background to understand the code being worked on. </p>\n\n<p>Its indexing process finds all the Terraform files and modules in the current working directory, parses them, and builds an understanding of all the interdependencies and references. It holds this information inside an in-memory database, which is updated as files are changed. If a user opened a directory with many hundreds of folders and files, it would consume more CPU and memory than they expected.</p>\n\n<h2>Improved language server performance</h2>\n\n<p>Improving the efficiency and performance of the Terraform language server has been a frequent request from the Terraform community. To address the issue, we separated the LSP language features for several Terraform constructs:</p>\n\n<ul>\n<li><strong>Modules:</strong> This feature handles everything related to *.tf and *.tf.json files.</li>\n<li><strong>Root modules:</strong> This feature handles everything related to provider and module installation and lock files.</li>\n<li><strong>Variables:</strong> This handles everything related to *.tfvars and *.tfvars.json files.</li>\n</ul>\n\n<p>Splitting the existing language-related functionality into multiple, smaller, self-contained language features lets the server process the work related to the different constructs in parallel. At the same time, we were able to reduce the amount of work a feature does at startup and shift the work to a user&#39;s first interaction with a file.</p>\n\n<p>In addition, the language server now parses and decodes only the files a user is currently working with, instead of fetching the provider and module schemas for the entire workspace at startup. The indexing process begins only when a user later opens a file in a particular folder.</p>\n\n<p>This new process brings a significant reduction (up to 99.75%) in memory usage and startup time when opening a workspace. For example, we measured a workspace with 5,296 lines of code that previously took 450ms to open and consumed 523 MB of memory. After updating to the 0.34 language server and the 2.32 VS Code extension, open-time dropped to 1.4ms and only 1.6 MB of memory was consumed. The new process also reduces memory use and cuts startup time when opening files within a workspace. That’s because instead of keeping the schemas for everything in memory, Terraform now has only the schemas for the currently open directory.</p>\n<img src=https://www.datocms-assets.com/2885/1721838250-workspace_open.png alt=Startup and memory use reductions><h2>Summary and resources</h2>\n\n<p>Enhancements to the HashiCorp Terraform extension for Visual Studio Code and Terraform language server are available today. If you&#39;ve previously encountered problems with language server performance but have not yet tried these updates, we encourage you to check them out and share any bugs or enhancement requests with us via <a href=\"https://github.com/hashicorp/vscode-terraform/issues/new/choose\">GitHub issues</a>. Learn more by reading the <a href=\"https://github.com/hashicorp/terraform-ls/pull/1667\">LS state &amp; performance refactoring pull request details on GitHub</a>.</p>\n\n<p>If you are currently using Terraform Community Edition or are completely new to Terraform, <a href=\"http://hashi.co/tf-cloud-bc\">sign up for HCP Terraform</a> and get started using the free offering today.</p>",
      "summary": "New releases of the HashiCorp Terraform extension for Visual Studio Code and Terraform language server significantly reduce memory usage and start up time for large workspaces.",
      "date_published": "2024-07-24T16:00:00.000Z",
      "author": {
        "name": "Aurora Chun"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/why-use-vault-backed-dynamic-credentials-to-secure-hcp-terraform-infrastructure",
      "url": "https://www.hashicorp.com/blog/why-use-vault-backed-dynamic-credentials-to-secure-hcp-terraform-infrastructure",
      "title": "Why use Vault-backed dynamic credentials to secure HCP Terraform infrastructure?",
      "content_html": "<p>Many Terraform users still rely on static credentials (API keys, passwords, certificates, etc.) to authenticate workloads with cloud providers (AWS, Google Cloud, Azure). However, relying on this practice poses both operational and security challenges. Managing static, long-lived credentials does not scale well without tedious and time-consuming manual intervention. Additionally, users set credentials as workspace variables or variable sets in Terraform, adding additional complexity to their authentication process. </p>\n\n<p>This practice of manually securing static secrets only increases the likelihood of secrets leakage and <a href=\"https://www.hashicorp.com/resources/what-is-secret-sprawl-why-is-it-harmful\">secrets sprawl</a>, in which credentials end up scattered across multiple databases, clouds, and applications.</p>\n\n<p><a href=\"https://www.hashicorp.com/lp/vault-p?utm_source=google&utm_channel_bucket=paid&utm_medium=sem&utm_campaign=&utm_content=hashicorp%20vault%20cloud-161229712449-702136766602&utm_offer=&gad_source=1&gclid=CjwKCAjwps-zBhAiEiwALwsVYcbQ_eMaKOqbB2evCbYCiBHNJpDjDQXaKMUyYYUiHKEtHFkJaRmUVRoCGxkQAvD_BwE\">HashiCorp Vault</a> provides a secure and centralized repository for storing these secrets, eliminating the need to hardcode them within Terraform configurations. Vault also provides management and control over identity access management (IAM) policies that applications need to work with other applications and services. Using an API-first approach to authenticate all requests, Vault provides secure access only to authorized resources. </p>\n\n<p>One of the key features that Vault offers HCP Terraform users is <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/vault-backed\">Vault-backed dynamic credentials</a>. This feature provides a workflow to auto-generate and revoke secrets/credentials when they are no longer needed.  </p>\n\n<p>This blog will explain why securing your infrastructure with <a href=\"https://www.hashicorp.com/blog/why-we-need-dynamic-secrets\">dynamic secrets</a> through Vault-backed dynamic credentials is the most secure way to use Terraform and why it should be the new standard for your security roadmap.</p>\n\n<h2>What are Vault-backed dynamic credentials?</h2>\n\n<p>Vault-backed dynamic credentials are temporary, time-bound, and unique to each Terraform workload. When you adopt HashiCorp Vault as your secrets manager, Vault’s <a href=\"https://developer.hashicorp.com/vault/docs/secrets\">secrets engines</a> can connect with HCP Terraform and Terraform Enterprise’s <a href=\"https://www.hashicorp.com/blog/terraform-cloud-adds-vault-backed-dynamic-credentials\">dynamic provider credentials</a> feature to generate and manage dynamic credentials directly from Vault. </p>\n\n<p>The dynamic provider credentials feature automatically generates the API keys you need to access and build infrastructure for AWS, Microsoft Azure, and Google Cloud. It does this just-in-time with each provisioning run for one-time use. These dynamic credentials do not require manual rotation or revocation when they are no longer needed. </p>\n\n<p>By making credentials unique and used for one run only, teams drastically reduce the chance that they might be found by attackers, and eliminate the chance of long-term malicious access. </p>\n\n<h3>Standards-based</h3>\n\n<p>Drilling down into the security details of this feature, Vault-backed dynamic credentials strongly protect access workflows during infrastructure provisioning by leveraging workload identity and the <a href=\"https://en.wikipedia.org/wiki/OpenID#OpenID_Connect_(OIDC)\">OpenID Connect (OIDC)</a> standard. Before provisioning, operators must first configure Terraform as a trusted identity provider with Vault (or their cloud infrastructure providers in the case of basic dynamic provider credentials). HCP Terraform or Terraform Enterprise then generate a signed identity token for every workload to obtain single-run credentials that are injected into the run environment. This exchange happens automatically for the supported providers by adding a few simple environment variables to the workspace.</p>\n<img src=https://www.datocms-assets.com/2885/1721419685-oidc-vault-backed-dynamic-creds-hcp-tf.png alt=Vault-backed dynamic credentials flow><h2>Why use Vault-backed dynamic credentials?</h2>\n\n<p>Vault-backed dynamic credentials include several advantages over using only dynamic provider credentials without Vault:</p>\n\n<ul>\n<li>Consolidated management and auditing for all your cloud credentials and other secrets</li>\n<li>No OIDC setup required in your cloud provider</li>\n<li>Leverage Vault secrets engine configurations</li>\n<li>No need to expose inbound access to self-hosted Terraform Enterprise instances from cloud providers to validate OIDC metadata.</li>\n</ul>\n\n<h3>Consolidated management and auditing</h3>\n\n<p>Without being “Vault-backed”, dynamic provider credentials are still a step in the right direction to make your secrets management dynamic rather than static. But there isn’t as much auditing or management capability without the Vault integration. Secrets management is firmly in the purview of Vault, while Terraform is focused on provisioning. By using Vault-backed dynamic credentials instead of dynamic provider credentials without Vault, teams are able to logically consolidate Terraform credential management with all of the other secrets managed throughout the organization on one platform.</p>\n\n<p>Unlike static credentials, dynamic credentials are most effectively utilized within a secrets management platform, such as <a href=\"https://developer.hashicorp.com/vault/tutorials/get-started-hcp-vault-dedicated/vault-introduction\">HCP Vault Dedicated</a>, that automates their lifecycle. HCP Vault Dedicated can automatically generate temporary secrets as required and integrate dynamic secrets into infrastructure automation tools such as HCP Terraform.</p>\n\n<h3>No OIDC setup required in cloud provider</h3>\n\n<p>By setting up an <a href=\"https://developer.hashicorp.com/vault/tutorials/auth-methods/oidc-auth\">OIDC flow</a> from HCP Terraform to Vault instead of a cloud provider, teams can own the full security lifecycle process from authentication to secret generation. This lets teams use the more sophisticated feature set of Vault when managing dynamic provider credentials and reduce the surface area of security configurations required for all workloads.</p>\n\n<h3>Leverage Vault secrets engine configurations</h3>\n\n<p>Vault-backed dynamic credentials leverage Vault’s authentication and authorization capabilities to limit permissions based on metadata like the execution phase, workspace, or organization involved in the operation. Security teams already well-versed in configuring Vault policies and mapping workloads to cloud roles can use their existing workflows to authorize Terraform runs, saving time and effort.</p>\n\n<p>Another security benefit of Vault-backed dynamic credentials is the Vault token, which is revoked immediately after the plan or application runs. This means the cloud credentials are also immediately invalidated and cannot be re-used, as opposed to waiting for a fixed time-to-live to expire.</p>\n\n<h3>Protected inbound access</h3>\n\n<p>The OIDC workflow requires two-way communication so that the identity provider can validate the signature and metadata of the workload identity token presented by the client. For self-hosted Terraform Enterprise customers using standard dynamic provider credentials to authenticate directly to a cloud provider, <a href=\"https://www.hashicorp.com/blog/terraform-cloud-adds-vault-backed-dynamic-credentials\">inbound network access must be allowed</a> from the provider. Since the cloud providers don’t document the specific IP addresses used for OIDC integrations, this effectively means exposing Terraform Enterprise to the public internet.</p>\n\n<p>Instead, with Vault-backed dynamic credentials, only the Vault instance needs to directly access the metadata endpoints. Vault’s secret engines then use outbound-only connections to the cloud provider.</p>\n\n<h2>How do I start using Vault-backed dynamic credentials?</h2>\n\n<p>If you’re new to Vault, <a href=\"https://developer.hashicorp.com/vault\">start here</a> to try it out and see benefits quickly. If you’re already familiar with Vault, and have it set up in your organization, start by reading the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/vault-backed\">Vault-backed dynamic provider credentials documentation</a> to learn how to set up the feature. Then continue with a hands-on tutorial: <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/dynamic-credentials-vault\">Authenticate providers with Vault-backed dynamic credentials</a>.</p>\n\n<p>If you’re having trouble setting up Vault, or you just don’t have the time to self-manage and maintain an on-premises instance, let us manage it for you by signing up for a free HCP Vault Dedicated trial. Either of these two services are the easiest and fastest way to get started with Vault. You can also <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/users#log-in-with-your-hashicorp-cloud-platform-account\">link your HCP Terraform and HashiCorp Cloud Platform (HCP) accounts</a> together for a seamless sign-in experience.</p>",
      "summary": "Learn how HCP Terraform and Terraform Enterprise users can use Vault-backed dynamic credentials to secure their infrastructure during provisioning better than the base-level dynamic provider credentials.",
      "date_published": "2024-07-23T16:00:00.000Z",
      "author": {
        "name": "Sam Pandey"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/hcp-terraform-adds-granular-api-access-for-audit-trails",
      "url": "https://www.hashicorp.com/blog/hcp-terraform-adds-granular-api-access-for-audit-trails",
      "title": "HCP Terraform adds granular API access for audit trails",
      "content_html": "<p>Today we’d like to share the latest improvement to HCP Terraform’s permissions capabilities: <em>read-only permission to the HCP Terraform audit trails endpoint</em>. Available now in HCP Terraform, this new feature enables organization owners to generate a dedicated API key for least-privilege access to audit trails.</p>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/cloud-docs/api-docs/audit-trails\">HCP Terraform audit trails</a> let organization administrators quickly review the actions performed by members of their organization. It includes details such as who performed the action, what the action was, and when it was performed. It also contains the evaluation results of compliance-related features like <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/policy-enforcement\">policy enforcement</a> and <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/settings/run-tasks\">run tasks</a>. When paired with the <a href=\"https://splunkbase.splunk.com/\">Splunk app</a> it provides near real-time visibility into key actions. You can quickly see which workspaces are generating the most frequent changes, which policies are being evaluated most frequently, and which users are most active.</p>\n\n<p>In the past, within HCP Terraform, organization owners were required to create an <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/api-tokens#organization-api-tokens\">organization API token</a> to grant access to the audit trail endpoint. However, the excessive permissions associated with this token meant users had to vigilantly protect these credentials.</p>\n\n<h2>The new audit token for HCP Terraform audit trails</h2>\n\n<p>The new <em>audit token</em> type simplifies and enhances privilege management within organizations by letting owners adhere to the principle of least privilege access. This type allows read-only access to the HCP Terraform audit trail endpoint. By incorporating token expiration, organization owners gain complete control over the token&#39;s entire lifecycle, letting them specify when the audit token should expire. Users also now have the capability to effortlessly regenerate the token, which is particularly useful in situations where token rotation is required following a security incident. This advancement eliminates the need for users to possess owner-level access or manage the highly privileged organization API token.</p>\n\n<h2>Creating an audit token</h2>\n\n<p>To create an audit token, navigate to the API Tokens section within the Organization Settings page. Click the <em>Generate an audit token</em> button and configure the expiration settings as needed.</p>\n<img src=https://www.datocms-assets.com/2885/1721403524-token_setting.gif alt=audit trails API read-only><h2>Getting started</h2>\n\n<p>This feature is now available in HCP Terraform. Please refer to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/api-tokens\">Terraform’s API token documentation</a> for details on how to get started.</p>\n\n<p>If you are new to Terraform, you can get started with HashiCorp-managed <a href=\"https://cloud.hashicorp.com/products/terraform\">HCP Terraform</a> for free to begin provisioning and managing your infrastructure in any environment. And don’t forget to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/users#log-in-with-your-hashicorp-cloud-platform-account\">link your HCP Terraform and HashiCorp Cloud Platform (HCP) accounts</a> for a seamless sign-in experience.</p>",
      "summary": "HCP Terraform eliminates the need to rely on organization permissions to the audit trails endpoint, streamlining permissions workflows and reducing risk.",
      "date_published": "2024-07-22T16:00:00.000Z",
      "author": {
        "name": "Ryan Hall"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/simplifying-assertions-in-terraform-using-provider-defined-functions",
      "url": "https://www.hashicorp.com/blog/simplifying-assertions-in-terraform-using-provider-defined-functions",
      "title": "Simplifying assertions in Terraform using provider-defined functions",
      "content_html": "<p>Continuously validating your HashiCorp Terraform configurations greatly improves the user experience for those managing infrastructure. Continuous validation helps you deploy predictable and reliable infrastructure and provides direct feedback after changes are made. For instance, verifying if a website returns the expected status code post-deployment or the validity of a certificate after each run allows for early issue identification and resolution, minimizing impact and maintaining the integrity of a system.</p>\n\n<p>This post explores strategies for assertions and validations using a custom Terraform provider. By implementing these assertions and validations, you can <code>terraform apply</code> with greater confidence, which helps ensure your infrastructure meets your criteria, follows best practices, and reduces the risk of misconfigurations.</p>\n\n<h2>Terraform Assert provider</h2>\n\n<p>The <a href=\"https://registry.terraform.io/providers/hashicorp/assert/latest\">Assert provider</a> for Terraform, a provider managed by the community, offers a rich set of assertion capabilities through provider-defined functions such as <code>http_success()</code>, <code>expired()</code>, and <code>between()</code>. These assertion functions simplify your Terraform configurations, making it easier to do variable validation, continuous validation, and testing.</p>\n\n<p>The Assert provider functions complement Terraform’s <a href=\"https://developer.hashicorp.com/terraform/language/functions\">built-in functions</a> rather than replacing them. If Terraform’s built-in functions better fit your requirements, they should be your choice.</p>\n\n<p>To use the Assert provider, declare it as a <code>required_provider</code> in the <code>terraform {}</code> block:</p>\n<pre><code>terraform {\n  required_version = \">= 1.8.0\"\n  required_providers {\n    assert = {\n      source  = \"hashicorp/assert\"\n      version = \"0.11.1\"\n    }\n  }\n}</code></pre><p>You use the functions with a special syntax: <code>provider::assert::&lt;function_name&gt;</code>. For instance, to check if an HTTP status code falls within the success range, use the <code>http_success</code> function and call it using <code>provider::assert::http_success(data.http.example.status_code)</code>.</p>\n\n<p>Let&#39;s see how to use these functions to validate input variables.</p>\n\n<h2>Input variable validation</h2>\n\n<p>Terraform variables can have their default values overridden using CLI flags, <code>.tfvars</code> files, and environment variables. To ensure that any set value is within a required range of values, you can specify custom validation rules for a particular variable by adding a <code>validation</code> block within a <code>variable</code>. The <code>validation</code> block requires you to set a <code>condition</code> argument, which produces an error message If the condition evaluates to <code>false</code>. </p>\n\n<p>Using the Assert provider, the example below validates whether the value passed to the disk volume size variable is between 20GB and 40GB.</p>\n<pre><code>variable \"disk_volume_size\" {\n  type = number\n  validation {\n    condition     = provider::assert::between(20, 40, var.disk_volume_size)\n    error_message = \"Disk volume size must be between 20 and 40 GB\"\n  }\n}</code></pre><p>Without the Terraform Assert provider, you would need to create an &quot;or&quot; condition that references the <code>disk_volume_size</code> variable twice. While the condition validates the same criteria, it uses a less intuitive and readable expression:</p>\n<pre><code>condition = var.disk_volume_size >= 20 || var.disk_volume_size <= 40</code></pre><p>You can also use the <code>cidr</code> function to validate whether the provided value is a valid CIDR range:</p>\n<pre><code>variable \"subnet_a\" {\n  type = string\n  validation {\n    condition     = provider::assert::cidr(var.subnet_a)\n    error_message = \"Invalid CIDR range\"\n  }\n}</code></pre><p>Use the <code>key</code> or <code>value</code> functions to verify if a key or value is present in a map:</p>\n<pre><code>variable \"tags\" {\n  type = map(string)\n  validation {\n    condition     = provider::assert::key(\"key1\", var.tags)\n    error_message = \"Map must contain the key 'key1'\"\n  }\n}</code></pre><p>The Assert provider offers <a href=\"https://registry.terraform.io/providers/hashicorp/assert/latest/docs\">a wide range of functions</a> to help with variable validation, including numeric, IP, CIDR, JSON, YAML, Boolean, map, list, and string functions.</p>\n\n<p>The recent <a href=\"https://www.hashicorp.com/blog/terraform-1-9-enhances-input-variable-validations\">Terraform 1.9 release</a> includes enhanced input variable validation, allowing cross-object references. Previously, input validation conditions could reference only the variable itself. With Terraform 1.9, conditions can now reference other input variables, data sources, and local values. This significantly expands what authors can validate and allows for even more flexibility when using the Assert provider.</p>\n\n<p>Now that you have learned how to validate variables, let&#39;s investigate other Terraform features that can make your configuration more robust.</p>\n\n<h2>Custom conditions prevent problems</h2>\n\n<p>Besides input variable validation, Terraform supports several other custom conditions that are useful for asserting configurations, such as checks, preconditions, and postconditions.</p>\n\n<h3>Checks</h3>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/language/checks\">Checks</a> let you define custom conditions executed during every Terraform plan or apply, without impacting the overall status of the operation. They run as the final step of a plan or apply, after Terraform has planned or provisioned your infrastructure. Think of checks as a post-deployment monitoring capability.</p>\n\n<p>Using the Assert provider, here’s an example of how to verify if a website returns a successful status code:</p>\n<pre><code>data \"http\" \"terraform_io\" {\n  url = \"https://www.terraform.io\"\n}\n\ncheck \"terraform_io_success\" {\n  assert {\n    condition     = provider::assert::http_success(data.http.terraform_io.status_code)\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n  }\n}</code></pre><p>Without the Assert provider, you’d have to manually maintain a list of hard-coded success status codes, which reduces readability and maintainability.</p>\n\n<h3>Preconditions and postconditions</h3>\n\n<p>Another type of custom condition is a <a href=\"https://developer.hashicorp.com/terraform/language/checks#resource-preconditions-and-postconditions\">precondition or postcondition</a>. These function similarly to check blocks, but differ in their timing of execution: a precondition runs before a resource change is applied or planned, while a postcondition runs after. If either a precondition or postcondition fails, it blocks Terraform from executing the current operation. (If a <code>check</code> fails it does not prevent Terraform from executing an operation.)</p>\n<pre><code>data \"http\" \"terraform_io\" {\n  url = \"https://www.terraform.io\"\n\n  lifecycle {\n    postcondition {\n      condition = provider::assert::http_success(self.status_code)\n      error_message = \"${self.url} returned an unhealthy status code\"\n    }\n  }\n}</code></pre><p>Checks and validation help you improve the runtime quality of your Terraform configuration. Here are some techniques to improve the long-term health of your configuration.</p>\n\n<h2>Continuous validation in HCP Terraform</h2>\n\n<p>HCP Terraform features continuous validation, a form of <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/health\">health assessment</a>, allowing HCP Terraform to proactively monitor if a workspace’s configuration or modules with assertions are passing, and notify you if any assertions fail. Continuous validation evaluates preconditions, postconditions, and check blocks as part of a health assessment. We recommend using <a href=\"https://developer.hashicorp.com/terraform/language/checks\">check blocks</a> for post-apply monitoring.</p>\n\n<p>The example below shows a typical use of continuous validation to detect certificate renewals before they expire. The <code>expired</code> function within the Assert provider requires an RFC3339 timestamp as its input.</p>\n<pre><code>resource \"aws_acm_certificate\" \"example\" {\n  domain_name       = \"example.com\"\n  validation_method = \"DNS\"\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\ncheck \"example_certificate_renewal\" {\n  assert {\n    # Add 336 hours (14 days) to the expiration time, making sure we have enough time to renew the certificate\n    condition     = !provider::assert::expired(timeadd(aws_acm_certificate.example.not_after, \"336h\"))\n    error_message = \"Example certificate needs to be renewed\"\n  }\n}</code></pre><p>Health assessments can be enabled for <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/health#enable-health-assessments\">individual workspaces</a> or <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/organizations#health\">organization-wide</a>. To view health assessment results, including drift detection and continuous validation, go to the “Health” tab in an HCP Terraform workspace.</p>\n\n<p>If you use the <a href=\"https://registry.terraform.io/providers/hashicorp/tfe/latest/docs\">HCP Terraform and Terraform Enterprise provider</a> to manage workspace configurations, you can enable health assessments using the <code>assessments_enabled</code> argument in the <a href=\"https://registry.terraform.io/providers/hashicorp/tfe/latest/docs/resources/workspace#assessments_enabled\"><code>tfe_workspace</code></a> resource:</p>\n<pre><code>resource \"tfe_workspace\" \"example\" {\n  name                = \"example\"\n  assessments_enabled = true\n\n  # ... other workspace attributes\n}</code></pre><p>Finally, let&#39;s see how the assert provider can help simplify the process of testing Terraform modules.</p>\n\n<h2>Terraform test</h2>\n\n<p>The Terraform test framework allows you to ensure that Terraform configuration updates do not introduce breaking changes. By default, tests in Terraform create real infrastructure, so you can run assertions against this short-lived, test-specific infrastructure.</p>\n<pre><code>run \"health_check\" {\n  command = apply\n\n  assert {\n    condition     = provider::assert::http_success(data.http.index.status_code)\n    error_message = \"${data.http.index.url} returned an unhealthy status code\"\n  }\n}</code></pre><p>Note, you can configure Terraform to not create new infrastructure by setting the <code>command</code> argument to <code>plan</code>, which lets you validate logical operations and custom conditions without deploying resources.</p>\n<pre><code>run \"ebs_volume_size\" {\n  command = plan\n\n  assert {\n    condition     = provider::assert::between(1, 100, aws_ebs_volume.example.size)\n    error_message = \"EBS volume size must be between 1 and 100 GiB\"\n  }\n}</code></pre><p>Validation methods in Terraform, such as variable validation, preconditions, postconditions, and check blocks ensure the correctness and integrity of a Terraform configuration by enforcing custom conditions. For example, variable validation might prevent specifying an invalid subnet CIDR block. </p>\n\n<p>On the other hand, tests in Terraform validate the behavior and logic of the configuration, ensuring the deployed infrastructure behaves as expected.</p>\n\n<h2>Getting started with the Terraform Assert provider</h2>\n\n<p>The Terraform Assert provider, now available on the Terraform Registry, simplifies writing assertions, improving the reliability and integrity of your infrastructure deployments.</p>\n\n<p>To learn more about the Terraform Assert provider, check out these resources:</p>\n\n<ul>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/assert/latest/docs\">Terraform Registry documentation</a></li>\n<li><a href=\"https://github.com/hashicorp/terraform-provider-assert\">GitHub repository</a></li>\n</ul>\n\n<p>You can also read our <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">Terraform 1.8 provider-defined functions release blog post</a> to learn more about provider-defined functions. And, to learn how to leverage Terraform’s testing framework to write effective tests, see <a href=\"https://developer.hashicorp.com/terraform/tutorials/configuration-language/test\">Write Terraform tests</a> on the HashiCorp Developer site.</p>",
      "summary": "Learn about assertion and validation strategies for Terraform using the Terraform Assert utility provider. Plus: how to continuously validate your infrastructure using HCP Terraform.",
      "date_published": "2024-07-17T07:00:00.000Z",
      "author": {
        "name": "Bruno Schaatsbergen"
      }
    }
  ]
}
