{
  "version": "https://jsonfeed.org/version/1",
  "title": "HashiCorp Blog - Terraform",
  "home_page_url": "https://hashicorp.com/blog/products/terraform",
  "description": "Terraform is a platform for building, changing, and managing infrastructure in a safe, repeatable way.",
  "favicon": "https://www.hashicorp.com/favicon.svg",
  "author": {
    "name": "HashiCorp, Inc."
  },
  "items": [
    {
      "guid": "https://www.hashicorp.com/blog/automate-aws-deployments-with-hcp-terraform-and-github-actions",
      "url": "https://www.hashicorp.com/blog/automate-aws-deployments-with-hcp-terraform-and-github-actions",
      "title": "Automate AWS deployments with HCP Terraform and GitHub Actions",
      "content_html": "<p><em>Saravanan Gnanaguru is a <a href=\"https://www.hashicorp.com/ambassador\">HashiCorp Ambassador</a></em></p>\n\n<p>Using GitHub Actions with HashiCorp Terraform to automate infrastructure as code workflows directly from version control is a popular early path for many developer teams. However, this setup can make it difficult to stop configuration drift as your infrastructure codebase grows. </p>\n\n<p>Rather than running Terraform on the GitHub Actions instance runner, it’s much easier and safer to run configurations remotely via HCP Terraform. This ensures that the creation, modification, and deletion of Terraform resources is handled on a managed cloud platform rather than on the GitHub Actions runner. HCP Terraform has many more systems and safeguards for team Terraform management and <a href=\"https://www.hashicorp.com/resources/how-can-i-prevent-configuration-drift\">drift prevention</a>.</p>\n\n<p>This post shows how to use HCP Terraform to define AWS infrastructure and GitHub Actions to automate infrastructure changes. You’ll learn how to set up a GitHub Actions workflow that interacts with HCP Terraform to automate the deployment of AWS infrastructure, such as Amazon EC2 instances.</p>\n\n<h2>Workflow overview</h2>\n\n<p>For this tutorial you can use your own Terraform configuration in a GitHub repository, or use this <a href=\"https://github.com/chefgs/terraform_repo/tree/main/tfcloud_samples\">example repository</a>. The example repository’s GitHub Actions include a workflow that creates the AWS resources defined in the repository. Whenever the repository trigger event happens on the main branch, it runs the workflow defined in the <code>.github/workflows</code> directory. It then performs the infrastructure creation or management in AWS. The figure below outlines the interaction between the GitHub repository, Actions, HCP Terraform, and AWS.</p>\n<img src=https://www.datocms-assets.com/2885/1725634943-hcptf-gh-actions.png alt=HCP Terraform and GitHub Actions workflow><p>Here’s how to implement this workflow.</p>\n\n<h2>Prerequisites</h2>\n\n<p>Ensure you have the following:</p>\n\n<ul>\n<li>An AWS account with necessary permissions to create resources.</li>\n<li>A HCP Terraform account, with a workspace set up for this tutorial.</li>\n<li>A GitHub account, with a repository for Terraform configuration files.</li>\n<li>The Terraform CLI installed on your local machine for testing purposes.</li>\n</ul>\n\n<p>Follow the following steps below to add AWS credentials in the HCP Terraform workspace and the <code>TF_API_TOKEN</code> in GitHub Actions secrets.</p>\n\n<h2>Securely access AWS from HCP Terraform</h2>\n\n<p>HCP Terraform’s <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials\">dynamic provider credentials</a> allow Terraform runs to <a href=\"https://docs.aws.amazon.com/sdkref/latest/guide/access-assume-role.html#credOrSourceAssumeRole\">assume</a> an IAM role through native OpenID Connect (OIDC) integration and obtain temporary security credentials for each run. These AWS credentials allow you to call AWS APIs that the IAM role has access to at runtime. These credentials are usable for only one hour by default, so their usefulness to an attacker is limited.</p>\n\n<p>For more on how to securely access AWS from HCP Terraform with OIDC federation, check out the <a href=\"https://www.hashicorp.com/blog/access-aws-from-hcp-terraform-with-oidc-federation\">Access AWS from HCP Terraform with OIDC federation</a> blog.</p>\n\n<h2>Add HCP Terraform token to GitHub Actions</h2>\n\n<p>Fetch the <code>TF_API_TOKEN</code> by following <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/api-tokens\">instructions available in the HCP Terraform documentation</a>. This example creates a user API token for the GitHub Action workflow.</p>\n\n<p>Open the GitHub repository with the Terraform configuration.</p>\n\n<p>Click on &quot;Settings&quot; in the repository menu. From the left sidebar, select &quot;Secrets&quot; and then choose &quot;Actions&quot;.</p>\n\n<p>To add a new repository secret, click on &quot;New repository secret&quot;. Name the secret <code>TF_API_TOKEN</code> and add the HCP Terraform API token to the “Value” field. Click &quot;Add secret&quot; to save the new secret.</p>\n<img src=https://www.datocms-assets.com/2885/1725634973-tf_api_token-secret.png alt=Naming the secret><p>By following these steps, you will securely provide your AWS credentials to HCP Terraform and also provide the HCP Terraform API token to GitHub Actions, enabling automated infrastructure deployment through a GitHub Actions workflow.</p>\n\n<h2>Create the GitHub Actions workflow</h2>\n\n<p>After setting up the credentials, add a GitHub Actions workflow to your repository. The example repository uses a <a href=\"https://github.com/chefgs/terraform_repo/blob/main/.github/workflows/tf_cloud_aws.yml\">workflow</a> YAML file defining one job with four steps to initialize, plan, apply, and destroy Terraform. This workflow uses the <a href=\"https://github.com/hashicorp/setup-terraform\">HashiCorp official marketplace actions</a> for performing the Terraform command operations.</p>\n<pre><code># This workflow will create AWS resource using HCP Terraform\n# It is reusable workflow that can be called in other workflows\n\nname: AWS Infra Creation Using in HCP Terraform\n\non:\n workflow_call:\n   secrets:\n       TF_API_TOKEN:\n           required: true\n push:\n   branches: [ \"main\" ]\n pull_request:\n   branches: [ \"main\" ]\n workflow_dispatch:\n\nenv:\n tfcode_path: tfcloud_samples/amazon_ec2\n tfc_organisation: demo-tf-org # Replace it with your TFC Org\n tfc_hostname: app.terraform.io\n tfc_workspace: demo-tf-workspace # Replace it with your TFC Workspace\n\njobs:\n aws_tfc_job:\n   name: Create AWS Infra Using TFC\n\n   runs-on: ubuntu-latest\n\n   steps:\n   - name: Checkout tf code in runner environment\n     uses: actions/checkout@v3.5.2\n\n   # Configure HCP Terraform API token, since we are using remote backend option of HCP Terraform in AWS code\n   - name: Setup Terraform CLI\n     uses: hashicorp/setup-terraform@v2.0.2\n     with:\n       cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n   # Add the AWS Creds as ENV variable in HCP Terraform workspace, since the tf run happens in HCP Terraform environment\n\n   # Invoke the Terraform commands\n   - name: Terraform init and validate\n     run: |\n       echo `pwd`\n       echo \"** Running Terraform Init**\"\n       terraform init\n        \n       echo \"** Running Terraform Validate**\"\n       terraform validate\n     working-directory: ${{ env.tfcode_path }}\n\n   - name: Terraform Plan\n     uses: hashicorp/tfc-workflows-github/actions/create-run@v1.3.0\n     id: run\n     with:\n       workspace: ${{ env.tfc_workspace }}\n       plan_only: true\n       message: \"Plan Run from GitHub Actions\"\n       ## Can specify hostname,token,organization as direct inputs\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n\n   - name: Terraform Plan Output\n     uses: hashicorp/tfc-workflows-github/actions/plan-output@v1.3.0\n     id: plan-output\n     with:\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n       plan: ${{ steps.run.outputs.plan_id }}\n  \n   - name: Reference Plan Output\n     run: |\n       echo \"Plan status: ${{ steps.plan-output.outputs.plan_status }}\"\n       echo \"Resources to Add: ${{ steps.plan-output.outputs.add }}\"\n       echo \"Resources to Change: ${{ steps.plan-output.outputs.change }}\"\n       echo \"Resources to Destroy: ${{ steps.plan-output.outputs.destroy }}\"\n\n # Once the user verifies the Terraform Plan, the user can run the Terraform Apply and Destroy commands\n apply_terraform_plan:\n     needs: aws_tfc_job\n     if: github.event_name == 'workflow_dispatch'\n     runs-on: ubuntu-latest\n     steps:\n     - name: Checkout\n       uses: actions/checkout@v3.5.2\n     - name: Setup Terraform CLI\n       uses: hashicorp/setup-terraform@v2.0.2\n       with:\n         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n     # Invoke the Terraform commands\n     - name: Terraform init and validate\n       run: |\n         echo `pwd`\n         echo \"** Running Terraform Init**\"\n         terraform init\n      \n         echo \"** Running Terraform Validate**\"\n         terraform validate\n       working-directory: ${{ env.tfcode_path }}\n    \n     - name: Terraform Apply\n       run: echo \"** Running Terraform Apply**\"; terraform apply -auto-approve\n       working-directory: ${{ env.tfcode_path }}\n      - name: Terraform Destroy\n       run: echo \"** Running Terraform Destroy**\"; terraform destroy -auto-approve\n       working-directory: ${{ env.tfcode_path }}</code></pre><p>Let’s review each section of the workflow.</p>\n\n<h3>Define the triggers</h3>\n\n<p>When you push commits or open a pull request on the <code>main</code> branch, the workflow initializes, plans, and applies Terraform. This workflow can be triggered by a:</p>\n\n<ul>\n<li><code>workflow_call</code>: This allows the workflow to be reused in other workflows. It requires the <code>TF_API_TOKEN</code> secret.</li>\n<li><code>push</code>: Triggers the workflow when there is a push to the <code>main</code> branch.</li>\n<li><code>pull_request</code>: Triggers the workflow when a pull request is made to the <code>main</code> branch.</li>\n<li><code>workflow_dispatch</code>: Allows the GitHub Actions interface to manually trigger the workflow.</li>\n</ul>\n<pre><code># This workflow will create AWS resource using HCP Terraform\n# It is reusable workflow that can be called in other workflows\n\nname: AWS Infra Creation Using in HCP Terraform\n\non:\n workflow_call:\n   secrets:\n       TF_API_TOKEN:\n           required: true\n push:\n   branches: [ \"main\" ]\n pull_request:\n   branches: [ \"main\" ]\n workflow_dispatch:</code></pre><h3>Configure environment variables</h3>\n\n<p>Set the <code>tfcode_path</code> environment variable to specify the location of your Terraform configuration files within the repository. Include the HCP Terraform organization, workspace, and hostname for future jobs.</p>\n<pre><code>env:\n tfcode_path: tfcloud_samples/amazon_ec2 # Directory in which the tf files are stored\n tfc_organisation: demo-tf-org # Replace it with your HCP Terraform Org\n tfc_hostname: app.terraform.io\n tfc_workspace: demo-tf-workspace # Replace it with your HCP Terraform Workspace</code></pre><h3>Define jobs</h3>\n\n<p>In the GitHub Actions workflow, define each automation step inside the <code>jobs</code> block. The job consists of job name and workflow runner instance definition in the <code>runs-on</code> block. This workflow uses the <code>ubuntu-latest</code> instance runner.</p>\n\n<p>The first step in the <code>apply_terraform_plan</code> is the <code>actions/checkout</code> entry, which clones the repository into the GitHub Actions runner. This makes the Terraform configuration files available for subsequent steps.</p>\n<pre><code>jobs:\n aws_tfc_job:\n   name: Create AWS Infra Using HCPTF\n\n   runs-on: ubuntu-latest\n\n   steps:\n   - name: Checkout tf code in runner environment\n     uses: actions/checkout@v3.5.2</code></pre><p>In the second step, use the <code>hashicorp/setup-terraform</code> pre-built action to configure the Terraform CLI in the runner environment. It sets up the HCP Terraform API token (<code>TF_API_TOKEN</code>) for authentication. This token allows the Terraform CLI to communicate with HCP Terraform, enabling it to manage state, perform operations, and apply configurations in the context of the HCP Terraform workspace.</p>\n<pre><code>    - name: Setup Terraform CLI\n      uses: hashicorp/setup-terraform@v2.0.2\n      with:\n        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}</code></pre><p>Next, initialize and validate Terraform. <code>terraform init</code> initializes the Terraform working directory by downloading necessary providers and initializing backends. If you’re using HCP Terraform as the backend, this command configures the workspace and prepares it for operations. </p>\n\n<p><code>terraform validate</code> checks the syntax and validity of the Terraform files, ensuring they are correctly formatted and logically sound. These commands run inside the working directory defined by <code>env.tfcode_path</code>, which contains the Terraform configuration.</p>\n<pre><code>    - name: Terraform init and validate\n      run: |\n        echo `pwd`\n        echo \"** Running Terraform Init**\"\n        terraform init\n          \n        echo \"** Running Terraform Validate**\"\n        terraform validate\n      working-directory: ${{ env.tfcode_path }}</code></pre><p>In general, you will want to review the<code>terraform plan</code> before applying to verify the changes Terraform will make. Use the <code>hashicorp/tfc-workflows-github/actions/create-run</code> action to run a plan in HCP Terraform and export the plan using the <code>plan_only</code> attribute. Then, use the <code>hashicorp/tfc-workflows-github/actions/plan-output</code> action to get the output of the Terraform plan using the <code>plan_id</code> from the previous step&#39;s output. Finally, print the plan status and resource changes in the workflow’s output.</p>\n<pre><code>   - name: Terraform Plan\n     uses: hashicorp/tfc-workflows-github/actions/create-run@v1.3.0\n     id: run\n     with:\n       workspace: ${{ env.tfc_workspace }}\n       plan_only: true\n       message: \"Plan Run from GitHub Actions\"\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n\n   - name: Terraform Plan Output\n     uses: hashicorp/tfc-workflows-github/actions/plan-output@v1.3.0\n     id: plan-output\n     with:\n       hostname: ${{ env.tfc_hostname }}\n       token: ${{ secrets.TF_API_TOKEN }}\n       organization: ${{ env.tfc_organisation }}\n       plan: ${{ steps.run.outputs.plan_id }}\n  \n   - name: Reference Plan Output\n     run: |\n       echo \"Plan status: ${{ steps.plan-output.outputs.plan_status }}\"\n       echo \"Resources to Add: ${{ steps.plan-output.outputs.add }}\"\n       echo \"Resources to Change: ${{ steps.plan-output.outputs.change }}\"\n       echo \"Resources to Destroy: ${{ steps.plan-output.outputs.destroy }}\"</code></pre><p>The workflow outputs the plan status and any resources to add, change, or destroy in its log.</p>\n<img src=https://www.datocms-assets.com/2885/1725635607-tf-plan-reference-review.png alt=Workflow log output for plan><p>If the plan does not reflect the correct changes, fix the Terraform configuration to achieve the expected output. After verifying the plan in the previous job, manually trigger the workflow to run <code>terraform apply</code>. This command starts a run in the HCP Terraform workspace, where the actual infrastructure changes are made.</p>\n<pre><code> apply_terraform_plan:\n     needs: aws_tfc_job\n     if: github.event_name == 'workflow_dispatch'\n     runs-on: ubuntu-latest\n     steps:\n     - name: Checkout\n       uses: actions/checkout@v3.5.2\n     - name: Setup Terraform CLI\n       uses: hashicorp/setup-terraform@v2.0.2\n       with:\n         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}\n\n     # Invoke the Terraform commands\n     - name: Terraform init and validate\n       run: |\n         echo `pwd`\n         echo \"** Running Terraform Init**\"\n         terraform init\n      \n         echo \"** Running Terraform Validate**\"\n         terraform validate\n       working-directory: ${{ env.tfcode_path }}\n    \n     - name: Terraform Apply\n       run: echo \"** Running Terraform Apply**\"; terraform apply -auto-approve\n       working-directory: ${{ env.tfcode_path }}</code></pre><img src=https://www.datocms-assets.com/2885/1725635689-apply-terraform-after-review.png alt=Apply after reviewing plan><p>Optionally, you can add a <code>terraform destroy</code> step to clean up resources and avoid unnecessary costs. You can add this step in non-production or testing environments.</p>\n<pre><code>    - name: Terraform Destroy\n      run: |\n        echo \"** Running Terraform Destroy**\"\n        terraform destroy -auto-approve\n      working-directory: ${{ env.tfcode_path }}</code></pre><h2>Setup review and further learning</h2>\n\n<p>This setup leverages the strengths of both platforms: GitHub Actions for CI/CD automation and HCP Terraform for secure, collaborative infrastructure management. Integrating HCP Terraform with GitHub Actions provides a powerful, automated pipeline for deploying and managing AWS infrastructure. By leveraging these tools, teams can achieve more reliable and efficient infrastructure management, reduce manual errors, and ensure consistency across environments. </p>\n\n<p>HCP Terraform facilitates collaboration among team members ensuring that infrastructure changes are managed safely and efficiently. Platform teams can audit the runs in HCP Terraform while development teams can review runs in GitHub Actions.</p>\n\n<p>From a security perspective, HCP Terraform workspaces can be configured with environment variables, such as AWS credentials, or <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/dynamic-credentials\">dynamic credentials</a>. The GitHub Actions workflow does not directly handle the credentials, which minimizes the blast radius of compromised credentials through the workflow. HCP Terraform provides additional features like access controls, private module registry, and policy enforcement to ensure that infrastructure changes are secure and compliant with organizational policies. </p>\n\n<p>This guide has walked you through setting up a basic workflow, but the flexibility of both platforms allows for customization to fit your specific needs.</p>\n\n<p>For further questions on best practices, please refer to the GitHub Actions and HCP Terraform FAQs <a href=\"https://github.com/chefgs/terraform_repo/blob/main/tfcloud_samples/TFC_Workflow_BestPracticesFAQs.md\">available in this repository</a>. As mentioned before, this repository includes the full code example used in this post. For more information on GitHub Actions, review GitHub’s <a href=\"https://docs.github.com/en/actions\">documentation</a>. To learn more about automating Terraform with GitHub Actions, review the <a href=\"https://developer.hashicorp.com/terraform/tutorials/automation/github-actions\">official tutorial</a> on the HashiCorp Developer portal and the <a href=\"https://github.com/hashicorp/tfc-workflows-github/tree/main/actions\">starter workflow templates</a> to use HCP Terraform with GitHub Actions.</p>",
      "summary": "Learn how to use GitHub Actions to automate HCP Terraform operations.",
      "date_published": "2024-09-09T11:00:00.000Z",
      "author": {
        "name": "Saravanan Gnanaguru"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/access-aws-from-hcp-terraform-with-oidc-federation",
      "url": "https://www.hashicorp.com/blog/access-aws-from-hcp-terraform-with-oidc-federation",
      "title": "Access AWS from HCP Terraform with OIDC federation",
      "content_html": "<p>Storing access keys in HCP Terraform poses a security risk. While HCP Terraform secures sensitive credentials as write-only variables, you must audit the usage of long-lived access keys to detect if they are compromised. Not only is leaking the access key a risk, but many organizations have a policy to block the creation of such access keys. </p>\n\n<p>Fortunately, in many cases, you can authenticate with more secure alternatives to access keys. One such alternative is <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">AWS IAM OIDC federation</a>, which uses identity and access management (IAM) to grant external identities (such as HCP Terraform) the ability to <a href=\"https://docs.aws.amazon.com/sdkref/latest/guide/access-assume-role.html#credOrSourceAssumeRole\">assume</a> an IAM role.</p>\n\n<p>HCP Terraform’s <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials\">dynamic provider credentials</a> allow Terraform runs to assume an IAM role through native OpenID Connect (OIDC) integration and obtain temporary security credentials for each run. These AWS credentials allow you to call AWS APIs that the IAM role has access to at runtime. These credentials are usable for only one hour by default, so their usefulness to an attacker is limited.</p>\n\n<p>This brief tutorial will show you how to set up an OIDC provider and access AWS from HCP Terraform using dynamic provider credentials and OIDC federation.</p>\n\n<h2>Authentication sequence</h2>\n\n<p>For this tutorial, you will use HCP Terraform to provision an OIDC provider that establishes a trust relationship between HCP Terraform and your AWS account. This setup allows HCP Terraform to assume an IAM role at runtime and pass the obtained temporary security credentials to the AWS Terraform provider to run <code>terraform plan</code> or <code>apply</code>.</p>\n\n<p>The diagram below illustrates the authentication sequence for accessing AWS using dynamic provider credentials and OIDC federation.</p>\n<img src=https://www.datocms-assets.com/2885/1725487402-aws-provider-dynamic-credentials.png alt=AWS provider dynamic creds access flow.><p>To set up an OIDC provider, this step assumes that you already have a <a href=\"https://docs.aws.amazon.com/cli/v1/userguide/cli-chap-authentication.html\">method</a> available to authenticate to your AWS account.</p>\n\n<h2>Set up the OIDC provider</h2>\n\n<p>To set up the HCP Terraform OIDC provider for OIDC federation in AWS, use the following example configuration:</p>\n<pre><code>data \"tls_certificate\" \"provider\" {\n  url = \"https://app.terraform.io\"\n}\n\nresource \"aws_iam_openid_connect_provider\" \"hcp_terraform\" {\n  url = \"https://app.terraform.io\"\n\n  client_id_list = [\n    \"aws.workload.identity\", # Default audience in HCP Terraform for AWS.\n  ]\n\n  thumbprint_list = [\n    data.tls_certificate.provider.certificates[0].sha1_fingerprint,\n  ]\n}</code></pre><p>Once the HCP Terraform OIDC provider is created, create an ‘example’ IAM role that HCP Terraform will assume at runtime:</p>\n<pre><code>data \"aws_iam_policy_document\" \"example_oidc_assume_role_policy\" {\n  statement {\n    effect = \"Allow\"\n\n    actions = [\"sts:AssumeRoleWithWebIdentity\"]\n\n    principals {\n      type        = \"Federated\"\n      identifiers = [aws_iam_openid_connect_provider.hcp_terraform.arn]\n    }\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"app.terraform.io:aud\"\n      values   = [\"aws.workload.identity\"]\n    }\n\n    condition {\n      test     = \"StringLike\"\n      variable = \"app.terraform.io:sub\"\n      values   = [\"organization:ORG_NAME:project:PROJECT_NAME:workspace:WORKSPACE_NAME:run_phase:*\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"example\" {\n  name               = \"example\"\n  assume_role_policy = data.aws_iam_policy_document.example_oidc_assume_role_policy.json\n}</code></pre><p>The IAM role defined above currently includes only an <code>assume_role_policy</code> and lacks additional permissions. Depending on your requirements, you may need to add more permissions to the role to allow it to create and manage resources, such as <a href=\"https://aws.amazon.com/s3/\">S3</a> buckets, or <a href=\"https://aws.amazon.com/ec2/\">EC2</a> instances.</p>\n\n<p>In the <code>aws_iam_policy_document</code>, define a condition that evaluates the OIDC subject claim for HCP Terraform organization, project, workspace, and run phase. The subject claim in the example searches for specific organization, project, and workspace. However, you can make the claim more flexible by using wildcards (*), such as <code>organization:ORG_NAME:project:PROJECT_NAME:workspace:*:run_phase:*</code>. </p>\n\n<p>This claim allows for matching of all workspaces and run phases within a specific HCP Terraform project and organization, which can be helpful in scenarios like using <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/no-code-provisioning\">HCP Terraform’s no-code modules</a> to provide self-service infrastructure, where workspace names may not be known in advance.</p>\n\n<p>Note that wildcards in OIDC subject claims can simplify access policies but introduce potential security risks. To balance flexibility and security, use wildcards carefully. While you can scope claims down to a specific HCP Terraform workspace or run phase for maximum security, wildcards can be used selectively to replace certain values, offering a compromise between granularity and convenience.</p>\n\n<p>You can add additional permissions to an IAM role by using the <code>aws_iam_policy_document</code> data source and the <code>aws_iam_policy</code> resource. See the example below:</p>\n<pre><code>data \"aws_iam_policy\" \"s3_full_access\" {\n  arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n}\n\nresource \"aws_iam_role_policy_attachment\" \"example_s3_full_access\" {\n  policy_arn = data.aws_iam_policy.s3_full_access.arn\n  role       = aws_iam_role.example.name\n}</code></pre><h2>Using OIDC federation</h2>\n\n<p>When using OIDC federation, apart from the region argument, you don’t need to include any authentication configuration within the provider block. As long as you set up the correct environment variables in your workspace—specifically, set <code>TFC_AWS_PROVIDER_AUTH</code> to <code>true</code> and <code>TFC_AWS_RUN_ROLE_ARN</code> to the IAM role <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html\">ARN</a> that HCP Terraform should assume at runtime.</p>\n<pre><code>resource \"tfe_variable\" \"tfc_aws_provider_auth\" {\n  key          = \"TFC_AWS_PROVIDER_AUTH\"\n  value        = \"true\"\n  category     = \"env\"\n  workspace_id = tfe_workspace.example.id\n}\n\nresource \"tfe_variable\" \"tfc_example_role_arn\" {\n  sensitive    = true\n  key          = \"TFC_AWS_RUN_ROLE_ARN\"\n  value        = aws_iam_role.example.arn\n  category     = \"env\"\n  workspace_id = tfe_workspace.example.id\n}</code></pre><p>HCP Terraform will automatically assume the IAM role and inject the temporary credentials for you, using the workspace environment variables, allowing you to focus on creating infrastructure.</p>\n\n<h2>Implementing access management for your AWS organization</h2>\n\n<p>For improved security and scalability, we recommend implementing a pattern where one or more HCP Terraform workspaces inject the IAM role and OIDC provider ARNs into other workspaces using an <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/cloud-multiple-variable-sets#create-variable-sets\">HCP Terraform variable set</a>. This enables the platform/cloud team to create HCP Terraform workspaces with pre-configured AWS authentication, scoped to a specific IAM role and permissions.</p>\n\n<p>Whether you create an OIDC provider per AWS account, per environment, or use a single OIDC provider, providing pre-configured AWS authentication for teams’ HCP Terraform workspace is a win-win for both the platform/cloud team and the teams they enable to work autonomously.</p>\n\n<p>Below is an example configuration that creates a variable set for a specific IAM role and sets two environment variables. HCP Terraform uses these environment variables to assume the IAM role and obtain temporary security credentials at runtime, injecting them into the provider to enable access to any AWS API allowed by the IAM role’s policies.</p>\n\n<p>First, create the variable set:</p>\n<pre><code>resource \"tfe_variable_set\" \"example\" {\n  name         = aws_iam_role.example.name\n  description  = \"OIDC federation configuration for ${aws_iam_role.example.arn}\"\n  organization = \"XXXXXXXXXXXXXXX\"\n}</code></pre><p>Next, set up the required environment variables and link them to the variable set:</p>\n<pre><code>resource \"tfe_variable\" \"tfc_aws_provider_auth\" {\n  key             = \"TFC_AWS_PROVIDER_AUTH\"\n  value           = \"true\"\n  category        = \"env\"\n  variable_set_id = tfe_variable_set.example.id\n}\n\nresource \"tfe_variable\" \"tfc_example_role_arn\" {\n  sensitive       = true\n  key             = \"TFC_AWS_RUN_ROLE_ARN\"\n  value           = aws_iam_role.example.arn\n  category        = \"env\"\n  variable_set_id = tfe_variable_set.example.id\n}</code></pre><p>Finally, share the variable set with another HCP Terraform workspace. This ensures that the targeted workspace receives and uses the environment variables, allowing HCP Terraform to automatically assume the IAM role and inject the temporary security credentials:</p>\n<pre><code>resource \"tfe_workspace_variable_set\" \"example\" {\n  variable_set_id = tfe_variable_set.example.id\n  workspace_id    = \"ws-XXXXXXXXXXXXXXX\"\n}</code></pre><h2>Creating infrastructure from another workspace</h2>\n\n<p>Using the IAM role created earlier in this tutorial, which has been assigned S3 permissions, you can create a bucket  right away within the workspace you’ve delegated access to without needing any additional configuration:</p>\n<pre><code>provider \"aws\" {\n  region = \"us-west-2\"\n}\n\nresource \"aws_s3_bucket\" \"example\" {\n  bucket = \"example\"\n}</code></pre><h2>Learn more about OIDC federation</h2>\n\n<p>For more on how to securely access AWS from HCP Terraform with OIDC federation, check out the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/aws-configuration\">Dynamic Credentials with the AWS Provider</a> and <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">OIDC federation</a> documentation. Find a more complete example of configuring the AWS IAM OIDC identity provider on <a href=\"https://github.com/bschaatsbergen/hcp-terraform-aws-oidc-federation/blob/main/main.tf\">GitHub</a>.</p>",
      "summary": "Securely access AWS from HCP Terraform using OIDC federation, eliminating the need to use access keys.",
      "date_published": "2024-09-05T07:00:00.000Z",
      "author": {
        "name": "Bruno Schaatsbergen"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/new-infra-integrations-github-illumio-palo-alto-networks-tessell-more",
      "url": "https://www.hashicorp.com/blog/new-infra-integrations-github-illumio-palo-alto-networks-tessell-more",
      "title": "New infrastructure integrations with GitHub, Illumio, Palo Alto Networks, Tessell, and more",
      "content_html": "<p>The HashiCorp Infrastructure Lifecycle Management ecosystem of Terraform and Packer continues to expand with new integrations that provide additional capabilities to HCP Terraform, Terraform Enterprise and Community Edition, and HCP Packer users as they provision and manage their cloud and on-premises infrastructure. </p>\n\n<p>Terraform is the world’s <a href=\"https://survey.stackoverflow.co/2024/technology#1-other-tools\">most widely used</a> multi-cloud provisioning product. Whether you&#39;re deploying to Amazon Web Services (AWS), Microsoft Azure, Google Cloud, other cloud and SaaS offerings, or an on-premises datacenter, Terraform can be your single control plane to provision and manage your entire infrastructure.</p>\n\n<p>Packer provides organizations with a single workflow to build cloud and private datacenter images and continuously manage them throughout their lifecycle.</p>\n<img src=https://www.datocms-assets.com/2885/1725376353-tf-integrations-9-24.png alt=Terraform integration vendor logos><h2><strong>Terraform providers</strong></h2>\n\n<p>As part of our focus on helping drive innovation through new Terraform integrations and AI, we recently launched our AI Spotlight Collection on the <a href=\"https://registry.terraform.io/\">HashiCorp Terraform Registry</a>.  Our goal is to accelerate Terraform users’ IT operations and support AIOps implementations by integrating with our AI and ML partners.  Make sure to keep an eye on the spotlight section as we continue to expand our listed offerings in the coming months, and reach out to <a href=\"mailto:technologypartners@hashicorp.com\">technologypartners@hashicorp.com</a> if you have a provider you would like to see listed.</p>\n\n<p>Additionally, we had 16 new verified Terraform providers from 14 different partners over the previous quarter:</p>\n\n<h3><strong>Astronomer.io</strong></h3>\n\n<p>Astronomer.io Astro gives users a single control point to access and manage connections to their data. They released a new <a href=\"https://registry.terraform.io/providers/astronomer/astro/latest\">Astro Terraform provider</a> that allows users to leverage Terraform to programmatically manage Astro infrastructure as an alternative to the Astro CLI, Astro UI, or Astro API.</p>\n\n<h3><strong>Authsignal</strong></h3>\n\n<p>Authsignal is a drop-in digital identity and authentication platform. They released a new <a href=\"https://registry.terraform.io/providers/authsignal/authsignal/latest\">Authsignal provider</a> that allows users to leverage Terraform to manage the Authsignal platform.</p>\n\n<h3><strong>Catchpoint</strong></h3>\n\n<p>Catchpoint offers cloud monitoring and visibility services. They’ve released the <a href=\"https://registry.terraform.io/providers/catchpoint/catchpoint/latest\">Catchpoint provider</a>, to allow users to manage web, API, transaction, DNS, SSL, BGP, Traceroute, and Ping tests through Terraform with minimum configurations.</p>\n\n<h3><strong>Files.com</strong></h3>\n\n<p>Files.com provides unified control and reporting for all the file transfers in their customers business. They have released the new <a href=\"https://registry.terraform.io/providers/Files-com/files/latest\">Files.com Terraform provider</a>, which provides convenient access to the Files.com API via Terraform for managing a users’ Files.com account.</p>\n\n<h3><strong>Illumio</strong></h3>\n\n<p>Illumio develops solutions to help stop attacks and ransomware from spreading with intelligent visibility and microsegmentation. They have released the <a href=\"https://registry.terraform.io/providers/illumio/illumio-cloudsecure/latest\">Illumio-CloudSecure provider</a>, which enables DevOps teams to utilize Terraform to manage Illumio CloudSecure resources.</p>\n\n<h3><strong>incident.io</strong></h3>\n\n<p>incident.io, is a Slack-powered incident management platform that has released the <a href=\"https://registry.terraform.io/providers/incident-io/incident/latest\">incident Terraform provider</a>. The provider allows Terraform to manage configuration such as incident severities, roles, custom fields and more inside of users’ incident.io accounts.</p>\n\n<h3><strong>JetBrains</strong></h3>\n\n<p>JetBrains creates intelligent software development tools that cover all stages of the software development cycle, including IDEs and tools for CI/CD and collaboration. Their new <a href=\"https://registry.terraform.io/providers/JetBrains/teamcity/latest\">TeamCity Terraform provider</a> allows DevOps engineers to initialize the JetBrains TeamCity server and automate its administration via Terraform.</p>\n\n<h3><strong>Juniper Networks</strong></h3>\n\n<p>Juniper Networks engages in the design, development, and sale of products and services for high-performance networks. They’ve released the <a href=\"https://registry.terraform.io/providers/Juniper/mist/latest\">Mist provider</a> to allow Terraform to manage Juniper Mist Organizations. The provider currently focuses on Day 0 and Day 1 operations around provisioning and deployment of the Mist service.  </p>\n\n<h3><strong>Palo Alto Networks</strong></h3>\n\n<p>Palo Alto Networks offers security solutions for on-prem, hybrid, and multi-cloud environments, across the development lifecycle to secure networks, protect cloud applications, and enable the SOC. They have released a new <a href=\"https://registry.terraform.io/providers/PaloAltoNetworks/prismasdwan/0.1.0\">prismasdwan provider</a> that provides resources and data sources to manage and query Prisma SD-WAN related config from Strata Cloud Manager.</p>\n\n<h3><strong>Render</strong></h3>\n\n<p>Render offers a unified cloud to build and run apps and websites with free TLS certificates, global CDN, private networks and auto-deploys from Git. They have released a <a href=\"https://registry.terraform.io/providers/render-oss/render/latest\">Render provider</a> that allows users to leverage Terraform to interact with and manage resources on the Render platform.</p>\n\n<h3><strong>SkySQL</strong></h3>\n\n<p>SkySQL, who brings production-grade capabilities to MariaDB, has released two providers: the <a href=\"https://registry.terraform.io/providers/skysqlinc/skysql-beta/latest\">skysql-beta provider</a> and <a href=\"https://registry.terraform.io/providers/skysqlinc/skysql/latest\">skysql provider</a>. The skysql provider allows customers to manage resources in SkySQL with Terraform and the beta provider allows them to utilize features that are in tech preview.</p>\n\n<h3><strong>Solace</strong></h3>\n\n<p>Solace builds event broker technology, an architectural layer that seamlessly gets events from where they occur to where they need to be across clouds, networks, and applications. They have released two new providers: the <a href=\"https://registry.terraform.io/providers/SolaceProducts/solacebroker/latest\">Solace Broker provider</a> that enables users to configure PubSub+ Software Event Brokers through Terraform; and the <a href=\"https://registry.terraform.io/providers/SolaceProducts/solacebrokerappliance/latest\">Solace Broker Appliance provider</a>, which enables users to configure a PubSub+ Event Broker Appliance using Terraform.</p>\n\n<h3><strong>Styra</strong></h3>\n\n<p>Styra creates and maintains solutions that enable users to define, enforce, and monitor policy across their cloud-native environments with a combination of open source and commercial products. They have released the <a href=\"https://registry.terraform.io/providers/StyraInc/styra/latest\">Styra provider</a> which enables the provisioning and managing of the Enterprise OPA Platform and Open Policy Agent authorization platform itself as Terraform resources, reducing friction and increasing control.</p>\n\n<h3><strong>Tessell</strong></h3>\n\n<p>Tessell is a database-as-a-service (DBaaS) platform that simplifies the management, security, and scalability of relational databases in the cloud. They have released the <a href=\"https://registry.terraform.io/providers/tessell-cloud/tessell/latest\">Tessell provider</a>, that allows users to integrate with the Tessell API and manage resources including: Database services across public clouds (AWS, Azure), database engines (Oracle, PostgreSQL, MySQL, SQL Server), Availability Machines for data protection (snapshots), secondary environments (sanitized snapshots), as well as the creation of database service clones.</p>\n\n<h2><strong>HCP Packer integrations</strong></h2>\n\n<p>HCP Packer, which standardizes and automates the process of governing, tracking, and auditing image artifacts, has two new integrations.  </p>\n\n<h3><strong>GitHub</strong></h3>\n\n<p>GitHub is a developer platform that allows developers to create, store, manage and share their code. The new <a href=\"https://github.com/marketplace/actions/setup-hashicorp-packer\">HCP Packer integration</a> ties Packer pipeline metadata with GitHub Actions.  This integration will enable users to build images in a GitHub Actions pipeline and then find the details of the pipeline including pipeline ID/name, workflow identifiers, job names, and runner environment details. This automation and tracking is crucial for establishing software supply chain security, auditing machine images, and creating containers. </p>\n\n<h3><strong>GitLab</strong></h3>\n\n<p>GitLab is a single application for the whole software development and operations lifecycle. The new HCP Packer integration ties HCP Packer pipeline metadata with GitLab pipelines, allowing users to track information including pipeline specifics and associated pull requests. </p>\n\n<h2><strong>Learn more about Terraform integrations</strong></h2>\n\n<p>All these integrations are available for use in the <a href=\"https://registry.terraform.io/\">HashiCorp Terraform Registry</a>. If you are a technology provider and want to verify an existing integration, please refer to our <a href=\"https://developer.hashicorp.com/terraform/docs/partnerships\">Terraform Integration Program</a>.</p>\n\n<p>If you haven’t already, try the <a href=\"https://app.terraform.io/public/signup/account\">free tier of HCP Terraform</a> to help simplify your Terraform workflows and management.</p>",
      "summary": "18 new Terraform and Packer integrations from 16 partners provide more options to automate and secure cloud infrastructure management.",
      "date_published": "2024-09-03T16:00:00.000Z",
      "author": {
        "name": "Tom O’Connell"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-enterprise-improves-deployment-flexibility-with-nomad-and-openshift",
      "url": "https://www.hashicorp.com/blog/terraform-enterprise-improves-deployment-flexibility-with-nomad-and-openshift",
      "title": "Terraform Enterprise improves deployment flexibility with Nomad and OpenShift",
      "content_html": "<p><a href=\"https://developer.hashicorp.com/terraform/enterprise\">HashiCorp Terraform Enterprise</a> is the self-hosted distribution of HCP Terraform for customers with strict regulatory, data residency, or air-gapped networking requirements.  The latest Terraform Enterprise releases provide more flexible deployment options for customers with support for deployment on two new application platforms: HashiCorp Nomad and Red Hat OpenShift.</p>\n\n<h2>New deployment runtime options</h2>\n\n<p>In September 2023, we introduced new <a href=\"https://www.hashicorp.com/blog/terraform-enterprise-adds-new-flexible-deployment-options\">flexible deployment options for Terraform Enterprise</a>, with initial support for Docker Engine and cloud-managed Kubernetes services (Amazon EKS, Microsoft AKS, and Google GKE). These options were extended in the April 2024 release with the <a href=\"https://www.hashicorp.com/blog/terraform-enterprise-adds-podman-support-and-workflow-enhancements\">addition of Podman support</a>.</p>\n\n<h3>HashiCorp Nomad</h3>\n\n<p><a href=\"https://www.hashicorp.com/products/nomad\">HashiCorp Nomad</a> is a modern and efficient application scheduler for containers, binaries, and virtual machines. With the August 2024 (<a href=\"https://developer.hashicorp.com/terraform/enterprise/releases/2024/v202408-1\">v202408-1</a>) release of Terraform Enterprise, we are excited to add Nomad as a fully supported runtime environment. Nomad Enterprise customers also benefit from direct HashiCorp support for their Terraform Enterprise deployment and its runtime.</p>\n\n<p>The quickest way to deploy Terraform Enterprise on a Nomad cluster is to use <a href=\"https://developer.hashicorp.com/nomad/tutorials/job-specifications/nomad-pack-intro\">Nomad Pack</a>, the templating and packaging tool for Nomad, with the <a href=\"https://github.com/hashicorp/nomad-pack-community-registry/tree/main/packs/tfe_fdo_nomad\">Nomad Pack for Terraform Enterprise</a>. For more information, visit the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/nomad\">Nomad installation page in the Terraform Enterprise documentation</a>.</p>\n\n<h3>Red Hat OpenShift</h3>\n\n<p><a href=\"https://www.redhat.com/en/technologies/cloud-computing/openshift\">Red Hat OpenShift</a> is a Kubernetes application platform popular among enterprises due to its pre-packaged operational extensions, strong default security posture, and commercial support options. As of the July 2024 (<a href=\"https://developer.hashicorp.com/terraform/enterprise/releases/2024/v202407-1\">v202407-1</a>) release, Terraform Enterprise is supported for deployment on OpenShift clusters. This includes self-managed OpenShift Container Platform environments and hosted OpenShift services on Amazon Web Services (AWS ROSA), Microsoft Azure, Google Cloud, and IBM Cloud.</p>\n\n<p>Deploying Terraform Enterprise on OpenShift is similar to the existing Kubernetes options, with the addition of the <code>openshift.enabled</code> parameter in the Helm chart to support OpenShift security context requirements. To learn more, refer to the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/openshift\">Operate on Red Hat OpenShift documentation</a>.</p>\n\n<h2>Migration from Replicated-based installs</h2>\n\n<p>Customers still running a Replicated deployment of Terraform Enterprise are strongly encouraged to <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/replicated-migration\">migrate to one of the new flexible deployment options</a>. The final Replicated release of Terraform Enterprise is scheduled for November 2024. While HashiCorp Support will accept cases for this release until April 1, 2026, migrating by November ensures organizations will continue to receive the latest features and fixes.</p>\n\n<p>As of the August 2024 release, the flexible deployment options for Terraform Enterprise include:</p>\n\n<ul>\n<li>Docker Engine on any supported Linux distribution</li>\n<li>Podman on Red Hat Enterprise Linux 8 or 9</li>\n<li>Cloud-managed Kubernetes: Amazon EKS, Microsoft AKS, and Google GKE</li>\n<li>Red Hat OpenShift</li>\n<li>HashiCorp Nomad</li>\n</ul>\n\n<p>If you’re unsure which deployment option to adopt before November, HashiCorp recommends the Docker Engine option. Customers running Replicated already have Docker Engine installed on their host, so you’re already halfway there. To migrate, generate a Docker Compose file from your current configuration, stop Replicated, and start the new <code>terraform-enterprise</code> container. No data migrations are necessary. Check out the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy/replicated-migration#mounted-disk-to-docker\">Replicated migration guides</a> for step-by-step instructions.</p>\n\n<p>Customers can contact their HashiCorp representative for more information and to validate their migration and upgrade path.</p>\n\n<h2>Migration service offering</h2>\n\n<p>Looking for hands-on support for your migration off of Replicated? The HashiCorp Professional Services team can help you through this process from design to execution. Ask your account representative for more information on migration services to a new deployment option or to HCP Terraform.</p>\n\n<h2>Other recent Terraform Enterprise highlights</h2>\n\n<p>The last few Terraform Enterprise monthly releases brought new enhancements and features to improve efficiency and flexibility, including:</p>\n\n<ul>\n<li>A new dedicated <a href=\"https://www.hashicorp.com/blog/terraform-cloud-improves-visibility-and-control-for-projects#project-overview-page\">project overview page</a> in the UI (v202405-1) to make viewing and managing your organization’s projects more efficient</li>\n<li>New granular <a href=\"https://www.hashicorp.com/blog/terraform-improves-permissions-management-for-teams\">permissions to delegate team management</a> to non-owners (v202405-1)</li>\n<li>An <a href=\"https://developer.hashicorp.com/terraform/enterprise/users-teams-organizations/organizations#runs\">organization-level runs page</a> has been added in v202406-1 to view and cancel the current active run for all workspaces in an organization</li>\n<li><a href=\"https://www.hashicorp.com/blog/terraform-adds-a-new-setting-to-manage-team-tokens\">Improved control over team API token management</a> (v202408-1) using a new setting to control whether the members of a team can view or manage their team’s API token</li>\n</ul>\n\n<h2>Upgrade now</h2>\n\n<p>To learn more about the deployment options for Terraform Enterprise, review the <a href=\"https://developer.hashicorp.com/terraform/enterprise/deploy\">installation overview documentation</a>. To catch up on everything new and changed in recent Terraform Enterprise versions, check out the <a href=\"https://developer.hashicorp.com/terraform/enterprise/releases\">release notes</a>.</p>\n\n<p>To learn more about standardizing the infrastructure lifecycle with Terraform, explore our hosted and self-managed delivery options by visiting the <a href=\"https://www.hashicorp.com/products/terraform\">Terraform product page</a> or <a href=\"https://www.hashicorp.com/contact-sales\">contacting HashiCorp sales</a>.</p>",
      "summary": "Customers can now deploy Terraform Enterprise using Red Hat OpenShift or HashiCorp Nomad runtime platforms.",
      "date_published": "2024-08-29T16:00:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-provider-for-google-cloud-6-0-is-now-ga",
      "url": "https://www.hashicorp.com/blog/terraform-provider-for-google-cloud-6-0-is-now-ga",
      "title": "Terraform provider for Google Cloud 6.0 is now GA",
      "content_html": "<p>We are excited to announce the release of version 6.0 of the <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest\">HashiCorp Terraform Google provider,</a> with updates to <em>labels</em> designed to improve usability. Users now have the ability to view resources managed by Terraform when viewing/editing such resources through other tools. This post covers the details and benefits of the updated provider, and recaps key new features released this year.</p>\n\n<h2>2024 Terraform Google provider highlights</h2>\n\n<p>As the Terraform Google provider tops 300 million downloads this year, Google and HashiCorp continue to develop new integrations to help customers work faster, get benefits from more services and features, and find developer-friendly ways to deploy cloud infrastructure. This year, we focused on listening to the community by adding oft-requested new features to the Google provider, including:</p>\n\n<ul>\n<li>Provider-defined functions</li>\n<li>Default attribution label</li>\n<li>Expanding labels model support across more resources</li>\n</ul>\n\n<h2>Provider-defined functions</h2>\n\n<p>With the release of <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">Terraform 1.8</a>, providers can implement custom functions that you can call from the Terraform configuration. Earlier this year we announced the <a href=\"https://developer.hashicorp.com/terraform/plugin/framework/functions/concepts\">general availability of provider-defined functions in the Google Cloud provider</a>, adding a simplified way to get regions, zones, names, and projects from the IDs of resources that aren’t managed by your Terraform configuration. Provider-defined functions can now help parse Google IDs when adding an IAM binding to a resource that’s managed outside of Terraform:</p>\n<pre><code>resource \"google_cloud_run_service_iam_member\" \"example_run_invoker_jane\" {\n\n member   = \"user:jane@example.com\"\n\n role     = \"run.invoker\"\n\n service  = provider::google::name_from_id(var.example_cloud_run_service_id)\n\n location = provider::google::location_from_id(var.example_cloud_run_service_id)\n\n project  = provider::google::project_from_id(var.example_cloud_run_service_id)\n\n}</code></pre><p>This release represents another step forward in our unique approach to ecosystem extensibility.</p>\n\n<h2>Default Terraform attribution label</h2>\n\n<p>In version 5.16 of the Google provider, a new optional <code>goog-terraform-provisioned</code> provider-level default label helps users track resources created by Terraform. Previously, users had to explicitly opt into this feature by setting the <code>add_terraform_attribution_label</code> option in the provider configuration block. In version 6.0, this attribution label is now enabled by default, and will be added to all newly created resources that support labels. This helps users easily identify and report on resources managed by Terraform when viewing/editing such resources through tools like Google Cloud Console, Cloud Billing, etc.</p>\n\n<p>Users who wish to opt out of this new default label can do so by disabling the <code>add_terraform_attribution_label</code> option in the provider block:</p>\n<pre><code>provider \"google\" {\n  # Opt out of the “goog-terraform-provisioned” default label\n  add_terraform_attribution_label = false\n}</code></pre><p>By default, the label is added to resources only upon creation. To proactively apply the label to existing resources, set the <code>terraform_attribution_label_addition_strategy</code> option to <code>PROACTIVE</code> in the provider block, which adds the label to all supported resources on the next <code>terraform apply</code>:</p>\n<pre><code>provider \"google\" {\n  # Apply the “goog-terraform-provisioned” label to existing resources\n  add_terraform_attribution_label               = true\n  terraform_attribution_label_addition_strategy = “PROACTIVE”\n}</code></pre><h2>Removing deprecated attributes and other behavior changes</h2>\n\n<p>Since the last major release, the Terraform Google provider has accumulated resources and properties that have been deprecated, renamed, or are no longer supported by Google. As version 6.0 is a major release, we have removed a number of resources and attributes that have been deprecated over the course of the provider’s lifetime. A complete list of behavior changes and removed properties can be found in the <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/version_6_upgrade\">Google 6.0 upgrade guide</a>.</p>\n\n<h2>Learn more about Google Cloud and HashiCorp</h2>\n\n<p>To learn the basics of Terraform using the Google provider, check out the <a href=\"https://developer.hashicorp.com/terraform/tutorials/gcp-get-started\">Get Started tutorials for Google Cloud</a>.</p>\n\n<p>When upgrading to version 6.0 of the Terraform Google provider, please consult the <a href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/version_6_upgrade\">upgrade guide on the Terraform Registry</a>, which contains a full list of the changes and upgrade considerations. Because this release introduces breaking changes, we recommend <a href=\"https://developer.hashicorp.com/terraform/language/providers/requirements#best-practices-for-provider-versions\">pinning your provider version</a> to protect against unexpected results. For a complete list of the changes in 6.0, please refer to the <a href=\"https://github.com/hashicorp/terraform-provider-google/releases/tag/v6.0.0\">Google provider changelog</a>.</p>\n\n<p>HashiCorp and Google partner on cloud infrastructure to make it easy for users to provision and manage Google Cloud resources. You can find out more about our partnership on our <a href=\"https://www.hashicorp.com/partners/cloud/google-cloud\">Google Cloud partner page</a>.</p>\n\n<p>If you are currently using Terraform Community Edition or are completely new to Terraform, <a href=\"http://hashi.co/tf-cloud-bc\">sign up for HCP Terraform</a> and get started using the free offering today.</p>",
      "summary": "Version 6.0 of the HashiCorp Terraform Google provider brings updates to default labels, letting practitioners view and edit resources with ease.",
      "date_published": "2024-08-27T13:00:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-azurerm-provider-4-0-adds-provider-defined-functions",
      "url": "https://www.hashicorp.com/blog/terraform-azurerm-provider-4-0-adds-provider-defined-functions",
      "title": "Terraform AzureRM provider 4.0 adds provider-defined functions",
      "content_html": "<p>Today, we are announcing the general availability of the HashiCorp <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest\">Terraform AzureRM provider 4.0</a>. This version includes new capabilities to improve the extensibility and flexibility of the provider: provider-defined functions and improved resource provider registration. Initially launched in <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">April 2024</a>, provider-defined functions allow anyone in the Terraform community to build custom functions within providers to extend the capabilities of Terraform. </p>\n\n<p>This post reviews the details and benefits of this new major version of the provider and also covers a handful of new features released this year.</p>\n\n<h2>2024 AzureRM provider highlights</h2>\n\n<p>Since the provider’s last major release in March 2022, we’ve added support for some 340 resources and 120 data sources, bringing the totals to more than 1,101 resources and almost 360 data sources as of mid-August, 2024. As the Terraform AzureRM provide<a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs\">r</a> download count tops 660 million, Microsoft and HashiCorp continue to develop new, innovative integrations that further ease the cloud adoption journey for enterprise organizations. This year we focused on improving the user experience for practitioners by adding new services to the AzureRM provider including:</p>\n\n<ul>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#provider-functions\">Provider-defined functions</a></li>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#improved-resource-provider-registration\">Improved resource provider registration</a></li>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#additional-properties-for-subnets-in-the-virtual-network-resource\">Additional properties for subnets in the virtual network resource</a></li>\n</ul>\n\n<h2>Provider-defined functions</h2>\n\n<p>With the release of <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">Terraform 1.8</a> in April, providers can implement custom functions that you can call from the Terraform configuration. The latest release of the Terraform AzureRM provider adds two Azure-specific provider functions to let users correct the casing of their resource IDs, or to access the individual components of it.</p>\n\n<p>The <code>normalise_resource_id</code> function attempts to normalize the case-sensitive system segments of a resource ID as required by the Azure APIs:.</p>\n<pre><code>output \"test\" {\n value = provider::azurerm::normalise_resource_id(\"/Subscriptions/12345678-1234-9876-4563-123456789012/ResourceGroups/resGroup1/PROVIDERS/microsoft.apimanagement/service/service1/gateWays/gateway1/hostnameconfigurations/config1\")\n}\n\n# Result: /subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/resGroup1/providers/Microsoft.ApiManagement/service/service1/gateways/gateway1/hostnameConfigurations/config1</code></pre><p>The <code>parse_resource_id</code> function takes an Azure resource ID and splits it into its component parts:</p>\n<pre><code>locals {\n parsed_id = provider::azurerm::parse_resource_id(\"/subscriptions/12345678-1234-9876-4563-123456789012/resourceGroups/resGroup1/providers/Microsoft.ApiManagement/service/service1/gateways/gateway1/hostnameConfigurations/config1\")\n}\n\noutput \"resource_group_name\" {\n value = local.parsed_id[\"resource_group_name\"]\n}\n\noutput \"resource_name\" {\n value = local.parsed_id[\"resource_name\"]\n}\n\n# Result:\n# Outputs:\n# \n# resource_group_name = \"resGroup1\"\n# resource_name = \"config1\"</code></pre><h2>Improved resource provider registration</h2>\n\n<p>Previously, the AzureRM provider took an all-or-nothing approach to Azure resource provider registration, where the Terraform provider would either attempt to register a fixed set of 68 providers upon initialization or registration could be skipped entirely by setting <code>skip_provider_registration = true</code> in the provider block. This limitation didn’t match Microsoft’s recommendations, which is to register resource providers only as needed to enable the services you’re actively using. With the addition of two new feature flags, <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#resource_provider_registrations\">resource<em>provider</em>registrations</a> and <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#resource_providers_to_register\">resource<em>providers</em>to_register</a>, users now have more control over which providers to automatically register or whether to continue managing a subscription’s resource provider registrations outside of Terraform.</p>\n\n<h2>Changes and deprecations</h2>\n\n<p>Since the last major release, the AzureRM provider has accumulated resources and properties that have been deprecated, renamed, or are no longer supported by Azure. As version 4.0 is a major release, we have removed a number of <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#behaviour-changes-and-removed-properties-in-resources\">resources</a> and <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#behaviour-changes-and-removed-properties-in-data-sources\">data sources</a> that have been deprecated over the course of the provider’s lifetime. A complete list of behavior changes and removed properties can be found in the <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide#behaviour-changes-and-removed-properties-in-resources\">AzureRM provider 4.0 upgrade guide</a>.</p>\n\n<h2>Learn more about Microsoft and HashiCorp</h2>\n\n<p>The latest version of the AzureRM provider is available today. These features and enhancements will help simplify configurations and improve the overall experience of using the provider. Because this release introduces breaking changes, we recommend pinning your provider version to protect against unexpected results. For a complete list of the changes in 4.0, please review the <a href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/4.0-upgrade-guide\">AzureRM provider upgrade guide</a>. </p>\n\n<p>Please share any bugs or enhancement requests with us via <a href=\"https://github.com/hashicorp/terraform-provider-azurerm/issues\">GitHub issues</a>. We are thankful to our partners and community members for their valuable contributions to the HashiCorp Terraform ecosystem.</p>",
      "summary": "Version 4.0 of the HashiCorp Terraform AzureRM provider brings support for provider-defined functions and improved resource provider registration.",
      "date_published": "2024-08-22T16:00:00.000Z",
      "author": {
        "name": "Aurora Chun"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-extension-for-vs-code-speeds-up-loading-of-large-workspaces",
      "url": "https://www.hashicorp.com/blog/terraform-extension-for-vs-code-speeds-up-loading-of-large-workspaces",
      "title": "Terraform extension for VS Code speeds up loading of large workspaces",
      "content_html": "<p>We are excited to announce that version 0.34 of the HashiCorp <a href=\"https://github.com/hashicorp/terraform-ls\">Terraform language server</a>, bundled with version 2.32 of the Terraform extension for Visual Studio Code, is now available. This latest iteration brings significant reductions in initial work and memory usage when opening workspaces. Additionally, version 0.34 of the language server introduces parallel loading of Terraform language constructs, which enables instantaneous autocompletion. This blog post highlights the new enhancements and the results of the improvements.</p>\n\n<h2>Performance with large Terraform workspaces</h2>\n\n<p>The Terraform language server provides IDE features in <a href=\"https://microsoft.github.io/language-server-protocol/\">LSP</a>-compatible editors like <a href=\"https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform\">Visual Studio Code</a>, <a href=\"https://www.sublimetext.com/\">Sublime Text</a>, <a href=\"https://neovim.io/\">Neovim</a>, and others. With previous versions of the Terraform language server, the initial loading experience of large and/or complex Terraform configurations could be time-consuming and resource-intensive. That’s because when opening the editor, the Terraform language server did a lot of work in the background to understand the code being worked on. </p>\n\n<p>Its indexing process finds all the Terraform files and modules in the current working directory, parses them, and builds an understanding of all the interdependencies and references. It holds this information inside an in-memory database, which is updated as files are changed. If a user opened a directory with many hundreds of folders and files, it would consume more CPU and memory than they expected.</p>\n\n<h2>Improved language server performance</h2>\n\n<p>Improving the efficiency and performance of the Terraform language server has been a frequent request from the Terraform community. To address the issue, we separated the LSP language features for several Terraform constructs:</p>\n\n<ul>\n<li><strong>Modules:</strong> This feature handles everything related to *.tf and *.tf.json files.</li>\n<li><strong>Root modules:</strong> This feature handles everything related to provider and module installation and lock files.</li>\n<li><strong>Variables:</strong> This handles everything related to *.tfvars and *.tfvars.json files.</li>\n</ul>\n\n<p>Splitting the existing language-related functionality into multiple, smaller, self-contained language features lets the server process the work related to the different constructs in parallel. At the same time, we were able to reduce the amount of work a feature does at startup and shift the work to a user&#39;s first interaction with a file.</p>\n\n<p>In addition, the language server now parses and decodes only the files a user is currently working with, instead of fetching the provider and module schemas for the entire workspace at startup. The indexing process begins only when a user later opens a file in a particular folder.</p>\n\n<p>This new process brings a significant reduction (up to 99.75%) in memory usage and startup time when opening a workspace. For example, we measured a workspace with 5,296 lines of code that previously took 450ms to open and consumed 523 MB of memory. After updating to the 0.34 language server and the 2.32 VS Code extension, open-time dropped to 1.4ms and only 1.6 MB of memory was consumed. The new process also reduces memory use and cuts startup time when opening files within a workspace. That’s because instead of keeping the schemas for everything in memory, Terraform now has only the schemas for the currently open directory.</p>\n<img src=https://www.datocms-assets.com/2885/1721838250-workspace_open.png alt=Startup and memory use reductions><h2>Summary and resources</h2>\n\n<p>Enhancements to the HashiCorp Terraform extension for Visual Studio Code and Terraform language server are available today. If you&#39;ve previously encountered problems with language server performance but have not yet tried these updates, we encourage you to check them out and share any bugs or enhancement requests with us via <a href=\"https://github.com/hashicorp/vscode-terraform/issues/new/choose\">GitHub issues</a>. Learn more by reading the <a href=\"https://github.com/hashicorp/terraform-ls/pull/1667\">LS state &amp; performance refactoring pull request details on GitHub</a>.</p>\n\n<p>If you are currently using Terraform Community Edition or are completely new to Terraform, <a href=\"http://hashi.co/tf-cloud-bc\">sign up for HCP Terraform</a> and get started using the free offering today.</p>",
      "summary": "New releases of the HashiCorp Terraform extension for Visual Studio Code and Terraform language server significantly reduce memory usage and start up time for large workspaces.",
      "date_published": "2024-07-24T16:00:00.000Z",
      "author": {
        "name": "Aurora Chun"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/why-use-vault-backed-dynamic-credentials-to-secure-hcp-terraform-infrastructure",
      "url": "https://www.hashicorp.com/blog/why-use-vault-backed-dynamic-credentials-to-secure-hcp-terraform-infrastructure",
      "title": "Why use Vault-backed dynamic credentials to secure HCP Terraform infrastructure?",
      "content_html": "<p>Many Terraform users still rely on static credentials (API keys, passwords, certificates, etc.) to authenticate workloads with cloud providers (AWS, Google Cloud, Azure). However, relying on this practice poses both operational and security challenges. Managing static, long-lived credentials does not scale well without tedious and time-consuming manual intervention. Additionally, users set credentials as workspace variables or variable sets in Terraform, adding additional complexity to their authentication process. </p>\n\n<p>This practice of manually securing static secrets only increases the likelihood of secrets leakage and <a href=\"https://www.hashicorp.com/resources/what-is-secret-sprawl-why-is-it-harmful\">secrets sprawl</a>, in which credentials end up scattered across multiple databases, clouds, and applications.</p>\n\n<p><a href=\"https://www.hashicorp.com/lp/vault-p?utm_source=google&utm_channel_bucket=paid&utm_medium=sem&utm_campaign=&utm_content=hashicorp%20vault%20cloud-161229712449-702136766602&utm_offer=&gad_source=1&gclid=CjwKCAjwps-zBhAiEiwALwsVYcbQ_eMaKOqbB2evCbYCiBHNJpDjDQXaKMUyYYUiHKEtHFkJaRmUVRoCGxkQAvD_BwE\">HashiCorp Vault</a> provides a secure and centralized repository for storing these secrets, eliminating the need to hardcode them within Terraform configurations. Vault also provides management and control over identity access management (IAM) policies that applications need to work with other applications and services. Using an API-first approach to authenticate all requests, Vault provides secure access only to authorized resources. </p>\n\n<p>One of the key features that Vault offers HCP Terraform users is <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/vault-backed\">Vault-backed dynamic credentials</a>. This feature provides a workflow to auto-generate and revoke secrets/credentials when they are no longer needed.  </p>\n\n<p>This blog will explain why securing your infrastructure with <a href=\"https://www.hashicorp.com/blog/why-we-need-dynamic-secrets\">dynamic secrets</a> through Vault-backed dynamic credentials is the most secure way to use Terraform and why it should be the new standard for your security roadmap.</p>\n\n<h2>What are Vault-backed dynamic credentials?</h2>\n\n<p>Vault-backed dynamic credentials are temporary, time-bound, and unique to each Terraform workload. When you adopt HashiCorp Vault as your secrets manager, Vault’s <a href=\"https://developer.hashicorp.com/vault/docs/secrets\">secrets engines</a> can connect with HCP Terraform and Terraform Enterprise’s <a href=\"https://www.hashicorp.com/blog/terraform-cloud-adds-vault-backed-dynamic-credentials\">dynamic provider credentials</a> feature to generate and manage dynamic credentials directly from Vault. </p>\n\n<p>The dynamic provider credentials feature automatically generates the API keys you need to access and build infrastructure for AWS, Microsoft Azure, and Google Cloud. It does this just-in-time with each provisioning run for one-time use. These dynamic credentials do not require manual rotation or revocation when they are no longer needed. </p>\n\n<p>By making credentials unique and used for one run only, teams drastically reduce the chance that they might be found by attackers, and eliminate the chance of long-term malicious access. </p>\n\n<h3>Standards-based</h3>\n\n<p>Drilling down into the security details of this feature, Vault-backed dynamic credentials strongly protect access workflows during infrastructure provisioning by leveraging workload identity and the <a href=\"https://en.wikipedia.org/wiki/OpenID#OpenID_Connect_(OIDC)\">OpenID Connect (OIDC)</a> standard. Before provisioning, operators must first configure Terraform as a trusted identity provider with Vault (or their cloud infrastructure providers in the case of basic dynamic provider credentials). HCP Terraform or Terraform Enterprise then generate a signed identity token for every workload to obtain single-run credentials that are injected into the run environment. This exchange happens automatically for the supported providers by adding a few simple environment variables to the workspace.</p>\n<img src=https://www.datocms-assets.com/2885/1721419685-oidc-vault-backed-dynamic-creds-hcp-tf.png alt=Vault-backed dynamic credentials flow><h2>Why use Vault-backed dynamic credentials?</h2>\n\n<p>Vault-backed dynamic credentials include several advantages over using only dynamic provider credentials without Vault:</p>\n\n<ul>\n<li>Consolidated management and auditing for all your cloud credentials and other secrets</li>\n<li>No OIDC setup required in your cloud provider</li>\n<li>Leverage Vault secrets engine configurations</li>\n<li>No need to expose inbound access to self-hosted Terraform Enterprise instances from cloud providers to validate OIDC metadata.</li>\n</ul>\n\n<h3>Consolidated management and auditing</h3>\n\n<p>Without being “Vault-backed”, dynamic provider credentials are still a step in the right direction to make your secrets management dynamic rather than static. But there isn’t as much auditing or management capability without the Vault integration. Secrets management is firmly in the purview of Vault, while Terraform is focused on provisioning. By using Vault-backed dynamic credentials instead of dynamic provider credentials without Vault, teams are able to logically consolidate Terraform credential management with all of the other secrets managed throughout the organization on one platform.</p>\n\n<p>Unlike static credentials, dynamic credentials are most effectively utilized within a secrets management platform, such as <a href=\"https://developer.hashicorp.com/vault/tutorials/get-started-hcp-vault-dedicated/vault-introduction\">HCP Vault Dedicated</a>, that automates their lifecycle. HCP Vault Dedicated can automatically generate temporary secrets as required and integrate dynamic secrets into infrastructure automation tools such as HCP Terraform.</p>\n\n<h3>No OIDC setup required in cloud provider</h3>\n\n<p>By setting up an <a href=\"https://developer.hashicorp.com/vault/tutorials/auth-methods/oidc-auth\">OIDC flow</a> from HCP Terraform to Vault instead of a cloud provider, teams can own the full security lifecycle process from authentication to secret generation. This lets teams use the more sophisticated feature set of Vault when managing dynamic provider credentials and reduce the surface area of security configurations required for all workloads.</p>\n\n<h3>Leverage Vault secrets engine configurations</h3>\n\n<p>Vault-backed dynamic credentials leverage Vault’s authentication and authorization capabilities to limit permissions based on metadata like the execution phase, workspace, or organization involved in the operation. Security teams already well-versed in configuring Vault policies and mapping workloads to cloud roles can use their existing workflows to authorize Terraform runs, saving time and effort.</p>\n\n<p>Another security benefit of Vault-backed dynamic credentials is the Vault token, which is revoked immediately after the plan or application runs. This means the cloud credentials are also immediately invalidated and cannot be re-used, as opposed to waiting for a fixed time-to-live to expire.</p>\n\n<h3>Protected inbound access</h3>\n\n<p>The OIDC workflow requires two-way communication so that the identity provider can validate the signature and metadata of the workload identity token presented by the client. For self-hosted Terraform Enterprise customers using standard dynamic provider credentials to authenticate directly to a cloud provider, <a href=\"https://www.hashicorp.com/blog/terraform-cloud-adds-vault-backed-dynamic-credentials\">inbound network access must be allowed</a> from the provider. Since the cloud providers don’t document the specific IP addresses used for OIDC integrations, this effectively means exposing Terraform Enterprise to the public internet.</p>\n\n<p>Instead, with Vault-backed dynamic credentials, only the Vault instance needs to directly access the metadata endpoints. Vault’s secret engines then use outbound-only connections to the cloud provider.</p>\n\n<h2>How do I start using Vault-backed dynamic credentials?</h2>\n\n<p>If you’re new to Vault, <a href=\"https://developer.hashicorp.com/vault\">start here</a> to try it out and see benefits quickly. If you’re already familiar with Vault, and have it set up in your organization, start by reading the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/dynamic-provider-credentials/vault-backed\">Vault-backed dynamic provider credentials documentation</a> to learn how to set up the feature. Then continue with a hands-on tutorial: <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/dynamic-credentials-vault\">Authenticate providers with Vault-backed dynamic credentials</a>.</p>\n\n<p>If you’re having trouble setting up Vault, or you just don’t have the time to self-manage and maintain an on-premises instance, let us manage it for you by signing up for a free HCP Vault Dedicated trial. Either of these two services are the easiest and fastest way to get started with Vault. You can also <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/users#log-in-with-your-hashicorp-cloud-platform-account\">link your HCP Terraform and HashiCorp Cloud Platform (HCP) accounts</a> together for a seamless sign-in experience.</p>",
      "summary": "Learn how HCP Terraform and Terraform Enterprise users can use Vault-backed dynamic credentials to secure their infrastructure during provisioning better than the base-level dynamic provider credentials.",
      "date_published": "2024-07-23T16:00:00.000Z",
      "author": {
        "name": "Sam Pandey"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/hcp-terraform-adds-granular-api-access-for-audit-trails",
      "url": "https://www.hashicorp.com/blog/hcp-terraform-adds-granular-api-access-for-audit-trails",
      "title": "HCP Terraform adds granular API access for audit trails",
      "content_html": "<p>Today we’d like to share the latest improvement to HCP Terraform’s permissions capabilities: <em>read-only permission to the HCP Terraform audit trails endpoint</em>. Available now in HCP Terraform, this new feature enables organization owners to generate a dedicated API key for least-privilege access to audit trails.</p>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/cloud-docs/api-docs/audit-trails\">HCP Terraform audit trails</a> let organization administrators quickly review the actions performed by members of their organization. It includes details such as who performed the action, what the action was, and when it was performed. It also contains the evaluation results of compliance-related features like <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/policy-enforcement\">policy enforcement</a> and <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/settings/run-tasks\">run tasks</a>. When paired with the <a href=\"https://splunkbase.splunk.com/\">Splunk app</a> it provides near real-time visibility into key actions. You can quickly see which workspaces are generating the most frequent changes, which policies are being evaluated most frequently, and which users are most active.</p>\n\n<p>In the past, within HCP Terraform, organization owners were required to create an <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/api-tokens#organization-api-tokens\">organization API token</a> to grant access to the audit trail endpoint. However, the excessive permissions associated with this token meant users had to vigilantly protect these credentials.</p>\n\n<h2>The new audit token for HCP Terraform audit trails</h2>\n\n<p>The new <em>audit token</em> type simplifies and enhances privilege management within organizations by letting owners adhere to the principle of least privilege access. This type allows read-only access to the HCP Terraform audit trail endpoint. By incorporating token expiration, organization owners gain complete control over the token&#39;s entire lifecycle, letting them specify when the audit token should expire. Users also now have the capability to effortlessly regenerate the token, which is particularly useful in situations where token rotation is required following a security incident. This advancement eliminates the need for users to possess owner-level access or manage the highly privileged organization API token.</p>\n\n<h2>Creating an audit token</h2>\n\n<p>To create an audit token, navigate to the API Tokens section within the Organization Settings page. Click the <em>Generate an audit token</em> button and configure the expiration settings as needed.</p>\n<img src=https://www.datocms-assets.com/2885/1721403524-token_setting.gif alt=audit trails API read-only><h2>Getting started</h2>\n\n<p>This feature is now available in HCP Terraform. Please refer to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/api-tokens\">Terraform’s API token documentation</a> for details on how to get started.</p>\n\n<p>If you are new to Terraform, you can get started with HashiCorp-managed <a href=\"https://cloud.hashicorp.com/products/terraform\">HCP Terraform</a> for free to begin provisioning and managing your infrastructure in any environment. And don’t forget to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/users#log-in-with-your-hashicorp-cloud-platform-account\">link your HCP Terraform and HashiCorp Cloud Platform (HCP) accounts</a> for a seamless sign-in experience.</p>",
      "summary": "HCP Terraform eliminates the need to rely on organization permissions to the audit trails endpoint, streamlining permissions workflows and reducing risk.",
      "date_published": "2024-07-22T16:00:00.000Z",
      "author": {
        "name": "Ryan Hall"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/simplifying-assertions-in-terraform-using-provider-defined-functions",
      "url": "https://www.hashicorp.com/blog/simplifying-assertions-in-terraform-using-provider-defined-functions",
      "title": "Simplifying assertions in Terraform using provider-defined functions",
      "content_html": "<p>Continuously validating your HashiCorp Terraform configurations greatly improves the user experience for those managing infrastructure. Continuous validation helps you deploy predictable and reliable infrastructure and provides direct feedback after changes are made. For instance, verifying if a website returns the expected status code post-deployment or the validity of a certificate after each run allows for early issue identification and resolution, minimizing impact and maintaining the integrity of a system.</p>\n\n<p>This post explores strategies for assertions and validations using a custom Terraform provider. By implementing these assertions and validations, you can <code>terraform apply</code> with greater confidence, which helps ensure your infrastructure meets your criteria, follows best practices, and reduces the risk of misconfigurations.</p>\n\n<h2>Terraform Assert provider</h2>\n\n<p>The <a href=\"https://registry.terraform.io/providers/hashicorp/assert/latest\">Assert provider</a> for Terraform, a provider managed by the community, offers a rich set of assertion capabilities through provider-defined functions such as <code>http_success()</code>, <code>expired()</code>, and <code>between()</code>. These assertion functions simplify your Terraform configurations, making it easier to do variable validation, continuous validation, and testing.</p>\n\n<p>The Assert provider functions complement Terraform’s <a href=\"https://developer.hashicorp.com/terraform/language/functions\">built-in functions</a> rather than replacing them. If Terraform’s built-in functions better fit your requirements, they should be your choice.</p>\n\n<p>To use the Assert provider, declare it as a <code>required_provider</code> in the <code>terraform {}</code> block:</p>\n<pre><code>terraform {\n  required_version = \">= 1.8.0\"\n  required_providers {\n    assert = {\n      source  = \"hashicorp/assert\"\n      version = \"0.11.1\"\n    }\n  }\n}</code></pre><p>You use the functions with a special syntax: <code>provider::assert::&lt;function_name&gt;</code>. For instance, to check if an HTTP status code falls within the success range, use the <code>http_success</code> function and call it using <code>provider::assert::http_success(data.http.example.status_code)</code>.</p>\n\n<p>Let&#39;s see how to use these functions to validate input variables.</p>\n\n<h2>Input variable validation</h2>\n\n<p>Terraform variables can have their default values overridden using CLI flags, <code>.tfvars</code> files, and environment variables. To ensure that any set value is within a required range of values, you can specify custom validation rules for a particular variable by adding a <code>validation</code> block within a <code>variable</code>. The <code>validation</code> block requires you to set a <code>condition</code> argument, which produces an error message If the condition evaluates to <code>false</code>. </p>\n\n<p>Using the Assert provider, the example below validates whether the value passed to the disk volume size variable is between 20GB and 40GB.</p>\n<pre><code>variable \"disk_volume_size\" {\n  type = number\n  validation {\n    condition     = provider::assert::between(20, 40, var.disk_volume_size)\n    error_message = \"Disk volume size must be between 20 and 40 GB\"\n  }\n}</code></pre><p>Without the Terraform Assert provider, you would need to create an &quot;or&quot; condition that references the <code>disk_volume_size</code> variable twice. While the condition validates the same criteria, it uses a less intuitive and readable expression:</p>\n<pre><code>condition = var.disk_volume_size >= 20 || var.disk_volume_size <= 40</code></pre><p>You can also use the <code>cidr</code> function to validate whether the provided value is a valid CIDR range:</p>\n<pre><code>variable \"subnet_a\" {\n  type = string\n  validation {\n    condition     = provider::assert::cidr(var.subnet_a)\n    error_message = \"Invalid CIDR range\"\n  }\n}</code></pre><p>Use the <code>key</code> or <code>value</code> functions to verify if a key or value is present in a map:</p>\n<pre><code>variable \"tags\" {\n  type = map(string)\n  validation {\n    condition     = provider::assert::key(\"key1\", var.tags)\n    error_message = \"Map must contain the key 'key1'\"\n  }\n}</code></pre><p>The Assert provider offers <a href=\"https://registry.terraform.io/providers/hashicorp/assert/latest/docs\">a wide range of functions</a> to help with variable validation, including numeric, IP, CIDR, JSON, YAML, Boolean, map, list, and string functions.</p>\n\n<p>The recent <a href=\"https://www.hashicorp.com/blog/terraform-1-9-enhances-input-variable-validations\">Terraform 1.9 release</a> includes enhanced input variable validation, allowing cross-object references. Previously, input validation conditions could reference only the variable itself. With Terraform 1.9, conditions can now reference other input variables, data sources, and local values. This significantly expands what authors can validate and allows for even more flexibility when using the Assert provider.</p>\n\n<p>Now that you have learned how to validate variables, let&#39;s investigate other Terraform features that can make your configuration more robust.</p>\n\n<h2>Custom conditions prevent problems</h2>\n\n<p>Besides input variable validation, Terraform supports several other custom conditions that are useful for asserting configurations, such as checks, preconditions, and postconditions.</p>\n\n<h3>Checks</h3>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/language/checks\">Checks</a> let you define custom conditions executed during every Terraform plan or apply, without impacting the overall status of the operation. They run as the final step of a plan or apply, after Terraform has planned or provisioned your infrastructure. Think of checks as a post-deployment monitoring capability.</p>\n\n<p>Using the Assert provider, here’s an example of how to verify if a website returns a successful status code:</p>\n<pre><code>data \"http\" \"terraform_io\" {\n  url = \"https://www.terraform.io\"\n}\n\ncheck \"terraform_io_success\" {\n  assert {\n    condition     = provider::assert::http_success(data.http.terraform_io.status_code)\n    error_message = \"${data.http.terraform_io.url} returned an unhealthy status code\"\n  }\n}</code></pre><p>Without the Assert provider, you’d have to manually maintain a list of hard-coded success status codes, which reduces readability and maintainability.</p>\n\n<h3>Preconditions and postconditions</h3>\n\n<p>Another type of custom condition is a <a href=\"https://developer.hashicorp.com/terraform/language/checks#resource-preconditions-and-postconditions\">precondition or postcondition</a>. These function similarly to check blocks, but differ in their timing of execution: a precondition runs before a resource change is applied or planned, while a postcondition runs after. If either a precondition or postcondition fails, it blocks Terraform from executing the current operation. (If a <code>check</code> fails it does not prevent Terraform from executing an operation.)</p>\n<pre><code>data \"http\" \"terraform_io\" {\n  url = \"https://www.terraform.io\"\n\n  lifecycle {\n    postcondition {\n      condition = provider::assert::http_success(self.status_code)\n      error_message = \"${self.url} returned an unhealthy status code\"\n    }\n  }\n}</code></pre><p>Checks and validation help you improve the runtime quality of your Terraform configuration. Here are some techniques to improve the long-term health of your configuration.</p>\n\n<h2>Continuous validation in HCP Terraform</h2>\n\n<p>HCP Terraform features continuous validation, a form of <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/health\">health assessment</a>, allowing HCP Terraform to proactively monitor if a workspace’s configuration or modules with assertions are passing, and notify you if any assertions fail. Continuous validation evaluates preconditions, postconditions, and check blocks as part of a health assessment. We recommend using <a href=\"https://developer.hashicorp.com/terraform/language/checks\">check blocks</a> for post-apply monitoring.</p>\n\n<p>The example below shows a typical use of continuous validation to detect certificate renewals before they expire. The <code>expired</code> function within the Assert provider requires an RFC3339 timestamp as its input.</p>\n<pre><code>resource \"aws_acm_certificate\" \"example\" {\n  domain_name       = \"example.com\"\n  validation_method = \"DNS\"\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\ncheck \"example_certificate_renewal\" {\n  assert {\n    # Add 336 hours (14 days) to the expiration time, making sure we have enough time to renew the certificate\n    condition     = !provider::assert::expired(timeadd(aws_acm_certificate.example.not_after, \"336h\"))\n    error_message = \"Example certificate needs to be renewed\"\n  }\n}</code></pre><p>Health assessments can be enabled for <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/workspaces/health#enable-health-assessments\">individual workspaces</a> or <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/organizations#health\">organization-wide</a>. To view health assessment results, including drift detection and continuous validation, go to the “Health” tab in an HCP Terraform workspace.</p>\n\n<p>If you use the <a href=\"https://registry.terraform.io/providers/hashicorp/tfe/latest/docs\">HCP Terraform and Terraform Enterprise provider</a> to manage workspace configurations, you can enable health assessments using the <code>assessments_enabled</code> argument in the <a href=\"https://registry.terraform.io/providers/hashicorp/tfe/latest/docs/resources/workspace#assessments_enabled\"><code>tfe_workspace</code></a> resource:</p>\n<pre><code>resource \"tfe_workspace\" \"example\" {\n  name                = \"example\"\n  assessments_enabled = true\n\n  # ... other workspace attributes\n}</code></pre><p>Finally, let&#39;s see how the assert provider can help simplify the process of testing Terraform modules.</p>\n\n<h2>Terraform test</h2>\n\n<p>The Terraform test framework allows you to ensure that Terraform configuration updates do not introduce breaking changes. By default, tests in Terraform create real infrastructure, so you can run assertions against this short-lived, test-specific infrastructure.</p>\n<pre><code>run \"health_check\" {\n  command = apply\n\n  assert {\n    condition     = provider::assert::http_success(data.http.index.status_code)\n    error_message = \"${data.http.index.url} returned an unhealthy status code\"\n  }\n}</code></pre><p>Note, you can configure Terraform to not create new infrastructure by setting the <code>command</code> argument to <code>plan</code>, which lets you validate logical operations and custom conditions without deploying resources.</p>\n<pre><code>run \"ebs_volume_size\" {\n  command = plan\n\n  assert {\n    condition     = provider::assert::between(1, 100, aws_ebs_volume.example.size)\n    error_message = \"EBS volume size must be between 1 and 100 GiB\"\n  }\n}</code></pre><p>Validation methods in Terraform, such as variable validation, preconditions, postconditions, and check blocks ensure the correctness and integrity of a Terraform configuration by enforcing custom conditions. For example, variable validation might prevent specifying an invalid subnet CIDR block. </p>\n\n<p>On the other hand, tests in Terraform validate the behavior and logic of the configuration, ensuring the deployed infrastructure behaves as expected.</p>\n\n<h2>Getting started with the Terraform Assert provider</h2>\n\n<p>The Terraform Assert provider, now available on the Terraform Registry, simplifies writing assertions, improving the reliability and integrity of your infrastructure deployments.</p>\n\n<p>To learn more about the Terraform Assert provider, check out these resources:</p>\n\n<ul>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/assert/latest/docs\">Terraform Registry documentation</a></li>\n<li><a href=\"https://github.com/hashicorp/terraform-provider-assert\">GitHub repository</a></li>\n</ul>\n\n<p>You can also read our <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">Terraform 1.8 provider-defined functions release blog post</a> to learn more about provider-defined functions. And, to learn how to leverage Terraform’s testing framework to write effective tests, see <a href=\"https://developer.hashicorp.com/terraform/tutorials/configuration-language/test\">Write Terraform tests</a> on the HashiCorp Developer site.</p>",
      "summary": "Learn about assertion and validation strategies for Terraform using the Terraform Assert utility provider. Plus: how to continuously validate your infrastructure using HCP Terraform.",
      "date_published": "2024-07-17T07:00:00.000Z",
      "author": {
        "name": "Bruno Schaatsbergen"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-adds-a-new-setting-to-manage-team-tokens",
      "url": "https://www.hashicorp.com/blog/terraform-adds-a-new-setting-to-manage-team-tokens",
      "title": "Terraform adds a new setting to manage team tokens",
      "content_html": "<p>We’re excited to share the latest enhancement to HashiCorp Terraform’s permissions capabilities: Admins can now control whether team members can manage their team’s API token. Now generally available in HCP Terraform and coming soon to Terraform Enterprise, this addition helps organizations improve their security posture for API token management by limiting the exposure of team tokens. </p>\n\n<h2>API token management</h2>\n\n<p>Within HCP Terraform, three types of <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/api-tokens\">API tokens</a> exist to facilitate programmatic access: </p>\n\n<ul>\n<li>User API tokens that belong to a specific user</li>\n<li>Team API tokens that belong to a specific team without being tied to any one user</li>\n<li>The organization API token that provides administrative access to settings and resources at the organizational level</li>\n</ul>\n\n<p>Team tokens are the most commonly used token type for automation workflows because they can be scoped with granular access to projects and workspaces. And since they’re not tied to an individual user, there’s less operational risk when users leave the organization.</p>\n\n<p>Previously in HCP Terraform, generating and managing team API tokens could cause difficulties for security teams, since any user added to a team, even temporarily, could create, view, regenerate, or delete the team API token. Since each team can have only one active API token at a time, an errant regeneration or deletion could interrupt critical workflows. Moreover, until the token was regenerated, former members could continue to use the team token they previously obtained even after they were removed from the team. This behavior concerned security teams and called for a more comprehensive approach to team-token permissions management.</p>\n\n<h2>Improved control for API token management</h2>\n\n<p>A new setting for team API tokens addresses this challenge. This addition boosts privilege-management efforts by letting admins control whether the members of a team can view or manage their team’s API token.</p>\n\n<p>This new setting, labeled “Team members can manage this API token”, is found on the settings page for each team, under the Teams menu in the Organization Settings. Admins can enable or disable this option for each team to meet their organizational requirements. When disabled, only members of the organization “owners” team or users with the org-wide “Manage teams” permission can create, delete, regenerate, or view the team API token.</p>\n\n<p>For existing teams, the previous behavior is unchanged — team members can manage the team token unless this setting is disabled by an administrator. For new teams, this option defaults to disabled, following a secure-by-default approach.</p>\n<img src=https://www.datocms-assets.com/2885/1720819938-team-token-mgmt.png alt=Admins can now control the ability of team members to manage the team API token in HCP Terraform.><h2>Summary and resources</h2>\n\n<p>Similar to the release of Terraform’s <em><a href=\"https://www.hashicorp.com/blog/terraform-improves-permissions-management-for-teams\">manage teams</a></em> and <em><a href=\"https://www.hashicorp.com/blog/terraform-adds-granular-permissions-to-manage-agent-pools\">manage agent pools</a> capabilities</em>, this new team-API token management setting marks another step in our effort to help users simplify permissions management and enable the least privilege principle in their infrastructure workflows.</p>\n\n<p>This feature is now available for all tiers in HCP Terraform and is coming soon to Terraform Enterprise. Please refer to Terraform’s <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/teams\">Teams documentation</a> for details on getting started.</p>\n\n<p>If you are new to Terraform, you can get started with HashiCorp-managed <a href=\"https://cloud.hashicorp.com/products/terraform\">HCP Terraform </a>for free to begin provisioning and managing your infrastructure in any environment. And don’t forget to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/users#log-in-with-your-hashicorp-cloud-platform-account\">link your HCP Terraform and HashiCorp Cloud Platform (HCP) accounts</a> for a seamless sign-in experience.</p>",
      "summary": "HCP Terraform and Terraform Enterprise improve team API token management, streamlining permissions workflows and reducing risk.",
      "date_published": "2024-07-15T17:00:00.000Z",
      "author": {
        "name": "Mitchell Ross"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/solving-the-data-security-challenge-for-ai-builders",
      "url": "https://www.hashicorp.com/blog/solving-the-data-security-challenge-for-ai-builders",
      "title": "Solving the data security challenge for AI builders",
      "content_html": "<p>This post takes a hands-on look at implementing a Microsoft Azure AI text search application that leverages the <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models\">Azure OpenAI</a> GPT3 models and <a href=\"https://www.pinecone.io/\">Pinecone</a> (a vector database) combined with <a href=\"https://www.vaultproject.io/\">HashiCorp Vault</a> to provide encryption and decryption capabilities that help protect the integrity of data in the <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">RAG</a>-based, large language model (LLM) application. </p>\n\n<p>Generative AI chatbots such as ChatGPT, Google’s Gemini, and Microsoft’s Copilot, are powerful tools that generate human-like text based on user prompts. However, the ability of these generative AI systems to follow instructions also makes them vulnerable to misuse. “Prompt injections” can let attackers bypass safety guardrails and manipulate the model’s responses. For instance, users have coerced ChatGPT into endorsing harmful content or suggesting illegal activities. </p>\n\n<p>Even as companies work to improve LLM security, integrating AI chatbots into products that interact with the internet opens up new risks. Many companies are already using chatbots like ChatGPT for real-world actions such as booking flights or scheduling meetings. However, this could allow malicious actors to <a href=\"https://www.technologyreview.com/2023/04/03/1070893/three-ways-ai-chatbots-are-a-security-disaster/\">exploit these chatbots to create phishing attacks or leak private information</a>. </p>\n\n<h2>RAG makes it even more complicated</h2>\n\n<p>Securing retrieval-augmented generation (RAG) enhanced AI applications is complex. RAG involves fetching information from external sources to provide more accurate and comprehensive responses. For instance, RAG increases the risk of data leakage, because sensitive information from external databases could inadvertently be included in the model&#39;s responses. With the integration of diverse data sources, the ability to manipulate these sources becomes easier, as bad actors have more surface area to work with and inject harmful or misleading information that the AI system then disseminates.</p>\n\n<p>In this post’s example AI text search application, we use <a href=\"https://www.terraform.io/\">HashiCorp Terraform</a> to build an Azure OpenAI based application to provide inputs to the LLM prompt. It doesn&#39;t train the model, as the LLM is pre-trained using public data, but it does generate responses that are augmented by information from the additional context the user adds. The demo leverages Microsoft&#39;s Azure to create an image in the Azure Container Registry (ACR) and uses this image to build a container within Azure Kubernetes Service (AKS). We then use Terraform to create a <a href=\"https://developer.hashicorp.com/terraform/language/functions/templatefile\">template file</a> using the outputs from the Terraform outputs to deploy an application into the container.</p>\n\n<p>As part of this architecture, we also use Pinecone, a vector database, which plays a crucial role in storing and retrieving relevant information based on the AI search&#39;s queries. Pinecone enables efficient searching and retrieval of data, enhancing the AI&#39;s ability to provide accurate and relevant responses. However, this also introduces new vulnerabilities: Attackers could exploit weaknesses in the vector database, gaining unauthorized access to sensitive information stored within Pinecone. They could manipulate the database to alter the information retrieved by the AI, leading to inaccurate or harmful responses. Ensuring the security of the data in the vector database is essential to protect against such threats and maintain the integrity of the RAG system. <strong>This is where HashiCorp Vault can help</strong>.</p>\n\n<p>Building an encryption engine is challenging due to the complex nature of cryptographic algorithms and the rigorous security requirements needed to protect sensitive data. Cryptography demands a deep understanding to implement algorithms that are both secure and efficient. Additionally, to ensure there are no vulnerabilities or weaknesses that could be exploited by attackers, continued testing is required. Proper key management, access control, and compliance with various regulations further complicate the process. Even small errors in design or implementation can lead to significant security flaws, hence using a dedicated tool such as Vault, which is a robust tool for managing secrets and encrypting sensitive data, is an easy choice. Using Vault, sensitive information can be encrypted before it is stored in Pinecone so that the data remains protected even if the vector database is compromised. Vault also provides fine-grained access control, so only authorized entities can decrypt and access the sensitive information.</p>\n\n<h2><strong>How to secure a Pinecone-based RAG system</strong></h2>\n\n<p>To illustrate how HashiCorp Vault can enhance the security of a RAG system, we have built a demo using Pinecone as the vector database. This demo shows how to integrate Vault into the RAG workflow to encrypt and manage sensitive data.</p>\n\n<h3><strong>Architecture overview</strong></h3>\n\n<p>The architecture for this demo involves several components:</p>\n\n<ol>\n<li><strong>AI language model</strong>: Generates responses and retrieves relevant information based on user prompts.</li>\n<li><strong>Pinecone vector database</strong>: Stores and retrieves vectorized information.</li>\n<li><strong>HashiCorp Vault</strong>: Manages secrets and encrypts sensitive data.</li>\n<li><strong>The RAG workflow</strong>: Manages the interaction between the AI search, Pinecone, and Vault.</li>\n</ol>\n<img src=https://www.datocms-assets.com/2885/1720709764-gpt-rag.png alt=RAG workflow between the AI search, Pinecone, and Vault.><h3><strong>Building the demo</strong></h3>\n\n<p>Creating this demo involves five steps:</p>\n\n<p>1.&nbsp;<strong>Set up the environment</strong>:\n- Clone this <a href=\"https://github.com/dawright22/azure-pinecone-terraform-deployment.git\">repository</a>.\n- Install Vault and configure it to handle secret management and encryption.</p>\n\n<p>2.&nbsp;<strong>Integrate Vault with Pinecone</strong>:\n- Modify the data flow to include encryption and decryption steps using Vault.\n- When storing data in Pinecone, first encrypt it using Vault&#39;s <a href=\"https://developer.hashicorp.com/vault/docs/secrets/transit\">transit secrets engine</a>.\n- When retrieving data from Pinecone, decrypt it using Vault before using it in the AI search.</p>\n\n<p>3.&nbsp;<strong>Implement the encryption and decryption logic</strong>:\n- Use Vault&#39;s API to handle encryption and decryption, as shown in this Python example code: </p>\n<pre><code>import hvac\n\n# Initialize the Vault client\nclient = hvac.Client(url='http://127.0.0.1:8200', token='YOUR_VAULT_TOKEN')\n\n# Encrypt data\nresponse = client.secrets.transit.encrypt_data(\n    name='your-transit-key',\n    plaintext='your-plain-text-data'\n)\nciphertext = response['data']['ciphertext']\n\n# Decrypt data\nresponse = client.secrets.transit.decrypt_data(\n    name='your-transit-key',\n    ciphertext=ciphertext\n)\nplaintext = response['data']['plaintext']</code></pre><p>4.&nbsp;<strong>Modify the RAG flow</strong>:\n- Update the RAG flow to include steps for encryption before storing data in Pinecone and decryption after retrieving data.\n- Ensure that the deployed application can handle the encrypted and decrypted data seamlessly.</p>\n\n<p>5.&nbsp;<strong>Test the integration</strong>:\n- Run the demo to ensure the data is correctly encrypted before storage and decrypted after retrieval.\n- Validate that the deployed application can generate accurate responses using the encrypted data workflow.</p>\n\n<p>See the full example in our <a href=\"https://github.com/dawright22/azure-pinecone-terraform-deployment.git\">git repository</a>.</p>\n\n<p>You can significantly enhance data security by integrating HashiCorp Vault into a RAG system using Pinecone and Terraform. This demo showcases how encryption and secrets management can protect sensitive information, mitigate risks, and boost compliance with data protection regulations.</p>\n\n<h2>More on accelerating AI adoption on Azure with HashiCorp</h2>\n\n<p>If you’d like to learn more about using HashiCorp products for AI use cases, check out these blog posts:</p>\n\n<ul>\n<li><a href=\"https://www.hashicorp.com/blog/building-a-secure-azure-reference-architecture-with-terraform\">Building a secure Azure reference architecture with Terraform </a></li>\n<li><a href=\"https://www.hashicorp.com/blog/building-a-secure-azure-reference-architecture-with-terraform\">Deploying securely into Azure architecture with HCP Terraform and HCP Vault</a></li>\n<li><a href=\"https://www.hashicorp.com/blog/accelerating-ai-adoption-on-azure-with-terraform\">Accelerating AI adoption on Azure with Terraform</a></li>\n<li><a href=\"https://registry.terraform.io/providers/pinecone-io/pinecone/latest\">Terraform Pinecone Provider</a></li>\n</ul>",
      "summary": "This demo highlights the potential risks of using contextual data with LLMs and demonstrates how HashiCorp Vault can integrate with Pinecone to tackle AI data security challenges.",
      "date_published": "2024-07-11T19:00:00.000Z",
      "author": {
        "name": "HashiCorp, Inc."
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-aws-provider-tops-3-billion-downloads",
      "url": "https://www.hashicorp.com/blog/terraform-aws-provider-tops-3-billion-downloads",
      "title": "Terraform AWS provider tops 3 billion downloads",
      "content_html": "<p>The Terraform AWS provider, which just celebrated its 10-year anniversary, has now surpassed three billion downloads. </p>\n<img src=https://www.datocms-assets.com/2885/1719850467-aws-provider-at-3b.png alt=AWS provider at 3 billion downloads><p>&nbsp; </p>\n\n<p>While three billion downloads add up to a historic milestone, AWS and HashiCorp continue to develop new integrations to help customers work faster, use more services and features, and provide developer-friendly ways to deploy cloud infrastructure. Here are some of the initiatives that we have been partnering on:</p>\n\n<ul>\n<li>AWS and HashiCorp <a href=\"https://www.globenewswire.com/news-release/2024/06/04/2893429/0/en/HashiCorp-and-AWS-sign-strategic-collaboration-agreement-to-expand-joint-product-and-go-to-market-initiatives.html\">strategic collaboration agreement</a> </li>\n<li>Infrastructure Lifecycle Management (ILM) on AWS </li>\n<li>AWSCC provider is <a href=\"https://www.hashicorp.com/blog/terraform-aws-cloud-control-api-provider-now-generally-available\">now GA</a></li>\n</ul>\n\n<h2>AWS and HashiCorp strategic collaboration agreement</h2>\n\n<p>Earlier in June, in partnership with AWS, HashiCorp agreed to <a href=\"https://www.globenewswire.com/news-release/2024/06/04/2893429/0/en/HashiCorp-and-AWS-sign-strategic-collaboration-agreement-to-expand-joint-product-and-go-to-market-initiatives.html\">co-develop a comprehensive set of Terraform policies</a> to provide expert guidance on architecting, configuring, and operating on AWS. These policies will enforce compliance with standards such as CIS, HIPAA, FINOS, and the <a href=\"https://aws.amazon.com/architecture/well-architected/?wa-lens-whitepapers.sort-by=item.additionalFields.sortDate&wa-lens-whitepapers.sort-order=desc&wa-guidance-whitepapers.sort-by=item.additionalFields.sortDate&wa-guidance-whitepapers.sort-order=desc\">AWS Well-Architected Framework</a>. </p>\n\n<p>This joint effort is intended to help customers implement infrastructure and security lifecycle management, accelerate time-to-value, mitigate implementation risk, and provide a framework to address outcome-driven use cases surrounding security and compliance.</p>\n\n<h2>Infrastructure Lifecycle Management on AWS</h2>\n\n<p>Earlier this year, HashiCorp launched <a href=\"https://www.hashicorp.com/infrastructure-cloud\">The Infrastructure Cloud</a>, an approach powered by HashiCorp Cloud Platform (HCP) to unify <a href=\"https://www.hashicorp.com/blog/standardize-your-cloud-approach-with-infrastructure-lifecycle-management\">Infrastructure Lifecycle Management (ILM)</a> and <a href=\"https://www.hashicorp.com/blog/mitigate-cloud-risk-with-security-lifecycle-management\">Security Lifecycle Management (SLM)</a> on one platform. The ILM side of The Infrastructure Cloud gives platform teams the systems they need to build, deploy, and manage infrastructure throughout its entire lifecycle. HCP’s ILM products enable teams to enforce policies, boost productivity, sharpen visibility, and remove unneeded resources.  </p>\n\n<p>ILM on AWS includes three stages:</p>\n\n<h3>Build</h3>\n\n<p>The first step of effective ILM involves establishing a systematic and repeatable approach to creating infrastructure. <a href=\"https://www.terraform.io/\">HashiCorp Terraform</a> lets you define AWS resources in human-readable configuration files that you can version, reuse, and share. Terraform creates and manages resources on AWS through their APIs, using plugins known as providers. Defining infrastructure as human-readable code makes it easier to build, change, and version AWS resources safely and efficiently. </p>\n\n<h3>Deploy</h3>\n\n<p>In the next stage of effective ILM, organizations focus on taking configurations shared among teams and converting them into standard workflows and modules so those configurations can be reused as part of a larger cloud platform. HashiCorp and AWS have collaborated to provide the ability to trigger industry best-practice account creation via Terraform, using the <a href=\"https://developer.hashicorp.com/terraform/tutorials/aws/aws-control-tower-aft\">Control Tower Account Factory for Terraform </a>. These account creations incorporate security and cost-centric policies to limit unneeded and insecure infrastructure.</p>\n\n<h3>Manage</h3>\n\n<p>In the last stage, organizations expand the infrastructure as code provisioning workflow to include all environments. To do this, platform teams need to ensure their infrastructure upholds organizational requirements over time by effectively detecting and remediating changes. AWS and HashiCorp jointly announced the <a href=\"https://www.hashicorp.com/blog/aws-and-hashicorp-announce-service-catalog-support-for-terraform-cloud\">integration between AWS Service Catalog and HCP Terraform</a> (formerly Terraform Cloud), allowing customers to use <a href=\"https://aws.amazon.com/servicecatalog/\">AWS Service Catalog</a> as a single tool to organize, govern, and distribute their HCP Terraform configurations within AWS at scale. This integration adds advanced governance and visibility into those Terraform workflows.</p>\n\n<h2>AWSCC provider is now GA</h2>\n\n<p>Built around the AWS Cloud Control API and designed to bring new services to HashiCorp Terraform faster, the <a href=\"https://www.hashicorp.com/blog/terraform-aws-cloud-control-api-provider-now-generally-available\">AWS Cloud Control (AWSCC) provider is now generally available</a>. The 1.0 release of the AWSCC provider represents another step forward in our effort to offer launch-day support for AWS services. The <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest\">Terraform AWS Cloud Control provider</a> is automatically generated based on the Cloud Control API published by AWS, which means the latest features and services on AWS can be supported right away. The AWSCC provider gives developers access to several new AWS services, including such as <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/billingconductor_pricing_rule\">AWS Billing Conductor</a>, <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/chatbot_slack_channel_configuration\">AWS Chatbot</a>,<a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/personalize_dataset\"> Amazon Personalize</a>, <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/qbusiness_index\">Amazon Q Business</a>, and more.</p>\n\n<p>The AWSCC provider is a great complementary provider to add to your existing Terraform configurations that use the standard <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest\">AWS provider.</a> Given its ability to automatically support new features and services, this AWSCC provider can will increase resource coverage and significantly reduce the time it takes to support new capabilities. </p>\n\n<h2>Learn more about AWS and HashiCorp</h2>\n\n<p>Developers can use the Terraform AWS provider to interact with the many resources supported by AWS. To learn the basics of Terraform using this provider, follow the hands-on tutorials for <a href=\"https://developer.hashicorp.com/terraform/tutorials/aws-get-started\">getting started with Terraform on AWS</a> on our developer education platform. Interact with AWS services, including <a href=\"https://aws.amazon.com/lambda/\">AWS Lambda</a>, <a href=\"https://aws.amazon.com/rds/\">Amazon RDS</a>, and <a href=\"https://aws.amazon.com/iam/\">AWS IAM</a> by following the <a href=\"https://developer.hashicorp.com/terraform/tutorials/aws\">AWS services tutorials</a>.</p>\n\n<p>If you are currently using Terraform Community Edition or are completely new to Terraform, <a href=\"http://hashi.co/tf-cloud-bc\">sign up for HCP Terraform</a> and get started using the Free offering today.</p>",
      "summary": "HashiCorp and AWS continue to support the widespread demand for standardized infrastructure as code.",
      "date_published": "2024-07-01T16:15:00.000Z",
      "author": {
        "name": "Aurora Chun"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-1-9-enhances-input-variable-validations",
      "url": "https://www.hashicorp.com/blog/terraform-1-9-enhances-input-variable-validations",
      "title": "Terraform 1.9 enhances input variable validations",
      "content_html": "<p>HashiCorp Terraform 1.9 is now generally available, <a href=\"https://developer.hashicorp.com/terraform/downloads\">ready for download</a>, and immediately available for use in <a href=\"https://www.hashicorp.com/products/terraform\">HCP Terraform</a>. Terraform 1.9 includes several new features that have been highly requested by the Terraform community, along with a number of improvements to existing capabilities to enhance developer productivity.</p>\n\n<h2>Cross-object referencing for input variable validations</h2>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/language/expressions/custom-conditions#input-variable-validation\">Input variable validations</a>, first introduced in Terraform 0.13, ensure that input variable values meet specific requirements before execution. This reduces the likelihood of provisioning errors and misconfigurations caused by invalid or unexpected user input and lets Terraform authors create more reliable code while providing clear feedback to users through custom error messages.</p>\n\n<p>Previously, the condition block of an input validation could refer only to the variable itself. With Terraform 1.9, conditions can now refer to other input variables and even to other objects, such as data sources and local values, greatly expanding the kinds of scenarios that authors can validate.</p>\n\n<p>The following example demonstrates a variable validation condition referencing the value of a different variable. Previously, this scenario would require a precondition block on a data source or resource later in the configuration, potentially leading to a partially completed deployment:</p>\n<pre><code>variable \"create_cluster\" {\n  description = \"Whether to create a new cluster.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"cluster_endpoint\" {\n  description = \"Endpoint of the existing cluster to use.\"\n  type        = string\n  default     = \"\"\n\n  validation {\n    condition     = var.create_cluster == false ? length(var.cluster_endpoint) > 0 : true\n    error_message = \"You must specify a value for cluster_endpoint if create_cluster is false.\"\n  }\n}</code></pre><p>If a user attempts to execute this Terraform configuration with <code>create_cluster = false</code> and without providing a value for <code>cluster_endpoint</code>, they will encounter a validation error:</p>\n<pre><code># terraform plan\n\nPlanning failed. Terraform encountered an error while generating this plan.\n\n╷\n│ Error: Invalid value for variable\n│ \n│   on variables.tf line 7:\n│    7: variable \"cluster_endpoint\" {\n│     ├────────────────\n│     │ var.cluster_endpoint is \"\"\n│     │ var.create_cluster is false\n│ \n│ You must specify a value for cluster_endpoint if create_cluster is false.\n│ \n│ This was checked by the validation rule at variables.tf:12,3-13.</code></pre><p>In the next example, a data source is used to dynamically validate the instance type supplied by a user:</p>\n<pre><code>data \"aws_ec2_instance_types\" \"valid\" {\n  filter {\n    name   = \"current-generation\"\n    values = [\"true\"]\n  }\n\n  filter {\n    name   = \"processor-info.supported-architecture\"\n    values = [\"arm64\"]\n  }\n}\n\nvariable \"instance_type\" {\n  description = \"The EC2 instance type to provision.\"\n  type        = string\n\n  validation {\n    condition     = contains(data.aws_ec2_instance_types.valid.instance_types, var.instance_type)\n    error_message = \"You must select a current-generation ARM64 instance type.\"\n  }\n}</code></pre><p>This powerful new feature unlocks many new and dynamic input validation possibilities for Terraform authors to make provisioning workflows more reliable.</p>\n\n<h2>New templatestring function</h2>\n\n<p>Terraform 1.9 also brings a new built-in <a href=\"https://developer.hashicorp.com/terraform/language/functions/templatestring\"><code>templatestring</code></a> function. Similar to the existing <a href=\"https://developer.hashicorp.com/terraform/language/functions/templatefile\"><code>templatefile</code></a> function, <code>templatestring</code> is designed to render templates obtained dynamically, like from a data source result, without having to save it to a file on the local disk. The function takes two parameters: a direct reference to a named string object in the current module, and an object representing the templated variables to interpolate.</p>\n\n<p>The most common use case for this function is to retrieve a template from an external location using a data source, transform it, and pass it to another resource. This example retrieves a Kubernetes resource manifest template from an HTTP location, injects some input variables and resource references, and references it in a <code>kubernetes_manifest</code> resource:</p>\n<pre><code>data \"http\" \"manifest\" {\n  url = \"https://git.democorp.example/repocontent/k8s-templates/ingress-template.yaml\"\n}\n\nlocals {\n  manifest_final = templatestring(data.http.manifest.response_body, {\n    APP_NAME       = var.app_name\n    NAMESPACE      = kubernetes_namespace_v1.example.metadata.0.name\n    SERVICE_NAME   = kubernetes_service_v1.example.metadata.0.name\n    CONTAINER_PORT = var.container_port\n  })\n}\n\nresource \"kubernetes_manifest\" \"example\" {\n  manifest = local.manifest_final\n}</code></pre><h2>Other improvements and next steps</h2>\n\n<p>Terraform 1.9 also brings improvements to existing features:</p>\n\n<ul>\n<li><p>Building on the cross-type refactoring feature <a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions#refactor-across-resource-types\">introduced in Terraform 1.8</a>, the deprecated <code>null_resource</code> type in the <a href=\"https://registry.terraform.io/providers/hashicorp/null/latest\">hashicorp/null provider</a> can now be refactored directly to the new <a href=\"https://developer.hashicorp.com/terraform/language/resources/terraform-data\"><code>terraform_data</code> resource type</a> using <a href=\"https://developer.hashicorp.com/terraform/language/moved\"><code>moved</code> blocks</a>, allowing authors to seamlessly modernize their code.</p></li>\n<li><p>&nbsp;<code>removed</code> blocks can now declare provisioners that will be executed when the associated resource instances are destroyed. This is useful in cases where you want to remove the resource declaration from your configuration, but still execute the destroy-time provisioner. For example uses, see the <a href=\"https://developer.hashicorp.com/terraform/language/resources/syntax#removing-resources\">Removing Resources documentation</a>.</p></li>\n</ul>\n\n<p>To learn more about all of the enhancements in Terraform 1.9, review the full <a href=\"https://github.com/hashicorp/terraform/releases/tag/v1.9.0\">Terraform 1.9 changelog</a>. To get started with HashiCorp Terraform:</p>\n\n<ul>\n<li><a href=\"https://developer.hashicorp.com/terraform/downloads\">Download Terraform 1.9</a></li>\n<li><a href=\"https://app.terraform.io/public/signup/account\">Sign up for a free HCP Terraform account</a></li>\n<li>Read the <a href=\"https://developer.hashicorp.com/terraform/language/v1.9.x/upgrade-guides\">Terraform 1.9 upgrade guide</a></li>\n<li>Get hands-on with tutorials at <a href=\"https://developer.hashicorp.com/terraform/tutorials\">HashiCorp Developer</a></li>\n</ul>\n\n<p>As always, this release wouldn&#39;t have been possible without the great community feedback we&#39;ve received via GitHub issues, HashiCorp Discuss forums, and from our customers. Thank you!</p>",
      "summary": "Terraform 1.9 is now generally available, bringing enhancements to input variable validations, a new string templating function, and more.",
      "date_published": "2024-06-26T20:00:00.000Z",
      "author": {
        "name": "Dan Barr"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/new-terraform-integrations-with-cisco-dell-red-hat-servicenow-and-more",
      "url": "https://www.hashicorp.com/blog/new-terraform-integrations-with-cisco-dell-red-hat-servicenow-and-more",
      "title": "New Terraform integrations with Cisco, Dell, Red Hat, ServiceNow, and more",
      "content_html": "<p>The HashiCorp Terraform ecosystem continues to expand with new integrations that provide additional capabilities to HCP Terraform, Terraform Enterprise, and Community Edition users as they provision and manage their cloud and on-premises infrastructure. </p>\n\n<p>Terraform is the world’s most widely used multi-cloud provisioning product. Whether you&#39;re deploying to Amazon Web Services (AWS), Microsoft Azure, Google Cloud, other cloud and SaaS offerings, or an on-premises datacenter, Terraform can be your single control plane, using infrastructure as code for infrastructure automation to provision and manage your entire infrastructure.</p>\n<img src=https://www.datocms-assets.com/2885/1718378819-screenshot-2024-06-14-at-11-26-26-am.png alt=Terraform q2 24 integrations><h2><strong>HCP Terraform integrations</strong></h2>\n\n<p>HCP Terraform (earlier known as Terraform Cloud) is a fully managed SaaS platform for teams and organizations to adopt and scale their Infrastructure Lifecycle Management, including state management, policy as code enforcement, drift detection, continuous health checks, and more. </p>\n\n<h3><strong>ServiceNow</strong></h3>\n\n<p>Version 2.5 of the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/integrations/service-now/service-catalog-terraform\">ServiceNow Service Catalog for Terraform</a>, now <a href=\"https://www.hashicorp.com/blog/servicenow-catalog-for-terraform-adds-no-code-integration\">generally available</a>, has the ability to provision <a href=\"https://developer.hashicorp.com/terraform/tutorials/cloud/no-code-provisioning\">no-code workspaces</a> through the Service Catalog. This update gives ServiceNow administrators the option to link Catalog Items in the Terraform Catalog to no-code modules in the HCP Terraform private registry instead of directly connecting to version control system (VCS) repositories. This feature opens up a new avenue for creating Terraform workspaces and provisioning infrastructure using no-code modules, empowering users to seamlessly deploy pre-approved modules without writing any Terraform code. </p>\n\n<h2><strong>Terraform Enterprise integrations</strong></h2>\n\n<p>Terraform Enterprise is the self-managed distribution of HCP Terraform that allows teams and organizations to adopt and scale their Infrastructure Lifecycle Management, including state management, policy as code enforcement, drift detection, continuous health checks, and more.</p>\n\n<h3><strong>Red Hat</strong></h3>\n\n<p>Terraform Enterprise is now fully supported for deployment on Podman with Red Hat Enterprise Linux 8 and above. With the<a href=\"https://www.hashicorp.com/blog/terraform-enterprise-adds-podman-support-and-workflow-enhancements\"> general availability</a> of Podman support in <a href=\"https://developer.hashicorp.com/terraform/enterprise/releases/2024/v202404-2\">Terraform Enterprise v202404-2</a> and above, organizations that have standardized on Red Hat Enterprise Linux can continue to leverage their preferred platform with a fully supported upgrade path.</p>\n\n<h2><strong>Terraform providers</strong></h2>\n\n<p>As part of our focus on helping drive innovation through new Terraform integrations and AI, we recently launched our AI Spotlight Collection on the <a href=\"https://registry.terraform.io/\">HashiCorp Terraform Registry</a>.  Our goal is to accelerate Terraform users’ IT operations and support AIOps implementations by integrating with our AI and ML partners. Make sure to keep an eye on the spotlight section as we continue to expand our listed offerings in the coming months.</p>\n\n<p>Additionally, we had 15 new verified Terraform providers from 11 different partners over the previous quarter:</p>\n\n<h3><strong>CAST.AI</strong></h3>\n\n<p>CAST.AI, makers of a Kubernetes automation platform, released a new <a href=\"https://registry.terraform.io/providers/castai/castai/latest\">Terraform provider for CAST AI</a> that can be used to onboard your cluster and manage resources supported by CAST AI.</p>\n\n<h3><strong>Cisco Systems</strong></h3>\n\n<p>Cisco delivers software-defined networking, cloud, and security solutions to help transform businesses. Cisco DevNet has released four new providers, including the <a href=\"https://registry.terraform.io/providers/cisco-open/dcloud/latest\">Terraform Provider for dCloud Topology Builder</a>, which allows you to manage topologies, networks, VMs, and hardware using the dCloud Topology Builder API. With this provider, you can use Terraform to create, update, and delete topologies and their associated resources in dCloud. Next is the <a href=\"https://registry.terraform.io/providers/cisco-open/appd/latest\">AppDynamics Cloud Terraform provider</a>, a plugin that allows Terraform to manage resources on the AppDynamics Cloud Platform. Third, Cisco released the <a href=\"https://registry.terraform.io/providers/cisco-open/observability/latest\">Cisco Terraform provider for Observability</a>, which lets users manage resources in the <a href=\"https://www.cisco.com/site/us/en/solutions/full-stack-observability-platform/index.html\">Cisco Observability Platform</a> using Terraform. The final release is the <a href=\"https://registry.terraform.io/providers/cisco-open/meraki/latest\">Terraform provider for Cisco Meraki</a>, which allows Terraform to manage and automate your <a href=\"https://github.com/cisco-open/terraform-provider-meraki/blob/main\">Cisco Meraki</a> environment. </p>\n\n<h3><strong>The Cloud Software Group (NetScaler)</strong></h3>\n\n<p>NetScaler — part of The Cloud Software Group — offers a variety of cloud networking products. They’ve released the <a href=\"https://registry.terraform.io/providers/netscaler/netscalersdx/0.4.0\">NetScalerSDX provider</a>, which provides infrastructure as code (IaC) to manage your application delivery code (ADC) via SDX. The Terraform provider lets you provision, start, stop, and reboot VPXs on SDX.</p>\n\n<h3><strong>Dell Technologies</strong></h3>\n\n<p>Dell Technologies, provider of broad and innovative technology and services for the data era, released the <a href=\"https://registry.terraform.io/providers/dell/apex/latest\">Terraform provider for Apex Navigator</a>. This provider allows datacenter and IT administrators to automate and orchestrate the provisioning and management of Dell Apex resources with Terraform. Currently, the provider is in beta with limited support.</p>\n\n<h3><strong>F5 Networks</strong></h3>\n\n<p>F5 Networks, whose services makes it easy to optimize availability, protect data, speed deployments, and support enterprise-grade apps, released the <a href=\"https://registry.terraform.io/providers/F5Networks/f5os/1.3.5\">F5OS provider</a> for F5 VELOS and F5 rSeries. The provider helps organizations automate configurations and interactions with various services provided by the F5 VELOS platform and F5 rSeries appliances.</p>\n\n<h3><strong>Grafana Labs</strong></h3>\n\n<p>Grafana Labs, makers of dashboarding technology to visualize and correlate data, has released a pair of new providers.  The <a href=\"https://registry.terraform.io/providers/grafana/schemas/latest\">Grafana Schemas Terraform provider</a> (experimental), is a generated provider to manage Grafana dashboards. The code in this repository should be considered experimental with no support and is not meant to be used in production environments, but the goal is for these generated data sources to become a part of the official Grafana provider once this project becomes more mature. The <a href=\"https://registry.terraform.io/providers/grafana/grafana-adaptive-metrics/latest\">Terraform provider for Grafana Adaptive Metrics</a> allows Terraform to run and manage Adaptive Metrics functionality within the Grafana platform.</p>\n\n<h3><strong>NHN Cloud</strong></h3>\n\n<p>NHN Cloud, makers of cloud-based platforms to drive customers’ business objectives, released the <a href=\"https://registry.terraform.io/providers/nhn-cloud/nhncloud/latest\">Terraform provider for NHN Cloud</a>, which allows users of the NHN cloud platform to manage and provision infrastructure with Terraform.</p>\n\n<h3><strong>Pinecone</strong></h3>\n\n<p>Pinecone makes a fully managed vector database with the ability to easily add vector search to production applications. They’ve released the <a href=\"https://registry.terraform.io/providers/pinecone-io/pinecone/latest\">Terraform provider for Pinecone</a>, which lets Terraform manage Pinecone resources.</p>\n\n<h3><strong>Streamkap</strong></h3>\n\n<p>Streamkap, makers of modern real-time data exchange systems to sync data between systems and organizations, released its <a href=\"https://registry.terraform.io/providers/streamkap-com/streamkap/latest\">Terraform provider for Streamkap</a>. The new provider leverages Terraform IaC to deploy and manage Streamkap tools and services.</p>\n\n<h3><strong>Temporal</strong></h3>\n\n<p>Temporal, which makes an open source durable execution platform, released the <a href=\"https://registry.terraform.io/providers/temporalio/temporalcloud/latest\">Temporal Cloud Terraform provider</a>, which allows Terraform to interact with and manage Temporal Cloud resources.</p>\n\n<h3><strong>Zilliz</strong></h3>\n\n<p>Zilliz, makers of Zilliz Cloud, released the new <a href=\"https://registry.terraform.io/providers/zilliztech/zillizcloud/latest\">Terraform Zilliz Cloud provider</a>. Zilliz Cloud is a fully managed vector database, and customers can now use its Terraform provider to use Terraform with Zilliz Cloud.</p>\n\n<h2><strong>Learn more about Terraform integrations</strong></h2>\n\n<p>All these integrations are available for review in the <a href=\"https://registry.terraform.io/\">HashiCorp Terraform Registry</a>. To verify an existing integration, please refer to our <a href=\"https://developer.hashicorp.com/terraform/docs/partnerships#terraform-cloud-integrations\">Terraform Cloud Integration Program</a>.</p>\n\n<p>If you haven’t already, try the <a href=\"https://app.terraform.io/public/signup/account\">free tier of HCP Terraform</a> to help simplify your Terraform workflows and management.</p>",
      "summary": "17 new Terraform integrations from 13 partners provide more options to automate and secure cloud infrastructure management.",
      "date_published": "2024-06-17T16:00:00.000Z",
      "author": {
        "name": "Tom O’Connell"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-adds-granular-permissions-to-manage-agent-pools",
      "url": "https://www.hashicorp.com/blog/terraform-adds-granular-permissions-to-manage-agent-pools",
      "title": "Terraform adds granular permissions to manage agent pools",
      "content_html": "<p>Today we’d like to share our latest improvement to HashiCorp Terraform’s permissions management capabilities: <em>granular permissions to manage agent pools</em>. Now available in HCP Terraform and coming soon to Terraform Enterprise, this addition lets users delegate permissions for agent pool management at the organization level.</p>\n\n<p><a href=\"https://developer.hashicorp.com/terraform/cloud-docs/agents\">HCP Terraform agents</a> let Terraform manage isolated, private, or on-premises infrastructure securely, without needing intricate networking configurations. HCP Terraform organizes agents into pools, and users can designate which agent pool handles the workloads for specific workspaces.</p>\n\n<p>Previously in HCP Terraform and Terraform Enterprise, managing <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/agents/agent-pools\">agent pools</a> could be cumbersome for organization owners, since this functionality was restricted to the owner permission level. Owners needed to review and approve frequent permission requests or elevate other users to the organizational owner team, granting full platform access to users who potentially should not hold such permissions, which could introduce security risks.</p>\n\n<h2>Introducing granular permissions to manage agent pools</h2>\n\n<p>Similar to the new <a href=\"https://www.hashicorp.com/blog/terraform-improves-permissions-management-for-teams\"><em>manage teams capability</em> added to Terraform in April</a>, the new <em>manage agent pools</em> permission streamlines and secures privilege management efforts by letting organization owners delegate the ability to manage agent pools to individual teams. This enhancement alleviates the bottleneck of relying solely on the owner to manage agent pools, as approved team members can create, update, and delete agent pools without having organization owner membership. Agent pool automation workflows like the <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/integrations/kubernetes\">HCP Terraform Operator for Kubernetes</a> also benefit from these new permissions capabilities as a key function is managing agent pools within a Kubernetes cluster. Now, users can specifically scope permissions for the agent pool rather than requiring owner-level access and developers are no longer required to handle the highly privileged organization API token, which was risky.</p>\n\n<p>You can now check the Manage agent pool checkbox under the Organization Access section of the team’s Organization Settings page and configure permissions to meet your organizational requirements.</p>\n<img src=https://www.datocms-assets.com/2885/1717625918-manage-agent-pools.png alt=Grant team permissions to create, update, and delete agent pools.><h2>Getting started</h2>\n\n<p>This feature is now available in HCP Terraform and coming soon to Terraform Enterprise. Please refer to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/permissions#manage-agent-pools\">Terraform’s permissions documentation</a> for details on getting started.</p>\n\n<p>If you are new to Terraform, you can get started with HashiCorp-managed <a href=\"https://cloud.hashicorp.com/products/terraform\">HCP Terraform </a>for free to begin provisioning and managing your infrastructure in any environment. And don’t forget to <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/users-teams-organizations/users#log-in-with-your-hashicorp-cloud-platform-account\">link your HCP Terraform and HashiCorp Cloud Platform (HCP) accounts</a> for a seamless sign-in experience.</p>",
      "summary": "HCP Terraform and Terraform Enterprise eliminate the need to rely on owner permissions to manage agent pools, streamlining permissions workflows and reducing risk.",
      "date_published": "2024-06-06T07:00:00.000Z",
      "author": {
        "name": "Mitchell Ross"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/hashicorp-at-re-inforce-security-lifecycle-management-with-aws",
      "url": "https://www.hashicorp.com/blog/hashicorp-at-re-inforce-security-lifecycle-management-with-aws",
      "title": "HashiCorp at re:Inforce: Security Lifecycle Management with AWS",
      "content_html": "<p><a href=\"https://reinforce.awsevents.com/?trk=cffe6abb-24b3-4a17-92a4-c062a07d5950\">AWS re:Inforce</a> is an immersive cloud security learning event kicking off Monday, June 10, in Philadelphia. HashiCorp once again has a major presence at the event, including breakout sessions, expert talks, and product demos. </p>\n\n<p>Earlier this year, HashiCorp announced <a href=\"https://www.hashicorp.com/blog/introducing-the-infrastructure-cloud\">The Infrastructure Cloud</a> and showed how <a href=\"https://www.hashicorp.com/infrastructure-cloud/security-lifecycle-management\">Security Lifecycle Management (SLM)</a> helps users protect sensitive elements of their infrastructure, inspect their security posture, and securely connect machines, users, and services. The AWS re:Inforce security conference offers an opportunity to detail the new additions and improvements we have made across our SLM product portfolio that do just that. That includes new announcements and integrations with AWS to help customers enforce security best practices across AWS resources. </p>\n\n<p>Recent HashiCorp/AWS security developments include:</p>\n\n<ul>\n<li>An agreement to co-develop new policy as code for HashiCorp Terraform</li>\n<li>Secrets sync with HashiCorp Vault and AWS Secrets Manager </li>\n<li>AWS Secrets Manager now displays secrets managed via Vault </li>\n<li>Workload identity federation for AWS</li>\n<li>Scanning secrets in Terraform using HCP Vault Radar</li>\n<li>re:Inforce speaking session: Security Lifecycle Management in a multi-cloud world</li>\n</ul>\n\n<h3>Policy as code co-development</h3>\n\n<p>HashiCorp and AWS have announced a <a href=\"https://www.globenewswire.com/news-release/2024/06/04/2893429/0/en/HashiCorp-and-AWS-sign-strategic-collaboration-agreement-to-expand-joint-product-and-go-to-market-initiatives.html\">Strategic Collaboration Agreement</a> to help customers drive innovation with HashiCorp solutions on AWS. The agreement includes a joint initiative to create policy as code using Terraform. </p>\n\n<p>The concept of <a href=\"https://developer.hashicorp.com/sentinel/docs/concepts/policy-as-code\">policy as code</a> involves writing code in a high-level language to manage and automate policies. For example, you could write a policy using HashiCorp’s policy as code language, <a href=\"https://www.hashicorp.com/sentinel\">Sentinel</a>, to not allow deployments on Fridays, or you could write a policy that only allows approved Terraform modules to be provisioned, unless you’re an administrator. </p>\n\n<p>As part of the agreement, AWS and HashiCorp will co-develop comprehensive Terraform policies that provide expert guidance on architecting, configuring, and operating on AWS. These solutions will include policies for additional AWS services and ensure compliance with standards set by governing bodies such as CIS, HIPAA, and FINOS, while also following the principles of the <a href=\"https://aws.amazon.com/architecture/well-architected/?wa-lens-whitepapers.sort-by=item.additionalFields.sortDate&wa-lens-whitepapers.sort-order=desc&wa-guidance-whitepapers.sort-by=item.additionalFields.sortDate&wa-guidance-whitepapers.sort-order=desc\">AWS Well-Architected Framework</a>. This joint effort will help accelerate customer time-to-value for policy as code initiatives, mitigate security and reliability risks, and provide a framework to address outcome-driven use cases surrounding security and compliance. We will provide updates as this collaboration expands. </p>\n\n<h2>Sync secrets with Vault and AWS Secrets Manager</h2>\n\n<p>Moving towards an identity-based secrets management solution helps organizations move away from non-scalable, incomplete solutions created by developers to broker secrets across cloud environments. <a href=\"https://developer.hashicorp.com/vault/docs/sync\">Secrets sync</a> is a new feature for HCP Vault Secrets and Vault Enterprise that helps organizations manage secret sprawl by syncing with other secrets managers to centralize the governance and control of secrets. Secrets sync lets users sync changes out to multiple external secrets managers, including <a href=\"https://developer.hashicorp.com/vault/docs/sync/awssm\">AWS Secrets Manager</a>, in multi-cloud environments to solve the challenges around isolated secrets management, compliance, and protecting expanded attack surfaces. For more information, check out how to <a href=\"https://hcp-docs-git-assembly-hcp-vault-secrets-hashicorp.vercel.app/hcp/docs/vault-secrets/integrations/aws-secrets-manager\">Integrate with AWS Secrets Manager</a> on the HashiCorp Cloud Platform (HCP).</p>\n<img src=https://www.datocms-assets.com/2885/1717540578-vault-secrets-sync-aws.png alt=Secrets sync><h2>AWS Secrets Manager displays secrets managed via Vault</h2>\n\n<p>To further support secrets sync, users can now search for secrets managed by Vault in AWS Secrets Manager. In the AWS Secrets Manager console, search for secrets “Managed by: HashiCorp” to display secrets managed in Vault.</p>\n<img src=https://www.datocms-assets.com/2885/1717542960-hashicorp-demo-1-2.gif alt=Secrets managed via HashiCorp Vault in the AWS Secrets Manager console><h2>Workload Identity Federation for AWS</h2>\n\n<p>Vault plugins that integrate with external systems require a set of security credentials at configuration, which are often long-lived and highly privileged. This can raise security concerns and be a non-starter for organizations that require ephemeral credentials. <a href=\"https://www.sans.org/webcasts/destroying-long-lived-cloud-credentials-workload-identity-federation/\">Workload Identity Federation</a> (WIF), coming soon to Vault Enterprise in version 1.17, enables secretless configuration for Vault plugins that integrate with external systems supporting WIF, such as Amazon Web Services. By enabling secretless configuration, a Vault-minted identity token can be exchanged for AWS credentials. This helps organizations reduce security concerns that can come with using long-lived and highly privileged security credentials. With WIF, Vault no longer needs access to highly sensitive root credentials for cloud providers, giving operators a solution to the “secret zero” problem.</p>\n\n<h2>Scanning secrets in Terraform using Vault Radar</h2>\n\n<p><a href=\"https://developer.hashicorp.com/hcp/docs/vault-radar\">HCP Vault Radar</a> automates the detection and identification of unmanaged secrets in your code, including AWS infrastructure configurations, so that security teams can take appropriate actions to remediate issues. Radar continuously scans in real-time for the following types of information:</p>\n\n<ul>\n<li>Secrets</li>\n<li>Personally identifiable information (PII)</li>\n<li>Non-inclusive language (NIL)</li>\n</ul>\n\n<p>Once the scanning is complete, Radar displays the detected risks in your code by categories and ranks.</p>\n\n<p>For DevOps engineers, secret scanning in Terraform is essential for keeping your infrastructure secure. When you write and commit code, it&#39;s easy to accidentally include sensitive information like passwords or API keys, especially when experimenting or locally testing your Terraform configs. However, in production, this could lead to disaster. Secret scanning can help you catch these issues before they get saved in your version control system. This not only protects your infrastructure from security breaches but also ensures you&#39;re following best practices. Secret scanning offers peace of mind and lets engineers focus on building and maintaining systems without worrying about exposing sensitive data. </p>\n\n<p>For more information on HCP Vault Radar, please join the re:Inforce lightning talk mentioned below, where we will demonstrate how to scan for unsecure secrets and how to get real-time notifications when secrets are being introduced in unsecure places.</p>\n<img src=https://www.datocms-assets.com/2885/1717542989-image-3.png alt=Where scanning secrets fits into a Terraform workflow using Vault Radar><h2>Security lifecycle management in a multi-cloud world</h2>\n\n<p>If you’re attending AWS re:Inforce, please stop by our booth (#1103) to chat with our technical experts, take in a product demo, and learn how companies like yours are accelerating their cloud journey with HashiCorp and AWS.</p>\n\n<p>Join us for a lightning talk at re:Inforce covering how HashiCorp’s Security Lifecycle Management capabilities help organizations continuously protect secrets, certificates, and other credentials; inspect their digital estate for unsecured credentials; and connect authorized machines, services, and people. Please join HashiCorp for: Security lifecycle management in a multi-cloud world (Session ID: APS222-S) on Tuesday, June 11 at 1 p.m. ET. </p>\n\n<p>If you can’t make it to re:Inforce this year, we invite you to join HashiCorp, AWS, and fellow practitioners for a zero trust, identity-based security workshop on <a href=\"https://events.hashicorp.com/workshops/vaultonaws-june26\">how to use HashiCorp Vault on AWS for effective secrets management</a> on Wednesday, June 26 at 12 p.m. PT. </p>",
      "summary": "HashiCorp will be at AWS re:Inforce sharing expert talks, product demos, and news announcements.",
      "date_published": "2024-06-05T16:00:00.000Z",
      "author": {
        "name": "Mike Doheny"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/manage-your-infrastructure-lifecycle-with-new-terraform-packer-waypoint-and-nomad",
      "url": "https://www.hashicorp.com/blog/manage-your-infrastructure-lifecycle-with-new-terraform-packer-waypoint-and-nomad",
      "title": "Manage your infrastructure lifecycle with new Terraform, Packer, Waypoint, and Nomad features",
      "content_html": "<p>Today at <a href=\"https://www.hashicorp.com/conferences/hashidays/london\">HashiDays in London</a>, we are excited to announce new capabilities across our <a href=\"https://www.hashicorp.com/infrastructure-cloud/infrastructure-lifecycle-management\">Infrastructure Lifecycle Management (ILM)</a> portfolio, including HashiCorp Terraform, Packer, Nomad, and Waypoint, to help customers build, deploy, and manage infrastructure. </p>\n\n<p>New announcements today include:</p>\n\n<ul>\n<li><strong>HCP Terraform in Europe</strong> (limited availability) to help meet European data compliance requirements</li>\n<li><strong>HCP Terraform agent enhancements</strong> (limited availability) for private VCS access and private policy enforcement</li>\n<li><strong>HCP Waypoint actions</strong> (public beta available soon) will expose Day 2 operations and CI/CD golden workflows to developers</li>\n</ul>\n\n<p>Our ILM offerings help teams build the images and architectures required for their applications, deploy them automatically and consistently, and manage the health and performance of these environments from creation through end-of-life.</p>\n\n<p>(Learn more about ILM and its role in <a href=\"https://www.hashicorp.com/infrastructure-cloud\">The Infrastructure Cloud</a> — a unified platform for your entire digital estate that lets you manage the full lifecycle of your infrastructure and security resources — in these two blog posts: <a href=\"https://www.hashicorp.com/blog/standardize-your-cloud-approach-with-infrastructure-lifecycle-management\">Standardize your cloud approach with Infrastructure Lifecycle Management</a> and <a href=\"https://www.hashicorp.com/blog/introducing-the-infrastructure-cloud\">Introducing the Infrastructure Cloud</a>.)</p>\n\n<p>ILM delivered via the HashiCorp Cloud Platform (HCP) is designed to address challenges in each stage of the infrastructure lifecycle — Day 0 (build), Day 1 (deploy), and Day 2+ (manage). This blog post looks at the announcements across the entire infrastructure lifecycle journey:</p>\n\n<h2>Day 0: Build and publish infrastructure as code to drive standardization</h2>\n\n<p>Day 0, as organizations plan and define the requirements of their services, is the time to lay a strong foundation for Infrastructure Lifecycle Management. Organizations need a programmatic approach to defining and provisioning application environments quickly and securely. They must account for security vulnerabilities in the software supply chain, including their base images and build artifacts, and avoid complicated, manual processes for their image lifecycle management workflows. </p>\n\n<h3>HCP Packer improves workflows and visibility</h3>\n\n<p><strong><a href=\"https://www.hashicorp.com/blog/hcp-packer-improves-metadata-visibility-for-artifact-creation\">HCP Packer version and plugin version tracking</a></strong> gives users even more visibility into artifact creation letting them see which versions of Packer Community Edition and builder plugins were used to create each of their artifacts. With version/plugin information stored directly in the HCP Packer artifact registry, this enhancement sets the foundation for a secure build pipeline and helps organizations ensure they are leveraging the latest Packer features.</p>\n\n<p><strong><a href=\"https://www.hashicorp.com/blog/hcp-packer-webhooks-now-generally-available\">HCP Packer webhooks</a></strong> let organizations tie their image-related workflows into their existing automation pipelines. Webhooks can be used to trigger custom automation in response to image events such as creation, assignment to a channel, and revocation to help accelerate image lifecycle management efforts. </p>\n\n<p><strong><a href=\"https://www.hashicorp.com/blog/predictable-plugin-loading-in-packer-1-11\">HashiCorp Packer predictable plugin loading</a></strong>, generally available in version 1.11, standardizes the plugin loading system to load only binaries with accompanying SHA256SUM files from specified directories. This approach ensures Packer consistently uses the intended plugins, enhancing stability and streamlining the installation and update processes for users.</p>\n\n<h2>Day 1: Deploy resources fast and securely to support application workloads</h2>\n\n<p>On Day 1, when developers are ready to provision the infrastructure needed to deploy an application, they want to use functions and the newest services quickly. They don’t want to waste valuable time repeating complex workarounds.</p>\n\n<h3>Terraform simplifies quickly using functions and the newest services</h3>\n\n<p>The <strong><a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest\">AWS Cloud Control (AWSCC) provider</a></strong> gives developers near-launch-day support for new AWS services. A complementary provider to the standard <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest\">AWS provider</a>, the AWSCC provider is generated based on the Cloud Control API published by AWS. This approach gives practitioners an extensive catalog of resources and access to resources not yet available in the standard AWS provider, including AWS Billing Conductor, AWS Chatbot, Amazon Personalize, Amazon Q, and so on. With its ability to automatically support new features and services, the AWSCC provider increases the resource coverage and significantly reduces the time it takes to support new capabilities (To learn more, read our blog post: <a href=\"https://hashicorp.com/blog/terraform-aws-cloud-control-api-provider-now-generally-available\">Terraform AWS Cloud Control API provider now generally available</a>). </p>\n\n<p><strong><a href=\"https://www.hashicorp.com/blog/terraform-1-8-improves-extensibility-with-provider-defined-functions\">Provider-defined functions in HashiCorp Terraform 1.8</a></strong> bring new flexibility for anyone in the community and HashiCorp’s partner ecosystem to extend the capabilities of Terraform. These provider-defined functions expose custom logic beyond Terraform’s built-in functions, simplifying configurations and boosting developer velocity.</p>\n\n<h3>Terraform deployment flexibility without sacrificing security</h3>\n\n<p>Organizations with strict security and compliance requirements often need to operate a private version control system (VCS) and self-manage Terraform Enterprise because they don’t want to make their VCS accessible over the public internet. Similar challenges arise for customers attempting to integrate HashiCorp Sentinel and Open Policy Agent (OPA) policy enforcement with internal systems and data sources. These connectivity limitations can prevent organizations from adopting fully managed HCP Terraform and taking advantage of the newest features as soon as they become available.</p>\n\n<p>Today, we’re announcing enhanced agent capabilities including <strong>private VCS access and private policy enforcement</strong> (currently in limited availability) so HCP Terraform customers don’t have to compromise on their security and compliance efforts. VCS and policy operations can now be proxied through a self-hosted <a href=\"https://developer.hashicorp.com/terraform/cloud-docs/agents\">HCP Terraform agent</a> — all in a secure and performant manner — within the customer&#39;s private environment.</p>\n\n<p>For European customers bound by regulatory constraints or internal policies, storing data within the European region is critical. We are excited to introduce the limited availability of <strong>HCP Terraform in Europe</strong> to help organizations expand in Europe. This new expansion is our first European HCP region and it’s designed to help customers meet data compliance requirements by keeping certain Terraform-specific data, such as state files and secrets, within Europe. Thanks to closer physical proximity, customers can also enjoy better Terraform service performance.</p>\n\n<h2>Day 2+: Manage infrastructure over time to optimize for cost, risk, and speed</h2>\n\n<p>After deployment, on Day 2 and beyond, organizations need to continuously manage their environments. Without a system of record for all infrastructure, images, workloads, and applications, organizations don’t have the visibility and insights needed to optimize their operations. For example, when organizations scale their cloud environments and DevOps teams with multiple technologies and tools, it can become difficult to keep track of all the data, including module, provider, and Terraform versions. This can lead to teams using deprecated or revoked versions, posing a potential security risk. </p>\n\n<h3>Terraform improvements for additional infrastructure visibility</h3>\n\n<p>Early this year, <strong><a href=\"https://www.hashicorp.com/blog/terraform-gains-upgrades-for-module-tests-explorer-and-more\">Explorer for HCP Terraform made further improvements</a></strong> to help users ensure their environments are secure, reliable, and compliant. With Explorer’s filter capability, customers can more effectively find, view, and use their important operational data from HCP Terraform as they monitor workspace efficiency, health, and compliance. With the new public Explorer API and CSV downloads, users can automate the integration of their data into visibility and reporting workflows outside of HCP Terraform.</p>\n\n<h3>Waypoint to standardize application patterns</h3>\n\n<p>The complexity of application delivery poses additional challenges, such as a lack of standardization, self-service workflows, and cognitive overload for developers due to context switching. Platform teams want to standardize application patterns in their organizations and provide golden workflows to their development teams to get them up and running quickly. <a href=\"https://www.hashicorp.com/products/waypoint\">HCP Waypoint</a> is an <a href=\"https://www.youtube.com/watch?v=2Nrlkn-km5A\">internal developer platform</a> (IDP) that empowers platform teams to define golden patterns and workflows for developers to manage applications at scale.</p>\n\n<p><strong><a href=\"https://developer.hashicorp.com/hcp/docs/waypoint/concepts/templates\">HCP Waypoint templates</a></strong> abstract and standardize application scaffolding like infrastructure, dependencies, application workflows, and access control — all in one place. Terraform modules underpin templates and let developers create their applications without worrying about the infrastructure details.</p>\n\n<p><strong><a href=\"https://developer.hashicorp.com/hcp/docs/waypoint/concepts/add-ons\">HCP Waypoint add-ons</a></strong> let developers easily and seamlessly install infrastructure dependencies into their Waypoint-defined applications. Dependencies are also defined using Terraform. Example dependencies include databases, caches, and queues.</p>\n\n<p>Once an application and associated infrastructure resources are created using templates and add-ons, application owners must perform a set of Day 2 tasks. Specifically, organizations need to expose Day 2 operations such as artifact promotions, rollbacks, and schema migrations to their developers, in an opinionated way. </p>\n\n<p><strong>HCP Waypoint actions</strong> (public beta available soon) will provide a push-button experience to enable those Day 2 operations. Platform teams can share pre-configured tasks like build promotions, rollbacks, and modifying feature flags with developers, who can then use those actions to execute their workflows with the push of a button.</p>\n\n<p>To learn more, see our blog post: <a href=\"https://www.hashicorp.com/blog/hcp-waypoint-adds-actions-enhancements-to-golden-pattern-capabilities-and-more\">HCP Waypoint to add actions, enhances golden pattern capabilities, and more</a>.</p>\n\n<h3>Nomad gains first-class runtime support with Consul</h3>\n\n<p>Once organizations have built and deployed infrastructure using Packer and Terraform, the next step is to deploy the application itself through HCP Waypoint, then to run and register them as services, exposing those services to the rest of the environment. To address those needs, organizations can use HashiCorp Nomad and Consul. Nomad is part of the Infrastructure Lifecycle Management portfolio and provides a complete solution for scheduling any type of workloads. Consul helps organizations securely connect applications through the use of service-based identities. </p>\n\n<p>The latest Nomad 1.8 release and upcoming Consul 1.19 release enable Consul’s support for Nomad as a first-class runtime alongside Kubernetes. To that extent, most of the features that are available in a Kubernetes environment, including Container Network Interfaces, transparent proxy, admin partitions for multi-tenancy, and API gateways are now also available for Nomad. This tight integration between Consul and Nomad greatly simplifies developer workflows for these use cases. </p>\n\n<p>Nomad also now supports time-based execution and overrides of the tasks it schedules. This gives organizations governance and control over the workloads they plan on deploying to their infrastructure by letting operators schedule when tasks should execute and when they should stop. Lastly, as part of this new release, <a href=\"https://www.hashicorp.com/long-term-support\">Long-Term Support</a> (LTS) versions of Consul Enterprise and Nomad Enterprise are now available, as well. </p>\n\n<p>To learn more, see our blog post: <a href=\"https://www.hashicorp.com/blog/nomad-1-8-adds-exec2-task-driver-support-consul-api-gateway-transparent-proxy\">Nomad 1.8 adds exec2 task driver, support for Consul API gateway, and transparent proxy</a>. </p>\n\n<h2>Get started today</h2>\n\n<p>Trusted by many of our 4,500 customers, our Infrastructure Lifecycle Management portfolio, including HashiCorp Terraform, Packer, Nomad, and Waypoint, enables organizations to embrace a cloud operating model to realize the full benefits of cloud adoption and maximize their infrastructure investments.</p>\n\n<p>You can try many of these new features now. If you are new to our ILM products, sign up for <a href=\"https://app.terraform.io/public/signup/account\">HCP Terraform</a>, <a href=\"https://www.hashicorp.com/products/packer\">HCP Packer</a> and <a href=\"https://portal.cloud.hashicorp.com/sign-up\">HCP Waypoint</a> to get started for free today. To learn more about Nomad, check out our <a href=\"https://developer.hashicorp.com/nomad/tutorials/cluster-setup\">tutorials</a>. If you are interested in the products currently in limited availability, please <a href=\"https://www.hashicorp.com/contact-sales\">contact our sales team</a>.</p>",
      "summary": "New Infrastructure Lifecycle Management (ILM) offerings from HashiCorp Terraform, Packer, Nomad, and Waypoint help organizations build, deploy, and manage infrastructure.",
      "date_published": "2024-06-04T08:00:00.000Z",
      "author": {
        "name": "Yushuo Huang"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/terraform-aws-cloud-control-api-provider-now-generally-available",
      "url": "https://www.hashicorp.com/blog/terraform-aws-cloud-control-api-provider-now-generally-available",
      "title": "Terraform AWS Cloud Control API provider now generally available",
      "content_html": "<p>The AWS Cloud Control (AWSCC) provider, built around the AWS Cloud Control API and designed to bring new services to HashiCorp Terraform faster, is now generally available. The 1.0 release of the AWSCC provider represents another step forward in our effort to offer launch day support of AWS services. Initially launched in 2021 as a <a href=\"https://www.hashicorp.com/blog/managing-resources-with-the-terraform-aws-cloud-control-provider\">tech preview</a>, the <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest\">Terraform AWS Cloud Control provider</a> is automatically generated based on the Cloud Control API published by AWS, which means the latest features and services on AWS can be supported right away. The AWSCC provider gives developers access with several new AWS services such as: <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/billingconductor_pricing_rule\">AWS Billing Conductor</a>, <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/chatbot_slack_channel_configuration\">AWS Chatbot</a>,<a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/personalize_dataset\"> Amazon Personalize</a>, <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/qbusiness_index\">Amazon Q Business</a>, and more.</p>\n\n<p>Terraform users managing infrastructure on Amazon Web Services can typically use this provider alongside the existing <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest\">AWS provider</a>. Given its ability to automatically support new features and services, this AWSCC provider will increase the resource coverage and significantly reduce the time it takes to support new capabilities. AWS and HashiCorp will continue to deliver high-quality, consistent releases to both the AWS and AWSCC providers.</p>\n\n<p>Important new features in the AWS Cloud Control provider include:</p>\n\n<ul>\n<li>Sample configurations</li>\n<li>Enhanced schema-level documentation </li>\n</ul>\n\n<p>Let’s run through what&#39;s new.</p>\n\n<h2>Introducing AWS Cloud Control API</h2>\n\n<p><a href=\"https://aws.amazon.com/cloudcontrolapi/\">AWS Cloud Control API</a> is a set of common APIs that make it easy for developers and partners to manage the lifecycle of AWS and third-party services. Cloud Control API provides five operations for developers to create, read, update, delete, and list (CRUDL) their cloud infrastructure resources. This unified set of API actions, as well as common input parameters and error types across AWS services, makes it possible for developers to immediately integrate their workflows with brand new AWS services. Any resource type published to the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/registry.html\">CloudFormation Public Registry</a> exposes a standard JSON schema that can be acted upon by this interface.</p>\n\n<p>AWS Cloud Control API makes it easier to build solutions to integrate with new and existing AWS services, while HashiCorp’s foundational technologies solve the core challenges around infrastructure so that teams can focus on business-critical tasks. Integrating Terraform with AWS Cloud Control API means developers can use new AWS features and services as soon as they are available in Cloud Control API, typically on the day of launch.</p>\n\n<h2>AWS Cloud Control provider enhancements</h2>\n\n<p>During its technical preview, we’ve added several significant user experience enhancements to the AWSCC provider, including sample configurations and enhanced schema-level documentation. These documentation enhancements help practitioners use the provider more easily and efficiently, as they include full context about each of the attributes within the resource. The improved documentation will also reduce errors and the time required for practitioners to provision a resource, as all of the information about how to use the attribute is contained within the resource’s reference page in the Terraform Registry. These enhancements bring the AWSCC provider closer to the user experience of the standard AWS provider.</p>\n\n<h3>Sample configurations</h3>\n\n<p>While the AWSCC provider was in technical preview, the biggest feature request we received from customers was for sample configurations to use as a starting point when working with a new resource. Without a sample configuration, practitioners had to start with a completely blank slate to determine the required attributes for their use case and the values for each attribute.</p>\n\n<p>As a result of this feedback, over 270 resources (with more being added weekly) now have sample configurations. The sample configuration for a given resource shows the structure and expected values for each attribute. Customers can now start with the sample configuration, copy code, and begin building their resources. Here’s an example of a sample configuration to connect the <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/supportapp_slack_channel_configuration\">AWS Support App to a Slack channel</a>:</p>\n<pre><code>resource \"aws_iam_role\" \"example\" {\n  name = \"AWSSupportSlackAppTFRole\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"supportapp.amazonaws.com\"\n        }\n        Action = \"sts:AssumeRole\"\n      }\n    ]\n  })\n\n  managed_policy_arns = [\n    \"arn:aws:iam::aws:policy/AWSSupportAppFullAccess\"\n  ]\n}\n\nresource \"awscc_supportapp_slack_channel_configuration\" \"example\" {\n  team_id                              = \"TXXXXXXXXX\"\n  channel_id                           = \"C0XXXXXXXX\"\n  channel_name                         = \"tftemplatechannel1\"\n  notify_on_create_or_reopen_case      = true\n  notify_on_add_correspondence_to_case = false\n  notify_on_resolve_case               = true\n  notify_on_case_severity              = \"high\"\n  channel_role_arn                     = aws_iam_role.example.arn\n}</code></pre><h3>Attribute-level documentation</h3>\n\n<p>More than 75 resources have now been enhanced with attribute-level documentation, and we expect to similarly enhance hundreds more resources in the coming months. For more information, see this <a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs/resources/kms_key\">example of a resource with enhanced documentation</a>.</p>\n\n<p>Customers can now provision resources with this new provider as easily as with their existing implementation of the standard AWS provider. Here is an example of provisioning AWS Chatbot using the AWSCC provider:</p>\n<pre><code>resource \"awscc_chatbot_slack_channel_configuration\" \"example\" {\n  configuration_name = \"example-slack-channel-config\"\n  iam_role_arn       = awscc_iam_role.example.arn\n  slack_channel_id   = var.channel_id\n  slack_workspace_id = var.workspace_id\n}\n\nresource \"awscc_iam_role\" \"example\" {\n  role_name = \"ChatBot-Channel-Role\"\n  assume_role_policy_document = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Sid    = \"\"\n        Principal = {\n          Service = \"chatbot.amazonaws.com\"\n        }\n      },\n    ]\n  })\n  managed_policy_arns = [\"arn:aws:iam::aws:policy/AWSResourceExplorerReadOnlyAccess\"]\n}</code></pre><h2>Better together: Using both the AWS and AWSCC provider</h2>\n\n<p>The AWSCC provider is a great complementary provider to add to your existing Terraform configurations using the standard AWS provider. The AWS provider, which just celebrated its 10-year anniversary and has recorded more than 2.8 billion downloads, offers the best user experience and performance for over 1,300 resource types across nearly 200 services. The AWSCC provider builds on this by offering access to the latest AWS services generated from the Cloud Control API published by AWS. Using the AWSCC and AWS providers together equips developers with a large catalog of resources across established and new AWS services. </p>\n\n<p>Practitioners can easily add the AWSCC provider to their existing Terraform configurations alongside the standard AWS provider. Simply add the second provider block to the configuration to access the extensive catalog of resources available in the AWSCC provider. Below is an example of using both the AWSCC and AWS providers in tandem:</p>\n<pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n\n    awscc = {\n      source  = \"hashicorp/awscc\"\n      version = \"~> 1.0\"\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n\nprovider \"awscc\" {\n  region = var.region\n}\n\n# Use the AWS provider to provision an S3 bucket\nresource \"aws_s3_bucket\" \"example\" {\n  bucket_prefix = \"example\"\n}\n\n# Use the AWSCC provider to provision an Amazon Personalize dataset\nresource \"awscc_personalize_dataset\" \"interactions\" {\n  ...\n\n  dataset_import_job = {\n    data_source = {\n      data_location = aws_s3_bucket.interactions_import.bucket\n    }\n  }\n}</code></pre><h2>AWS and HashiCorp</h2>\n\n<p>The Terraform AWS Cloud Control provider gives developers near-launch day support for new AWS services and features. It provides practitioners with an extensive catalog of resources as well as access to resources not available in the standard AWS provider.</p>\n\n<p>For more details about the general availability of the AWSCC provider, please review the documentation and tutorials:</p>\n\n<ul>\n<li><a href=\"https://registry.terraform.io/providers/hashicorp/awscc/latest/docs\">AWS Cloud Control Provider documentation</a></li>\n<li><a href=\"https://developer.hashicorp.com/terraform/tutorials/aws-get-started\">Getting started with Terraform on AWS</a> tutorial<a href=\"https://developer.hashicorp.com/terraform/tutorials/aws-get-started\"> </a></li>\n<li><a href=\"https://developer.hashicorp.com/terraform/tutorials/aws/aws-cloud-control\">Manage new AWS resources with the Cloud Control provider tutorial </a></li>\n</ul>\n\n<p>We are thankful to our partners and community members for their valuable contributions to the HashiCorp Terraform ecosystem.</p>",
      "summary": "The Terraform AWS Cloud Control provider helps you use new AWS services faster with Terraform.",
      "date_published": "2024-05-30T16:00:00.000Z",
      "author": {
        "name": "Aurora Chun"
      }
    },
    {
      "guid": "https://www.hashicorp.com/blog/standardize-your-cloud-approach-with-infrastructure-lifecycle-management",
      "url": "https://www.hashicorp.com/blog/standardize-your-cloud-approach-with-infrastructure-lifecycle-management",
      "title": "Standardize your cloud approach with Infrastructure Lifecycle Management",
      "content_html": "<p>The cloud provides an environment where developers can create and deploy applications rapidly. However, many organizations that limit their focus to accelerating application delivery end up struggling to find cloud success. Because the cloud dramatically expands their infrastructure footprint across different environments, it gets harder to maintain visibility and control, slowing developer productivity and making it harder to realize the promised business value.</p>\n\n<p>Addressing the challenges of these dynamic environments across people, processes, and tools requires a modern, holistic approach to Infrastructure Lifecycle Management (ILM). ILM is the process by which organizations build, deploy, and manage the infrastructure that underpins cloud applications. The tools and workflows that your organization adopts for ILM should enable effective infrastructure management from Day 1 provisioning to Day N tracking, maintenance, and decommissioning. To work efficiently across different cloud environments and providers, <a href=\"https://www.hashicorp.com/resources/what-is-a-platform-team-and-why-do-we-need-them\">platform teams</a> need an ILM solution that helps them spread <a href=\"https://www.hashicorp.com/blog/a-golden-path-to-secure-cloud-provisioning-with-the-infrastructure-cloud\">golden patterns</a> across all teams to ensure they use approved resources with governance policies already baked in. </p>\n\n<p>This blog looks at how organizations can effectively manage the full lifecycle of their infrastructure to reduce risk, optimize cloud spend, and boost developer velocity to drive business value without creating future maintenance problems.</p>\n\n<h3>Build consistently with infrastructure as code</h3>\n\n<p>The first step of effective ILM involves establishing a systematic and repeatable approach to creating infrastructure. Organizations can do so by leveraging <a href=\"https://www.hashicorp.com/resources/what-is-infrastructure-as-code\">infrastructure as code (IaC)</a> to consistently codify, version, and provision infrastructure and the underlying system images across environments. Teams and individuals have varying provisioning skill sets, so establishing a common platform promotes collaboration to enable uniformity across all infrastructure. IaC lets multiple teams work on a common code base, reusing best practices and alleviating the need for development teams to reinvent existing processes. With IaC, organizations gain more visibility into the creation and provisioning of infrastructure across all cloud providers through a single platform.</p>\n\n<h3>Deploy policy-guarded infrastructure</h3>\n\n<p>In the next stage of effective ILM, a centralized platform team implements guardrails to enforce organizational requirements before infrastructure is deployed. They establish workflows for testing, validating, and approving standard images and reusable modules of code. <a href=\"https://developer.hashicorp.com/sentinel/docs/concepts/policy-as-code\">Policy as code</a> is used to codify unique organizational requirements regarding security, compliance, and cost and automatically enforce these conditions across all infrastructure. Approved modules can then be published to an internal library where teams throughout the organization can easily discover and reuse them.</p>\n\n<h3>Manage infrastructure health over time</h3>\n\n<p>In the third phase, organizations are concerned with making sure their standardized approach to infrastructure management remains persistent after deployment has taken place. To do this, platform teams need to ensure their infrastructure upholds organizational requirements over time by effectively detecting and remediating changes. They use granular visibility and monitoring capabilities to perform regular health assessments of their infrastructure and quickly resolve any changes that require attention. This phase also involves the standardization of end-of-life workflows. Organizations predefine how long some resources will live and ensure that once retired, all related provisioning pipelines are updated and they are no longer available for consumption.</p>\n\n<h2>ILM and The Infrastructure Cloud</h2>\n\n<p>Earlier this year we launched <a href=\"https://www.hashicorp.com/infrastructure-cloud\">The Infrastructure Cloud</a>, an approach powered by the HashiCorp Cloud Platform (HCP) that unifies Infrastructure Lifecycle Management and <a href=\"https://www.hashicorp.com/blog/mitigate-cloud-risk-with-security-lifecycle-management\">Security Lifecycle Management (SLM)</a> on one platform. The ILM side of The Infrastructure Cloud gives your platform teams the systems they need to build, deploy, and manage infrastructure throughout its entire lifecycle. HashiCorp offers a portfolio of products managed on HCP that enable a unified workflow to help with the different aspects of infrastructure and application deployment including HCP Terraform, Packer, and Waypoint, as well as HashiCorp Nomad.</p>\n\n<h3>Terraform for infrastructure provisioning</h3>\n\n<p><a href=\"https://www.hashicorp.com/products/terraform\">HashiCorp Terraform</a> is the industry standard for infrastructure automation, delivering IaC provisioning and managing resources across multiple cloud providers in a single workflow. Terraform’s extensive ecosystem of more than 4,000 providers lets it work with all types of infrastructure. Terraform enables organizations to establish infrastructure as a shared service to configure, publish, and consume resources. Users can start using infrastructure as code to bake in their specific security and cost requirements, with Terraform’s extensive <a href=\"https://registry.terraform.io/\">public registry</a> available to reference common configurations. Infrastructure modules can then be tested, validated, and marked for approval in an internal private registry for reuse throughout the organization. After deployment, Terraform’s Day 2 monitoring and health-assessment capabilities help confirm that infrastructure continues to meet organizational requirements until the day it is retired.  </p>\n\n<h3>Packer for image building and management</h3>\n\n<p><a href=\"https://www.hashicorp.com/products/packer\">HashiCorp Packer</a> helps standardize the building and management of images for use in any cloud or on-premises environment. Similar to Terraform’s approach at the infrastructure level, Packer lets organizations centralize image creation and management as a shared service. With Packer, users can codify security and compliance requirements and publish image versions to the HCP Packer artifact registry. This central registry stores metadata granting visibility of image artifacts. It also provides governance capabilities to ensure only approved versions are being used, and it simplifies lifecycle management tasks like health monitoring and revocation. When used with Terraform, Packer enables users to automate image updates across clouds and downstream provisioning pipelines, a pattern known as a <a href=\"https://www.hashicorp.com/blog/multicloud-golden-image-pipeline-terraform-cloud-hcp-packer\">golden image pipeline</a>. </p>\n\n<h3>Waypoint for creating an internal development platform</h3>\n\n<p><a href=\"https://www.hashicorp.com/products/waypoint\">HashiCorp Waypoint</a> is an internal developer platform (IDP) enablement service that empowers platform teams to define golden patterns and workflows for developers. For example, platform teams can define standard workflows for actions such as building an application environment, deploying to production, or performing a rollback, which developers can execute from a simple user interface. Standardized templates with Terraform configurations attached define exactly how these applications are provisioned. Close integration with Terraform lets platform teams use existing modules without having to reinvent configurations from scratch. This approach increases developer speed and alleviates the need for them to deeply understand specific infrastructure and security practices, <a href=\"https://www.waypointproject.io/\">enabling the management of applications at scale</a>.</p>\n\n<h3>Nomad for multi-tenant compute orchestration</h3>\n\n<p><a href=\"https://www.hashicorp.com/products/nomad\">HashiCorp Nomad</a> is a flexible scheduling and orchestration tool that brings modern application scheduling to any type of software. This versatility helps users manage containers, binaries, and virtual machines across cloud, edge, and on-premises environments from a single location. Nomad efficiently schedules work across large clusters, enabling companies to scale applications to any size while minimizing overhead.</p>\n\n<h2>Get started with Infrastructure Lifecycle Management</h2>\n\n<p>HashiCorp’s ILM offerings help organizations deliver cloud infrastructure fast while staying secure and cost-efficient from start to finish. To begin implementing ILM in your environment or organization, <a href=\"https://portal.cloud.hashicorp.com/sign-up\">sign up for a free HCP account</a> today.</p>",
      "summary": "Build, deploy, and manage all of your infrastructure with a single workflow with Infrastructure Lifecycle Management solutions from HashiCorp.",
      "date_published": "2024-05-29T14:00:00.000Z",
      "author": {
        "name": "Mitchell Ross"
      }
    }
  ]
}
